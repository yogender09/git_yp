                      mLDP Non-Stop Routing Design 

                    Santosh Esale (sesale@juniper.net)



Copyright (C) 2010, Juniper Networks, Inc.

NOTICE: This document contains proprietary and confidential
information of Juniper Networks, Inc. and must not be distributed
outside of the company without the permission of Juniper Networks
engineering.


TABLE OF CONTENTS

1.	INTRODUCTION
1.1	SCOPE
1.2	REVISION HISTORY
1.3	REFERENCED DOCUMENTS
1.4	GLOSSARY
2.	ARCHITECTURE OVERVIEW
2.1	DESIGN REQUIREMENTS
3.	DESIGN DETAILS
3.1	ASSUMPTIONS AND DEPENDENCIES
3.2	HIGH LEVEL DESIGN
3.3	DESIGN RISK ITEMS
3.4	IMPACTED SOFTWARE COMPONENTS
3.4.1	SOFTWARE COMPONENTS IMPACTED BY DESIGN
3.4.2	POTENTIALLY IMPACTED SOFTWARE COMPONENTS
3.5	COMPONENT INTERACTIONS
3.5.1	COMPONENT X
3.5.1.1	COMPONENT DEFINITION
3.5.1.2	LIST OF CHANGES
3.6	INTER PROCESS COMMUNICATION
3.7	CONCURRENCY STRATEGY
3.8	STARTUP STRATEGY
3.9	RESOURCE MANAGEMENT
3.9.1 MEMORY ALLOCATION STRATEGY
3.9.2 MEMORY UTILIZATION
3.9.3 STORAGE UTILIZATION
3.9.4 SYSTEM RESOURCE UTILIZATION
3.10	FAULT AND EXCEPTION HANDLING
3.11	DEBUG SUPPORT
3.12	TESTABILITY, SERVICEABILITY AND DIAGNOSE-ABILITY
3.13	CONSTRAINTS AND LIMITATIONS
3.14	REJECTED DESIGN ALTERNATIVES
4.	PATENT OPPORTUNITIES
5.	PERFORMANCE
5.1	PERFORMANCE RELATED RESOURCES
5.2	TARGET PERFORMANCE
5.3	64-BIT KERNEL SUPPORT
6.	SCALING
7.	MULTI-CORE ENVIRONMENT
8.	COMPATIBILITY ISSUES
9.	HIGH AVAILABILITY (HA)
9.1	GRACEFUL RE SWITCHOVER (GRES), ISSU/ NSSU IMPACT
9.2	NSR IMPACT
10.	LOGICAL SYSTEM / ROUTING INSTANCE IMPACT
11.	MULTI / VIRTUAL CHASSIS IMPACT (HOBSON/ FIXX/ CHAI)
12.	PLATFORMS SUPPORTED
13.	SDK IMPACT
13.1	SDK CUSTOMER USAGE
14.	JUNOS READY SOFTWARE CONSIDERATIONS
15.	FREE SOFTWARE CONSIDERATIONS
16.	3RD PARTY SOFTWARE CONSIDERATIONS
16.1	FUNCTIONAL ROLE OF 3RD PARTY CODE WITHIN JUNOS
16.2	INTEGRATION DETAILS OF 3RD PARTY CODE WITHIN JUNOS
17.	POSIX NON-COMPLIANCE
18.	FUTURE CONSIDERATIONS
19.	REVIEW COMMENTS
19.1	REVIEW STAKEHOLDER MATRIX


INDEX OF FIGURES
----------------
FIGURE 1: EXAMPLE OF SEQUENCE DIAGRAM
FIGURE 2: EXAMPLE OF CLASS DIAGRAM
FIGURE 3: EXAMPLE OF AN ACTIVITY DIAGRAM



1. INTRODUCTION


This document describes Non-Stop Routing (NSR) design for multicast label 
distribution protocol (mLDP).


1.1 SCOPE

This document describes detail design of mLDP support of NSR.


1.2 REVISION HISTORY

Revision 1.0
Date: 09/12/2012
Author: Santosh Esale
Comments: Initial revision.

Revision 1.1
Date: 09/18/2012
Author: Santosh Esale
Comments: Addressed and incorporated all the reviewer's comments. 


1.3 REFERENCED DOCUMENTS

[1] mLDP NSR functional specification
    <http://cvs.juniper.net/cgi-bin/viewcvs.cgi/sw-projects/os/nsr/rpd/mldp/mldp-nsr-functional-spec.txt>

[2] Functional spec for LDP support of NSR
    <http://cvs.juniper.net/cgi-bin/viewcvs.cgi/sw-projects/os/nsr/rpd/ldp/ldp_software_spec.txt>

[3] mLDP Link Protection functional specification
    <http://cvs.juniper.net/cgi-bin/viewcvs.cgi/sw-projects/mpls/ldp/frr/funcspec-mldp-link-protection.txt>

[4] Functional spec MLDP inband signalling for multicast traffic
    <http://cvs.juniper.net/cgi-bin/viewcvs.cgi/sw-projects/routing/multicast/mldp/MLDP_inband_signalling.txt>

[5] Supporting mLDP on Targeted LDP Sessions
    <http://cvs.juniper.net/cgi-bin/viewcvs.cgi/sw-projects/mpls/ldp/mldp/targeted-mldp-design.txt>

[6] Label Distribution Protocol Extensions for Point-to-Multipoint and 
    Multipoint-to-Multipoint Label Switched Paths
    <http://tools.ietf.org/html/rfc6388>


1.4 GLOSSARY

NSR    Non-Stop Routing.
IRS    Internal Routing Socket.
JRS    Junos Replication socket.
Ksyncd kernel synchronization process.
MCNH   Multicast composite nexthop.
CNH    Composite nexthop.
MBB    Make before break.
mLDP   Multicast label distribution protocol.
P2MP   Point-to-Multipoint.



Master/Standby RE and Primary/Backup are used interchangeably.


2. ARCHITECTURE OVERVIEW

mLDP NSR is achieved by syncing LDP P2MP LSP control states from master to 
standby RE. Standby uses these replicated control state to arrive at exact 
replica of LDP P2MP LSP control and forwarding states as master. NSR 
implementation for LDP P2MP LSPs can be sub-divided into four major parts. 

1) Initial Sync

These two communication channels are used for initial synchronization of P2MP 
LSP state.

a) Mirroring Subsystem

This is mainly used to sync
I1) Self-id of p2mp LSP route.
I2) lsp-id for NG-MVPN p2mp tunnel on Ingress. 
I3) Source and group address for PIM p2mp tunnel on Ingress.

b) Internal Routing socket (IRS)

This is a TCP connection between primary and standby LDP protocol. This 
channel is used to sync session InLib and OutLib p2mp LSP label and 
nexthop addresses.

c) Kernel state replication

This is used to replicate kernel state from master to standby RE and happens
during both intial and on-going sync. 
  

2) Ongoing Sync

Ongoing state i.e all label and address messages received over a TCP connection 
is synchronized using Junos replication socket (JSR). Messages sent from 
primary RE are first written to replicated socket to standby before writing it 
on the wire and messages received from peer are read by both RE's concurrently. 
Kernel state replication 1 (c) also happens during this stage. Initial and 
ongoing-sync could happen at same time and standby always treat information 
received on JSR as more recent than IRS. As the information on standby from 
different channels may arrive out of order, all the data structure will not 
constructed on standby till all the necessary information to establish a P2MP 
LSP is available on standby. Example - if the standby receives a out label for a
P2MP LSP, only standby data structures will be created and p2mp fec, p2mp nexus 
and p2mp nexus reference will not be created until Inlabel for P2MP LSP is seen 
by standby.


3) KRT flood nexthop Sync

For P2MP LSP KRT flood nexthop NSR, we maintain two lists on standby, one for 
branches learnt from kernel via RTSOCK messages and other for branches created 
by application i.e ldp. Each branch could be a single nexthop (PTX: CNH-->FNH) 
or a unilist/List nexthop(for PTX) for link-protection. We link the KRT flood 
nexthop to flood nexthop or multicast composite nexthop (MCNH) learnt from kernel. 
After RE switchover, we will check these two lists to see if we have to re-program 
the branches i.e delete some of the old kernel branches which no longer exists and 
program new branches created by ldp which do not exists in kernel. For successful 
NSR of KRT flood nexthop, the kernel learnt branches should match with ldp created 
branches. 


4) Switchover

After switchover, all the standby data structures created for NSR will be purged. 
If any of the p2mp out label data structure has incomplete sync information e.g 
self-id, this will be re-allocated and route is modified. If the standby is not 
able to allocate Out label, label withdraw followed by new label mapping message 
will be sent to the peer.
    

When the standby RE comes up, it will establish a mirroring (a) and IRS (b) TCP 
connection to master RE. Mirroring subsystem will callback LDP to send all 
Ingress P2MP tunnel and P2MP LSP’s auxiliary data to standby. Master RE will 
also send adjacencies, session and MP2P i.e vanilla LDP LSP label's auxiliary 
data as in reference [2]. Standby RE will make a list of sessions that need to be 
synchronized and will request an initial sync for the first session. Master RE 
will then send all next hop addresses, In and out labels for both vanilla and 
P2MP LSP, and label request for vanilla ldp LSPs via IRS to standby. Once all 
the data is sent to standby, the session goes to sync-complete state on both the 
RE’s. The same process is repeated for all the sessions that need to be synced. 
Note that master RE sends only session in operational state via mirroring sub 
system to standby so that it can make a list of sessions to be synced.  

During initial synchronization, session will also be receiving ongoing sync 
messages on both inbound and outbound JSR socket on standby. It uses the initial 
sync and on-going sync information to construct standby data structures.

After switchover all sessions, adjacencies and p2mp tunnels which are not synced 
will be purged.


2.1 DESIGN REQUIREMENTS

- Register with mirroring subsystem to synchronize P2MP LSP’s 
  a) Route self-id.
  b) Ingress tunnel lsp-id for NG-MVPN. 
  c) Source and group address for PIM (for mldp inband signaling).
- After standby RE comes up and TCP connection for mirroring subsystem is 
  established, mirroring subsystem will call mLDP to encode and send the above 
  information. 
- Primary will also replicate flood nexthop created for LDP P2MP LSP to backup 
  via ksyncd.
- Standby receives this information and creates a replication data structures 
  for P2MP LSP and ingress P2MP tunnel.
- Reference [2] will encode operational LDP sessions and send it via mirroring 
  subsystem to standby.
- After a TCP connection between standby and master LDP protocol is established 
  (IRS), LDP initial synchronization will start. 
- Standby using reference [2] will requests primary to sync the first session. 
  Master does an outbound and inbound socket replication, sends OutLib and Inlib 
  labels, nexthop addresses received from the peer to standby.
- Master RE will send LDP P2MP LSPs InLib and OutLib label to standby.
- Standby will create P2MP LSPs standby data structures such as ldp_sb_p2mp_fec, 
  ldp_sb_p2mp_nexus and ldp_sb_p2mp_nexus_ref on receiving InLib and OutLib label.
- Standby will queue the P2MP fec for evaluation.
- If self-ids for P2MP LSP are already received by standby, It will link p2mp 
  mirror nexus with p2mp nexus.
- While adding a branch nexthop for the P2MP LSP, standby will find the existing 
  replicated KRT flood nexthop and will start adding the new branches to the 
  existing flood nexthop's application list.  
- Standby will also start receiving ongoing LDP messages for the session over JSR.
- After switchover, all the sessions and P2MP LSPs which are not completely synced 
  will be closed/tear down.
- KRT flood nexthop of each P2MP LSP is evaluated to check if kernel learnt 
  branches are same as LDP added branches, if so kernel learnt branches are 
  deleted and NSR is successful for KRT flood nexthop.
- P2MP LSP’s link protection timer will be started if required.
- If P2MP LSP is in Make before break (MBB), It will continue from the stage were 
  older master RE left off.
  

3. DESIGN DETAILS


3.1 ASSUMPTIONS AND DEPENDENCIES

None.


3.2 HIGH LEVEL DESIGN

This section describes implementation details of mLDP NSR. 

3.2.1 P2MP LSP Ingress NSR

3.2.1.1 Algorithm

When NG-MVPN calls LDP to allocate p2mp lsp-id for a root address, It will create
a ldp_p2mp_tunnel data structure which is a association between p2mp tunnel and 
p2mp fec. For NSR, we will create a p2mp mirror tunnel and link this to p2mp 
tunnel data structure. We will send this to standby if mirroring subsystem is 
connected else this will be sent at later stage whenever standby comes up.

On standby RE when NG-MVPN calls LDP to allocate p2mp lsp-id, we will look up the 
existing p2mp tunnel created by NSR replication and return the lsp-id allocated 
by master RE. For mLDP inband signaling, the existing p2mp tunnel will be returned 
while creating inband tunnel.

3.2.1.2 Data structure

3.2.1.2.1

+ struct ldp_p2mp_mirror_tunnel: 
 +   patnode                    lpmt_patnode       
 +   ldp_p2mp_mirror_tunnel_key lpmt_key 
 +   in_addr                    lpmt_root_addr
 +   p2mp_ldp_pfx               lpmt_p2mp_ldp_pfx
 +   u_int32_t                  lpmt_refcount
 +   thread                     lpmt_thread  
 +   mirror_data_node           lpmt_mirror_node  
 +   ldp_p2mp_tunnel           *lpmt_p2mp_tunnel
 +   boolean                    lpmt_in_db


3.2.1.2.2

+struct ldp_p2mp_mirror_tunnel_key:
+   int                         lpmtk_inst_id
+   ldp_tunnel_id_type_t        lpmtk_type
+   char                        lpmtk_name[TAG_LSP_NAME_SIZE + 1]


3.2.1.2.3

struct ldp_p2mp_tunnel:
+   u_int32_t                   ldp_p2mp_tunnel_refcount	
+   ldp_p2mp_mirror_tunnel     *ldp_p2mp_tunnel_mirror_tunnel


3.2.1.2.4

enum rpd_mirror_data_id
+    MIRROR_DATA_LDP_P2MP_TUNNEL = 38,


3.2.1.2.5

enum ldp_mirror_tlv_type:
+    LDP_P2MP_MIRROR_TLV_TUNNEL_TYPE
+    LDP_P2MP_MIRROR_TLV_TUNNEL_NAME
+    LDP_P2MP_MIRROR_TLV_ROOT_ADDR
+    LDP_P2MP_MIRROR_TLV_OPAQUE_TYPE
+    LDP_P2MP_MIRROR_TLV_OPAQUE_VALUE

After switchover all standby p2mp tunnels data structure will be purged and 
unsynced p2mp tunnels will be deleted.


3.2.1.2.6

Below data structures are used to show p2mp tunnel and p2mp mirror tunnel.

+struct ldp_show_p2mp_tunnel_context:
+   mgmtPeer          *ldp_p2mp_tunnel_ctx_peer
+   FILE              *ldp_p2mp_tunnel_ctx_fileptr
+   mgmtVerbosity      ldp_p2mp_tunnel_ctx_verbosity
+   ldp_instance      *ldp_p2mp_tunnel_ctx_instance
+   ldp_p2mp_tunnel   *ldp_p2mp_tunnel_ctx_tunnel


+ #define LDP_SHOW_P2MP_TUNNEL_LIMIT 10

+struct ldp_show_p2mp_mirror_tunnel_context:
+  mgmtPeer            *ldp_p2mp_mirror_tunnel_ctx_peer
+  FILE                *ldp_p2mp_mirror_tunnel_ctx_fileptr
+  mgmtVerbosity        ldp_p2mp_mirror_tunnel_ctx_verbosity

+ #define LDP_SHOW_P2MP_MIRROR_TUNNEL_LIMIT  50  
 

3.2.2 P2MP LSP Transit NSR

3.2.2.1 Algorithm

When a p2mp nexus is created for a p2mp lsp, a p2mp mirror nexus data structure 
is created and linked to p2mp nexus. This will be sent to standby via mirroring 
subsystem if it is connected else it will be sent whenever standby connects to 
master at some later stage. 

3.2.2.2 Data structure

3.2.2.2.1 

+struct ldp_p2mp_mirror_nexus :
+    patnode                     lpmn_patnode
+    ldp_p2mp_mirror_nexus_key   lpmn_key
+    u_int32_t                   lpmn_ingress_selfID
+    u_int32_t                   lpmn_transit_selfID
+    u_int32_t                   lpmn_refcount
+    thread                      lpmn_thread  
+    mirror_data_node            lpmn_mirror_node
+    ldp_p2mp_nexus             *lpmn_nexus
+    boolean                     lpmn_in_db     


3.2.2.2.2
 
struct ldp_p2mp_nexus : 
    + ldp_p2mp_mirror_nexus       *ldp_p2mp_nexus_mirror_nexus;  


3.2.2.2.3

struct ldp_sb_p2mp_nexus :
+    tag_label_t                  lspn_label   
+    ldp_p2mp_nexus               *lspn_nexus    


3.2.1.2.4

enum rpd_mirror_data_id
+    MIRROR_DATA_LDP_P2MP_NEXUS = 39,


3.2.2.2.5 

This data structures are used to show p2mp mirror nexus.

+struct ldp_show_p2mp_mirror_path_context
+    mgmtPeer         *ldp_p2mp_mirror_path_ctx_peer
+    FILE             *ldp_p2mp_mirror_path_ctx_fileptr
+    mgmtVerbosity     ldp_p2mp_mirror_path_ctx_verbosity


+ #define LDP_SHOW_P2MP_MIRROR_PATH_LIMIT  50  


3.2.2.2.6

This data structures are used to show standby p2mp FEC database.

+struct ldp_show_sb_p2mp_fec_context
+    mgmtPeer         *ldp_sb_p2mp_fec_ctx_peer
+    FILE             *ldp_sb_p2mp_fec_ctx_fileptr
+    ldp_instance     *ldp_sb_p2mp_fec_ctx_instance

+ #define LDP_SHOW_SB_P2MP_FEC_LIMIT  50  


3.2.2.2.7

This data structures are used to show standby p2mp nexus database.

+struct ldp_show_sb_p2mp_nexus_context
+   mgmtPeer          *ldp_sb_p2mp_nexus_ctx_peer
+   FILE              *ldp_sb_p2mp_nexus_ctx_fileptr
+   ldp_instance      *ldp_sb_p2mp_nexus_ctx_instance


+ #define LDP_SHOW_SB_P2MP_NEXUS_LIMIT  50


3.2.3 P2MP LSP Bud and Egress NSR

Same algorthim and data structure will be used as section 3.2.2.


3.2.4 Replicate ldp flood nexthop in KRT


3.2.4.1 Algorithm

LDP adds KRT flood nexthop for p2mp LSP with LDP key in Master RE. This flood 
nexthop will be replicated by KRT via ksyncd to standby. On standby RE, ldp will 
try to find same KRT flood nexthop by using the key from replicated data structures.

3.2.4.2 Data structure

Below data structures are added/modified to replicate LDP p2mp lsp flood nexthop  
to standby RE.

3.2.4.2.1

+struct krt_ldp_p2mp_key:
+    int              inst_id    
+    u_int32_t        self_id    
+    u_int32_t        label    


3.2.4.2.2

+struct krt_floodnh_key:
+        krt_ldp_p2mp_key_t lkey


3.2.4.2.3

enum: 
+    KRT_FLNH_TLV_TYPE_LABEL,
+    KRT_FLNH_TLV_TYPE_LDP_INST_ID
+    KRT_FLNH_TLV_TYPE_LDP_SELF_ID
+    KRT_FLNH_TLV_TYPE_LDP_LABEL


3.3 DESIGN RISK ITEMS

None.


3.4 IMPACTED SOFTWARE COMPONENTS

3.4.1 Modified Components

- LDP User Interface
- LDP P2MP 
- RPD flood nexthop infrastructure

3.4.2 Added Components

- mLDP mirror infrastructure
- mLDP user interface

3.4.1 SOFTWARE COMPONENTS IMPACTED BY DESIGN


3.4.2 POTENTIALLY IMPACTED SOFTWARE COMPONENTS


3.5 COMPONENT INTERACTIONS

As described in section 3.2.

3.5.1 COMPONENT X


3.5.1.1 COMPONENT DEFINITION

All the new components are added in LDP protocol which resides on RE.


3.5.1.2 LIST OF CHANGES

Described in detail in section 3.2.


3.6 INTER PROCESS COMMUNICATION

Using existing IPC mechanism 1) Mirroring subsystem 2) IRS 3) JRS and 4) Ksyncd


3.7 CONCURRENCY STRATEGY

RPD runs as a single process. 


3.8 STARTUP STRATEGY

None.


3.9 RESOURCE MANAGEMENT


3.9.1 MEMORY ALLOCATION STRATEGY

Memory is allocated from existing RPD memory manager.


3.9.3 STORAGE UTILIZATION

N/A.


3.9.4 SYSTEM RESOURCE UTILIZATION

N/A.


3.10 FAULT AND EXCEPTION HANDLING

N/A.


3.11 DEBUG SUPPORT

Added a new traceoption flag under ldp to trace mLDP NSR events.

[edit protocols ldp traceoptions]
+ set flag p2mp-nsr-synchronization 


3.12 TESTABILITY, SERVICEABILITY AND DIAGNOSE-ABILITY

Below show commands have been added to debug mLDP NSR.

1) show ldp p2mp tunnel 
2) show ldp replication statistics
3) show ldp replication p2mp statistics 
4) show ldp replication p2mp path   
5) show ldp replication p2mp standby-path 
6) show ldp replication p2mp standby-fec   
7) show ldp replication p2mp tunnel 

Also these existing commands can be used 

1) show ldp database extensive
2) show ldp p2mp path 


3.13 CONSTRAINTS AND LIMITATIONS

None.


3.14 REJECTED DESIGN ALTERNATIVES

None.


4. PATENT OPPORTUNITIES

None.


5. PERFORMANCE

As stated in functional specification (reference [1], section 6).


5.1 PERFORMANCE RELATED RESOURCES

N/A.


5.2 TARGET PERFORMANCE

As stated in functional specification (reference [1], section 6).


5.3 64-BIT KERNEL SUPPORT

This feature will have 64-bit support. 


6. SCALING

As stated in functional specification (reference [1], section 6).


7. MULTI-CORE ENVIRONMENT

N/A.


8. COMPATIBILITY ISSUES

None.


9. HIGH AVAILABILITY (HA)


9.1 GRACEFUL RE SWITCHOVER (GRES), ISSU/ NSSU IMPACT

ISSU will be supported. GRES will not be supported.
Please ses reference [1], section 2.2 and 10.1 for more details. 


9.2 NSR IMPACT

This feature adds NSR support for mLDP.


10. LOGICAL SYSTEM / ROUTING INSTANCE IMPACT

None. 


11. MULTI / VIRTUAL CHASSIS IMPACT (HOBSON/ FIXX/ CHAI)

N/A.


12. PLATFORMS SUPPORTED

This feature will be supported on all platforms with dual REs.


13. SDK IMPACT

None.


13.1 SDK CUSTOMER USAGE

N/A.


14. JUNOS READY SOFTWARE CONSIDERATIONS

None.


15. FREE SOFTWARE CONSIDERATIONS

None.


16. 3RD PARTY SOFTWARE CONSIDERATIONS

None.


16.1 FUNCTIONAL ROLE OF 3RD PARTY CODE WITHIN JUNOS


16.2 INTEGRATION DETAILS OF 3RD PARTY CODE WITHIN JUNOS


17. POSIX NON-COMPLIANCE

N/A.


18. FUTURE CONSIDERATIONS

None.


19. REVIEW COMMENTS

Tracked via DRT
<https://drt.juniper.net/cgi-bin/drt/view_review.cgi?review_id=7994>


19.1 REVIEW STAKEHOLDER MATRIX

Function			Name			Required?	Approval State
---------------------------------------------------------------------------------------------------------------
SW1 (kernel, RPD, PFE etc)				Yes/No/Optional	Approved/Rejected/Approved with actions
SW2 (kernel, RPD, PFE etc)				Yes/No/Optional	Approved/Rejected/Approved with actions
SW3 (kernel, RPD, PFE etc)				Yes/No/Optional	Approved/Rejected/Approved with actions
SW Manager						Yes/No/Optional	Approved/Rejected/Approved with actions
JAB							Yes/No/Optional	Approved/Rejected/Approved with actions
HW							Yes/No/Optional	Approved/Rejected/Approved with actions
---------------------------------------------------------------------------------------------------------------




© Copyright 2010 Juniper Networks, Inc. -- Proprietary and Confidential –
Do not distribute outside of the company without the permission of Juniper Networks engineering
Printed copies are for reference only!

