$Header: /cvs/juniper/sw-projects/os/nsr/rpd/bgp/scaling/rli-6748-funcspec-old2.txt,v 1.1 2009/08/03 18:22:33 srik Exp $

                            RLI 6748

             NSR: BGP scale & performance improvements

                        Functional Specification

Author: Srikanth Seshadri <srik@juniper.net>
Contributors: Rex Fernando <rex@juniper.net>

Copyright (C) 2008, Juniper Networks, Inc.

NOTICE: This document contains proprietary and confidential
information of Juniper Networks, Inc. and must not be distributed
outside of the company without the permission of Juniper Networks
engineering.


1.  INTRODUCTION

{As necessary, present an overview of the feature, including:
- a general description of the feature
- who would use it
- why they'd use it
- a high level description of how it works and how one would use it

You MUST reference any relevant requirements, MRDs, PDDs, RLIs,
tracking PRs and/or external protocol definitions (RFCs, IETF drafts,
etc.) and reference documents here for traceability purposes.}

NOTE: This approach has been currently put on hold as there are some
of the base send-side code doesn't work as designed. This causes certain
basic assumptions made in the below approach to break and therefore
doesn't produce the necessary results without fixing those basic assumptions
to hold.

Please refer to rli-6748-funcspec.txt for the latest approach. This document
is archived for possible reference in the future.

As part of BGP NSR scalability and performance improvements, RLI 6386
implemented the Socket Data Cache Layer (SDCL). This was aimed to
address the problem of the backup BGP being slow to parse the RIB-OUT
updates from the snooped socket, which backpressures the Master tcp from
sending out updates thereby incerasing the convergence time. The SDCL
implements a cache such that data from the snooped socket can be cached
on the backup RE (either in memory or on disk) as fast as it arrives. The
Master socket send thus isn't blocked. The application can read and process
the snooped data at its own pace. For a detailed description of this
feature, refer [SDCL].

This approach had some issues that were addressed by [SDCL-IMPROV].
It was determied that the changes implemented as part of [SDCL-IMPROV]
didn't produce the desired results. The key problem identified was
in the amount of attribute-related processing required on every snooped
BGP packet that was required to normalize it and update the RIBOUT entry.

The proposed design employs a hybrid approach to address the scaling issues.
In addition to snooping the outgoing updates, the Backup BGP receives
"hints" from the Primary regarding the snooped packets through a out-of-band
channel (rsync) that helps in reducing the Backup processing drastically.
The solution does have to deal with some additional complexity due to
synchronizing data between two channels, enhancing peer-group neighbor
synrhconization logic to work correctly with this scheme, etc. described in
detail later.

    Target product        : JUNOS
    Target JUNOS release  : 9.5R1
    RLI                   : 6748
    NPI Program           : 326 JUNOS Non-Stop Routing (NSR)
    Tracking PR           : 290971

Referenced documents and RLIs

[SDCL]        - Socket Data Cache Layer (NSR: Kernel Performance Improvement), 
                RLI 6386

[SDCL-IMPROV] - Improvements to the SDCL approach described in 
                sw-projects/os/nsr/bgp/rli-6748-funcspec-old.txt

2.  FUNCTIONALITY

There are no externally visible functional changes with this RLI.

2.1 Changes to RPD Internal Behavior
------------------------------------
Internal enhancements in the SDCL layer and the interaction between rpd and
the SDCL library would fall under the scope of this RLI.


2.2 CLI Changes
---------------
There are no externally visible changes. Certain tunable parameters
to optimize the internal algorithms could be exposed as hidden commands.
They would be present under:

[edit protocols bgp nonstop-routing-cache]

and will be hidden.

Performance Statistics meant for debugging can be displayed using the

"show bgp statistics" 

command both in the Master and Standby REs.

3.  CAVEATS

L3VPN performance improvements are not specifically targeted. Depending
on the resources available, they could be tested and the belief is these
changes would have a positive effect on L3VPN scaling. But if additional
issues are discovered during the testing or if there aren't enough
resources for testing, addressing L3VPN scaling issues cannot be guaranteed 
as part of this RLI.


4.  OTHER REQUIREMENTS

Refer the Functional Specification for RLI 6386.


5.  IMPLEMENTATION DETAILS

On snooping a RIBOUT Update, the Backup processes the NLRIs once and it
caches them. The same Update that gets replicated to other peers within
a peer-group and minor modifications (nexthop-change) to other peer-groups
benefit from this cache with respect to NLRI processing. But the attributes
of this packets do get processed for each Update, get normalized and then a
pointer to the normalized attribute is stored as the bgp_metrics in the RTO
(RIBOUT) structure. This attribute processing is very expensive in the
Backup since its done for every peer, while the Master does it just once per a
peer-group.

Out of the various schemes discussed, the one described here was concluded to
be the best in terms of algorithmic and implementational complexity.

The overall idea is for the Master to send a "Update-hint" to the Standby
using a out-of-band mechanism (control channel). This Update-hint would
consist of the BGP Update containing the "normalized" bgp attributes and NLRIs 
and and the set of peers to which this update has been sent out to. The path
attributes and the NLRIs contained in the Update-hint is processed just once. 
This would lead to the bgp out_metrics pointer and a set of rt (route)
pointers. When the Backup reads the snooped BGP update sent to a peer, it
determines the associated Update-hint. It doesn't have to do any processing 
of the snooped Update. It can find the RTO corresponding to this peer and set
the advertised bit as well as the out_metrics. This process gets repeated for
all the peers that share this BGP Update.

5.1 Control Channel
-------------------
It will help to open a separate control channel between the Master and Standby
to send the Update-hint messages. This will be similar to the rsync
connection. Utilizing the existing rsync channel to send these control
messages could cause some undesirable effects especially when large amounts of
rsync data itself is being sent across. An example is that an Update-hint
could be stuck after a bunch of rsync messages, while the corresponding socket
data is lying in the Backup unconsumed. This could cause the Kernel Acks for
the replicated socket data to be delayed back-pressuring the Master and thus
delaying convergence.

The Master would start listening at a pre-determined port while the the Backup
would connect and establish this control channel. This mechanism is similar to
the rsync channel establishment mechanism.

5.2 Channel Synchronization
---------------------------
Before the Backup can start proessing snooped outbound updates, the
corresponding Update-hint should be available at the Backup. This cannot
automatically guaranteed through scheduling. The per-peer receive buffer can
be utilized to store the BGP updates until the corresponding Update-hint is
available before processing can begin. This receive buffer can be 16k or 32k
so that socket data can be read out of the kernel in an optimal manner.

5.3 Update-hint start
---------------------

Need to revisit. We might initiate the jsr_replicate() at the BGP packet
boundary. But the existing send socket buffer cannot be blindly replicated
since the current data in the buffer may not begin at t BGP PDU boundary
ending up confusing the backup.

During a rsync (Backup comes up after the Master is already up and running)
or a "clear neighbor" from the Backup, rsync data starts flowing. Also, the
Master could be in the middle of sending out updates to the peer. At some
point, the Master initiates socket replication (jsr_replicate). After this
completes, the Master starts sending Update-hints for the updates generated.
During the jsr_replicate call, there could be data sitting in the Master
socket buffer that gets replicated to the Backup as well and therefore gets
read-out and processed by the Backup. These packets don't have any associated 
Update-hint information. There has to a mark that indicates (to the Backup)
the beginning of packets that have associated Update-hint information. When
the Master performs a jsr_replicate() it gets a return value consisting
of the number of bytes in the send socket buffer. The contents of the
send socket buffer gets replicated to the Backup. This size of the send
socket buffer is passed to the Backup through the control channel. After
jsr_replicate returns successfully, the Master starts sending Update-hints
for all the subsequent Updates it enqueues. Based on the send socket buffer
size, the Backup can determine the Start of the Updates that contain the
Update-hint information.

5.4 Control Messages
--------------------
The format of the Update-hint message is similar to the RIBOUT rsync messages.
It consists of a set of peers (ip-address) and an Update message containing a
normalized attribute. An example of normalization is ignoring the contents
of the next-hop attribute. Similar to the next-hop calculation that occurs
while the update is about to be generated and could be different for each
peer, 4-byte ASPATH attribute generation also occurs at the end. These
nuances are accounted for in the rsync.

Note that we cannot use a bitmap repsentation (similar to that used to
represent peers within a peer-group) in the control messages. The reason for
that being it requires additional synchronization to allocate the same bit
number for a given peer which is non-trivial due to config changes as well as
dynamic changes (peer flaps).

Each Update-hint contains a "Update sequence number" (update_seqno). Refer 
to section "Peer Desychronization" for more details.

Another type of control message is to facilitate the Update-hint start as
described in Section 5.3. This would consist of a set of peer ip-addresses
followed by a setof TLVs for each peer. One TLV type would represent the send
socket buffer and its value would be the size fo the buffer. Other TLVs can be
used to send more data like the peer Established time, etc.

5.5 Update-hint processing
--------------------------
The Update-hints are scoped to a routing-instance/peer-group. The following
processing will be done on each Update-hint message:

- for each neighbor in the message:
    1. kernel-id/ip-address to bnp lookup
    2. get the bnp_bit, group
    3. allocate bitmap array space if needed
    4. get next-free index (update_hint_index) in the bitmap array
    5. set bnp_bit in the bitmask at the index update_hint_index

Along with the bitmask in the update_hint_index, the following are stored
as well:

    - process the path attributes in the update and store the bgp out_metrics 
      pointer.
    - process the nlri's in the update and store a pointer to the an element
      containing the rt pointers and the pointer to rtt (same as the standby
      cache entry we have today)
    - count of the number of peers in the Update-hint message

A hbt-tree keyed off the sequence number is maintained. Each node contains
a pointer the bgp out_metrics and the cache element. Please refer to
sections 5.6 and 5.7 or more details on the Update sequence number.

The bitmap from the Update-hint would look like:

    update_hint_    bnp_
    index           count       -- bnp_bit --   update  Seq_no

                                p1 p2 p3 p4
    0               2           1  1  0  0      U1      S1
    1               2           1  0  1  0      U2      S2
    2               2           0  1  0  1      U3      S3

A similar bitmap array is constructed after processing the snooped outbound
updates. 

When we read the socket buffer for a given peer, we need to find out the next
snooped_update_index so that we can set the bit in that index for this peer.
This can calculated by looking at the current index we've consumed for a peer.
This begins at 0. 

get_next_index (peer)
{
    - get current update_hint_index for that peer
    - while update_hint_index <= max_update_hint_index
        - get next update_hint_index
        - check if this peer's bit is set. If so, return update_hint_index
          else get next udpate_hit_index
    - if there's no match, return -1.
}

The socket consume logic will look like:

for all the snooped sockets that have data to be read:
    - get peer address
    - snooped_update_index = get_next_index(peer)
    - if (snooped_update_index is valid)
        - set bnp_bit in the index snooped_update_index
        - bump bnp_count
        - if bnp_count on snooped_update_index matches bnp_count in
          update_hint_index, process Update-hint

The bitmap array constructed from the snooped socket based on the above
algorithm will eventually look like below.

    snooped_update  bnp_   -- bnp_bit --
    index           count
                            p1 p2 p3 p4
    0               2       1  1  0  0
    1               2       1  0  1  0
    2               2       0  1  0  1

One of the reasons for maintaining two separate bitmaks is for debugging
purposes at the cost of additional transient memory.

Also refer to the section 5. "clear bgp neighbor" handling regarding the
required processing that needs to be done when a neighbor is cleared.

When a neighbor gets added or deleted to a peer-group, the bitmask size would
require to change. This condition needs to be handled.

Processing Update-hint
----------------------
For each update_hint_index
    - get the rto for each bnp from rt
    - set out_metrics and the the bgp's rt_bit for this bnp

Managing the bitmask array
--------------------------
Each block alloc could contain space for 100 Update-hints. A per peer-group
array of pointers can manage these blocks. For the peers in the Master
instance, this per peer-group array could be of size 100. This would yield 10k
Update-hints that could be buffered. For CE peers, this array could be of size
10 and the block containing the Update-hints themselves could be of size 5.

If we considered 4100 peer groups (4k CE peer groups and 100 in the Master
instance), the approximate memory consumption could be:

Note that these are per AFI/SAFI. Currently, there isn't a need for doing any
optimization for the CE peers since the number of peers within a CE peer group
are very less.

fixed (32-bit machine):

4000 CE peer-groups x 10 entries x 4 bytes  = 160KB.
100  PE peer-groups x 100 entries x 4 bytes = 40KB.

variable (transient memory):

entry in each block for CE peer:
64 byte bit-mask + 4 byte mets/rt container pointer + 2 bytes bnp_count
    = 72 bytes x (5 to 50 entries) = 360 bytes to 3.6KB

entry in each block for PE peer
64 byte bit-mask + 4-byte container + 2 byte bnp_count
    = 72 bytes x (100 to 10000 entries) = 7.2KB to 720K

For a more typical configuration of 10 PE peer groups, this would consume 7.2MB.
The data structure of peers that received an Update constructed from the
snooped socket would be similar, say 7.2MB.

Sections 5.6 and 5.7 discuss additional memory requirements due to the
maintenance and processing related to Update_seqno.

5.6 Master Peer-desync handling
-------------------------------
A peer can get desynced from a peer-group because its consuming its updates
slowly than the other peers in the peer-group. If this happens, we mark
the peer out-of-sync. When the peer has consumed its updates and there's
space available, bgp sends the updates pending for that peer. When all
the pending updates have been sent, that peer has caught up with the rest
of the peers in the peer-group and is marked in-sync again. This is an
overhead for bgp since it needs to re-generate the updates again for
out-of-sync peers.

This new Update-hint algorithm is most optimized when all the peers in
a peer-group remain in-sync. In this case, the Update-hint would contain
the complete peer-list that this Update is being sent to. When the
corresponding Updates have been read from the sockets, this Update-hint can 
be processed just once and the RTOs for all the routes contained in the
Update can be set. When some peers go out-of-sync, the Update-hint won't
contain the complete peer-list. If we process this Update-hint after
reading socket Updates, we may soon get more Update-hint(s) for the
same Update with some Peer(s) in the peer-list (these peers were out-of-sync
while generating the initial Update-hint, now they're trying to catch up). 
These Updae-hints(s) need to be processed separately. The algorithm
will deteriorate when a lot of peers go out-of-sync.

We cannot control a slow peer and when a peer goes out-of-sync. But certain
optimizations are possible (especially detecting false positives when
NSR is configured):

- When NSR is configured, socket data gets replicated to the Backup RE. Due to
  many factors like the Backup RE being slow in consuming this data or due to 
  delay in replication, etc. the Master socket buffers can fill up quickly. If
  this happens, Master bgp would mark the corresponding peer out-of-sync.
  The migitation would be to make sure the kernel replication mechanism
  can consume the sockets soon to avoid this artificial condition. Also, the
  Backup BGP needs to consume socket buffers quickly. The latter is achieved
  through the Update-sync algorithm atomatically since there's no processing of
  the Updates involed.

- There is a known issue currently due to the above point that ends up in
  all the peers going out-of-sync at the same time. This happens during
  initial convergence when a set of updates gets replicated to all the peers
  in the peer group. This condition can be detected and handled separately.
  An approach could be to retry after a certain interval when the situation
  occurs. That delay would help in draining the send socket buffers.

- When the kernel notifies that a out-of-sync peer now is ready to consume
  more data, it might be worthwhile to wait until the send socket buffer has a
  good amount of space (say 16k). This might prevent this peer to get
  out-of-sync again immediatetely when there are lots of Updates enqueued.

- We can group the slow peers together in their own peer-group through
  configuration. This point is generic to the non-NSR case as well.

Update Sequence numbers
-----------------------
There are some optimizations that can be done to handle the following
situations:

1. peer(s) get de-synced
2. peers come up in a staggered way
3. new peers are added

The optimizations are benificial in first two situations while the it does help
in the third situation under certain circumstances when the peers are added
while the initial convergence is in progress which can be assumed as a
degenerate case of the second situation.

This algorithm hinges on the fact that a give BGP Update that was sent to 
in-sync peers will get sent again to out-of-sync peers when they are no longer
blocked. There could be cases where this situation may not be true - an
example being metrics change for some prefixes in that update, those prefixes
get withdrawn, etc. In these situations, the best possible optimization may
not be attainable. But for the rest of the cases, this algorithm will produce
beneficial results on peers getting de-synced.

The idea is to generate a unique sequence number for each unique update. If
the same update needs to be sent later for a set of peers that were unblocked,
the following optimizations are done:

    1. there is no need to send the entire Update as part of the Update-hint
       message to the Backup. The update-sequence-number sent will identify
       the Update.
    2. The Backup therefore doesn't need to process the Update. It builds a
       Update-seqno-tree through which it can identify the Update based on the
       Update-sequence-number.

Sequence Number Generation
--------------------------
We can hang a pointer to a data structure (update_seqno_cache) from the
bgp_out_metrics that describes the updates that have been sent using that
metrics. This structure is an hbt. If the hbt doesnt' contain any elements, it
means that we need to generate a new sequence number for this Update to be
sent with the Update-hint message.  Otherwise, we search the hbt to match
a previously sent update and get the update_seqno from the matched node and
send this seqno in the Update-hint message..

When generating a new Update, we get a new update_seqno, then add a new node
to the hbt to store the seqno. The key is certain properties of the update like
its len, AFI/SAFI, # NRLI, etc. The node also contains a pointer to the
actual Update message itself.

The data structure is also threaded in a time-ordered list. Elements are
cleaned from the cache in this order when either the cache-timeout expires or
the cache size execeeds the maximum memory limit.

Note that this cache is a global cache. Its not per peer-group.

Other data stuctures considered
-------------------------------
Just as a note, a data struture similar to the mrto_hash was considered.
When we need to add an update to this cache, we hash the metrics
(the hash calculation is free since its already done in this function to
index into the mrto_hash).  In that hash bucket, we maintain the Updates
in a hbt. The hbt is maintained in the way as in the above scheme.

This scheme maintains a per peer-group data structure. If the same updates are
shared across peer-groups, the search cost with this scheme is slightly less.
But the disadvantage is that the update_seqno cannot be shared across
peer-groups which would require re-sending the update in the Update-hint
message and the cost of parsing the update in the Standby. Also, this scheme
would require additional memory both in the Master and Standby since
individual copies of the updates need to be maintaned per peer-group.

An Update is added to the cache irrespective of whether the rto is still
present (doesn't get converted to a tsi) after the Update has been enqueued. 
If the rto is present, there are de-sync peers in the peer-group being
seriviced and cache entry can be used to send updates to those peers. The
reason for adding the cache entry even when the rto gets converted to tsi is
for sharing the update across peer-groups. 

The Upate_seqno itself is a monotonically increasing value and is global across
peer-groups.

The update-seqno algorithm runs only when NSR is configured and the Backup rpd
is up and has established the control rsync connection with the Master.

Transient Memory Requirements
------------------------------
Considering a data structure that contains the following elements: update_len,
AFI/SAFI, number of nlris, overhead due to hbt_node and thread and an average
update size of 100b internet size (attributes + 5 prefixes), a node occupies
120 bytes. To store 10k updates, the memory required would be 12MB. Increasing
this to 20k updates, the memory would be 24MB.

* where the logic may not produce optimal results (route churn)

5.7 Backup Peer-desync handling
-------------------------------
This section describes how the Backup matches the Update_seqno to the actual
Update while processing an Update-hint message that contains an Update_seqno,
but not the actual Update.

When a Update-hint is received with a BGP Update, the regular BGP Update
processing happens (similar to the Update Processing in rsync messages). The
normalized attributes are then added to the the regular bgp metrics data
structure. The AF metrics are also normalized. An hbt is maintained that is
keyed off the Update_seqno. While processing this Update-hint, a node is
created that contains bgp_metrics, rt pointers (for the routes contained in
the Update), pointers to af_metrics for each rt.

When a Update-hint arrives with just the Update_seqno, and after the Updates
for all the peers contained in the Update-hint has been snooped, a lookup is
done in this tree with Update_seqno as the key. This gives the mets, set of
rts and the corresponding set of af_mets. The rto for a given rt corresponding
to the peer-group (calclated from the kenrel-id/bnp) can then be updated with
the metrics pointers and the peer bitmask can be populated.

Similar to the Master's Update_seqno_cache, the elements are threaded in a
time based list for the purpose of cleaning up the seqno_tree.

Memory Requirements
-------------------
Considering 5 prefixes per update, we'd require around 40 bytes per node.
To store 10k Updates requires 400kB, and 20k Updates requires 800KB.

In case of VPN Updates, we'd need to store af_metrics pointers for reach rt.
That would require 60 bytes per node for a 5 prefixes per update distribution.
To store 10k Updates requires 600KB, and 20k Updates requires 1.2MB.

5.8 Control Channel Error handling
----------------------------------
Situations can arise such that when the Master is attempting to write an
Update-hint message to the control channel, the channel is full and the write
returns EAGAIN. This would indicate the Backup is busy and hasn't yet gotten a
chance to consume the Update-hints. Most likely, this would be a transient
scenario and would clear up very soon. It could happen if the Backup is
busy reading the snooped socket Updates. The processing required itself
on these snooped Updates itself is very minimal. o

A solution could be to throttle the Master from sending more Updates until the
Backup has drained the control channel.


5.9 Initial Convergence improvements
------------------------------------
During the initial router startup, peers would come up in a staggered manner.
Also, the routes could be learnt in a non optimimal way such that bestpath
routes are learnt later than other paths leading to sending out increased
number of Updates. The caching algorithm may not work very Optimally due to
peers coming up in a staggered way. Some optimizations that could be done here
include:

    - A short term solution such that the rsync session itself is deferred
      until the initial convergence is complete. This would prevent the Backup
      RE being a bottleneck during the initial congergence.

    - A long term solution is to make the router behave as if its restarting
      gracefully (like when graceful-restart is configured). This would make
      the DUT wait for the configured neighbors to come up and send EORs
      before bestpath calculations are made and Updates sent out. Since we
      don't explicitly advertise graceful-restart restarting-speaker
      capabilities when NSR is configured, the peer may not send an EOR.
      Therefore additional mechanisms (like a timeout) can be utilized to
      achieve the desired effect.

5.10 Future kernel optimizations
--------------------------------
With this new scheme, the Backup is not processing the snooped BGP Updates.
Its just reading them and throwing them in the floor and accounting that the
Update was sent to a particular peer. The Update processing happens through
the enqueued Update in the Update-hint message. Reading and throwing away the
Updates is additional overhead to the Backup RE (both rpd and kernel) in terms
of cpu processing. 

Some optimizations are possible in this area:

    - TCP delivers just notifications about received updates. It could be one
      notification that tells the number of Updates received in a snooped
      socket.

5.11 Other alternatives considered
----------------------------------
Out of the may schemes considered, two are worth some discussion.

Cookie approach
---------------
In this approach, the Master sends a cookie along with the the BGP Updates.
This can be considered as a meta-data that is delivered to the Backup RE in
the snooped socket, but isn't sent out in the wire. The cookie can be made to
contain information that relate replicates updates in a peer-group. The Backup
thus needs to process the Update just once and then update the RTOs.

Further optimizations as described in section 5.10 can be made such that not
all the similar Updates are delivered to the Backup in the snooped socket.
Instead just the metadata is delivered.

The downside with this approach is that it would involve reasonable changes
to the Kernel.

ASPATH caching
--------------
The Backup rpd currently employs a caching scheme that caches the NLRIs after
parsing the snooped Update. Subsequent replicated updates match this cache and
therefore nlri processing, nlri->rt lookup, etc is avoided. We currently
don't process the attributes. This processing takes a long time and has to be
done for every replicated Update. 

This scheme can be extended such that the attributes are also cached and
therefore processed only once. Subsequent updates need to be normalized and
will attempt to match the cache and avoid processing overhead.

Normalizing the updates themselves isn't a overly easy task and requires some
processing overhead. The currently chosen scheme seems be slightly
advantageous than this.

6.  PERFORMANCE

Refer [SDCL].

7.  COMPATIBILITY ISSUES

Refer [SDCL].

8.  SECURITY ISSUES

None.


9.  Graceful RE Switchover (GRES), Hobson or ISSU Impact 

The functionality will support GRES and will also be supported on TX.

There is no impact on GRES or TX as result of this functionality.

This feature will not impact ISSU.

10. NSR Impact

This functionality directly changes the NSR implementation.

The system should exhibit the same or improved P&S numbers.

11.  Platforms Supported

This feature is disabled on platforms without hard-disk.

12.  SDK Impact

None.

13.  NOTES

None.

14.  GLOSSARY

SDCL        Socket Data Caching Layer
P&S         Performance and Scaling
RT          Router Tester

15.  REVIEW COMMENTS

