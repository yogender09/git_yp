$Header: /cvs/juniper/sw-projects/os/nsr/rpd/bgp/scaling/rli-6748-funcspec-old.txt,v 1.1 2008/10/07 06:01:53 srik Exp $

                            RLI 6748

             NSR: BGP scale & performance improvements

                        Functional Specification

Author: Srikanth Seshadri <srik@juniper.net>
Contributors: Rex Fernando <rex@juniper.net>

Copyright (C) 2008, Juniper Networks, Inc.

NOTICE: This document contains proprietary and confidential
information of Juniper Networks, Inc. and must not be distributed
outside of the company without the permission of Juniper Networks
engineering.


1.  INTRODUCTION

{As necessary, present an overview of the feature, including:
- a general description of the feature
- who would use it
- why they'd use it
- a high level description of how it works and how one would use it

You MUST reference any relevant requirements, MRDs, PDDs, RLIs,
tracking PRs and/or external protocol definitions (RFCs, IETF drafts,
etc.) and reference documents here for traceability purposes.}

As part of BGP NSR scalability and performance improvements, RLI 6386
implemented the Socket Data Cache Layer (SDCL). This was aimed to
address the problem of the backup BGP being slow to parse the RIB-OUT
updates from the snooped socket, which backpressures the Master tcp from
sending out updates thereby incerasing the convergence time. The SDCL
implements a cache such that data from the snooped socket can be cached
on the backup RE (either in memory or on disk) as fast as it arrives. The
Master socket send thus isn't blocked. The application can read and process
the snooped data at its own pace. For a detailed description of this
feature, refer [SDCL].

While testing BGP scale with SDCL, it was observed that the steady state NSR
performance is excellent (similar to non NSR performance). But there were
issues in the backup rpd while processing large amounts of cached data.
This would be the typical case in the following scenarios:
    - initial convergence
    - session flaps
    - If a RE switchover happens during one of the above scenarios 

This RLI attempts to determine where the bottlenecks are in the current
scheme and tries to address them.

    Target product        : JUNOS
    Target JUNOS release  : 9.4R1
    RLI                   : 6748
    NPI Program           : 326 JUNOS Non-Stop Routing (NSR)
    Tracking PR           : 290971

Referenced documents and RLIs

[SDCL] - Socket Data Cache Layer (NSR: Kernel Performance Improvement), 
         RLI 6386


2.  FUNCTIONALITY

There are no externally visible functional changes with this RLI.

2.1 Changes to RPD Internal Behavior
------------------------------------
Internal enhancements in the SDCL layer and the interaction between rpd and
the SDCL library would fall under the scope of this RLI.


2.2 CLI Changes
---------------
There are no externally visible changes. Certain tunable parameters
to optimize the internal algorithms could be exposed as hidden commands.
They would be present under:

[edit protocols bgp nonstop-routing-cache]

and will be hidden.

3.  CAVEATS

L3VPN performance improvements are not specifically targeted. Depending
on the resources available, they could be tested and the belief is these
changes would have a positive effect on L3VPN scaling. But if additional
issues are discovered during the testing or if there aren't enough
resources for testing, addressing L3VPN scaling issues cannot be guaranteed 
as part of this RLI.


4.  OTHER REQUIREMENTS

Refer the Functional Specification for RLI 6386.


5.  IMPLEMENTATION DETAILS

From discussions and data analysis, one of the ways to address the
current SDCL bottlenecks is to handle simultaneous read (read from socket/
cache) and write (caching) efficiently therby increasing the performance.

The following are some of the issues which would need detailed analysis
and could provide a positive improvement in performance.

5.1 Seeking

Read
----
Draining socket data from the disk_file can be achieved in multiple ways.

    1. we can round robin sequentially over all the sockets that have data to
       be drained and read and process data until the "drain-fairness-ratio".
       Data for each socket could be interleaved in the disk_file and
       reading data socketwise could lead to considerable amount of time
       spent in seeking the data to be read.

    2. An alternative to the above mechanism is to read and process the
       socket data data sequentially as it is contained in the disk file. 
       This could lead to better read throughput since the reads are
       sequential.
      
       This could lead to "unfair/uneven" processing of data among the
       various snooped sockets, but since this is happening on the backup RE,
       there shouldn't be any downside due to this behavior.

Write
-----
The VBL code currently maitains a freelist of the "blocks" from which it
allocates new blocks required for writing (caching to the disk_file). When
sockets are in gulp mode, socket data is written to the file sequentially.
When drain occurs, some of the blocks in the disk are freed and they are
prepended to the freelist. Subsequent allocation for gulps are done from the
start of this freelist. This type of allocation helps in reducing the
size of the disk_file, but could lead to increased seek time in some scenarios.

Test data hasn't indicated any performance issue in the current algorithm,
but this might be revisited in the future.

5.2 Switchover Performance

If a RE Switchover occurs in the middle of convergence (when there are lots
of updates being queued to be sent out to peers), there are known issues
in this area.

5.2.1 Session flaps

The snooped data from the jsr socket has to be drained before any new
updates can be sent out or data received. The drain process takes a
long time and this causes the bgp session to flap since we are unable to
send out updates or keepalives to the peer during this time. One area to
consider is to make the jsr socket ready early on to send keepalives out
to prevent the bgp session from flapping.

5.2.2 Reacting to updates

Related to the above area to make the socket ready early on to receive
and react to incoming bgp updates. This could be tricky to implement
without building the RIB-OUT state for a peer since we need to know what
was previously sent to the peer before sending out a new update message.

5.2.3 Socket drain

As part of the switchover processing, for every peer in a peer group, bgp
drains any pending updates on the socket (not yet snooped) synchronously
and processes them. These updates are currently automatically capped to 
the socket buffer size (16k as an example). With sdcl caching, this is
a variable value and could be arbitrarily large depending on the number of
updates sent to a peer. The worst case is a Switchover during initial
convergence where sdcl was in the gulp stage (or just finished gulping and
entered drain stage) for all sockets. A mechanism needs to be implemented
to handle this gracefully.

5.3 Buffered i/o

The current write throughput is around 4 MB/s while the current read
throughput is around 2 MB/s. Some testing has suggested that buffered i/o
seems to produce better read/write throughput. Further, the default buffer
size (as opposed to specifying various buffer sizes) produces the best
throughput. Testing was done on standalone test programs using netbsd.
This area needs more investigation.

5.4 Data copies

Need to study if any existing copy mechanisms are having a negative
effect and if so, whether some can be reduced or eliminated. An example
would be reading of 16k data from the disk_file to vbl_cache and copying
4k into bgp's buffers everytime bgp does a read operation. The alternatives
to consider would be to share this buffer.

5.5 partition management

The disk_file shares the disk partition with the rest of the data (eg:
logfiles, etc.). This isn't currently seen as an issue since in general,
there isn't lots of disk activity happening on the Standby RE. The current
tests themselves collected lots of data through logfiles in the same
disk partition. If more data collected through adding statistics indicate
otherwise, this might need to be revisited.

As the disk blocks get allocated and freed, fragmentation could occur
thus resulting in the disk_file being non-sequential physically in the disk
in terms of the actual disk blocks.  This could result performance impact 
due to disk seeking.

An alternate mechanism is to create a raw partition and manage the disk blocks
within the library so that we have a finer control of arranging the
disk_file to be in contiguous physical disk blocks reducing the performance
impact due to seek time. This mechanism could have some drawbacks in terms
of losing the caching provided by the file system.

5.6 RAMDISK usage

Current testing so far has shown that the disk_file doesn't grow beyond
4GB or so in size with the target amount of routes and peers. For 64-bit
architectures (Safari, Absolut, etc.), an alternative to consider would be
to implement the disk_file in the RAMDISK (in memory disk). This would
result in really fask access times. Further, there won't be a "seek-time"
impact due to the RAMDISK being random access.

5.7 BGP standby caching

Since the sdcl caching is done in a library and doesn't have any intelligence
about bgp peer groups, if the bgp drain scheduling is such that there are
lots of bgp standby cache misses, this would increase the per packet
cpu processing time on the backup causing the drain to be slower.

Data from 500 peers with 500k prefixes has shown very good cache
hits (85% and above). Therefore this doesn't seem to be an issue. Need to
analyze the data with 1000 peers with 1 million prefixes.

5.8 Others

- Currently, detailed logs are turned on and they are parased post test
  completion to produce various detailed reports/graphs to analyze
  performance. An alternate to consider is to add more statistics collection 
  that can be displayed while the test runs (reducing runtime in an
  iterative process) as well as post completion.
- Currently, the gulp-fairness ratio is 0 (we gulp all the data before
  giving a chance to drain). This parameter seems to produce the best 
  performance. In some architectures though, this implementation produces
  some idle cpu times (gulp operation involves more i/o (dma) time) that
  could be useful for drain processing.
- Investigation into drain time

Testing methodology
-------------------
1. Router Tester simulation

   The DUT will have an ebgp peer capable of sending 256k, 500k or 1 million
   prefixes. These prefixes will be distributed to a set of 250, 500 or
   1000 ibgp peers. Different peer group sizes will be tried including
   the worst case scenario of all the ibgp peers configured under a single
   peer group.

2. Real life testing
   The ebgp router-tester will be replaced by an internet feed. This will
   produce lesser prefixes per update message due to a varying path
   attributes. 
   

Notes
-----
1. The initial sdcl design implemented one disk_file per socket. It was
   determined that the i/o performance exponentially degrades as the number
   of files increases. Therefore, the design was modified to use a single
   file.

2. The issue with socket close has been addressed. A retry has been inroduced
   when ENOENT is returned.


6.  PERFORMANCE

Refer [SDCL].

7.  COMPATIBILITY ISSUES

Refer [SDCL].

8.  SECURITY ISSUES

None.


9.  Graceful RE Switchover (GRES), Hobson or ISSU Impact 

The functionality will support GRES and will also be supported on TX.

There is no impact on GRES or TX as result of this functionality.

This feature will not impact ISSU.

10. NSR Impact

This functionality directly changes the NSR implementation.

The system should exhibit the same or improved P&S numbers.

11.  Platforms Supported

This feature is disabled on platforms without hard-disk.

12.  SDK Impact

None.

13.  NOTES

None.

14.  GLOSSARY

SDCL        Socket Data Caching Layer
disk_file   disk cache created by SDCL
P&S         Performance and Scaling
RT          Router Tester

15.  REVIEW COMMENTS

