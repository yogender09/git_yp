$Header: /cvs/juniper/sw-projects/os/nsr/rpd/bgp/scaling/rli-6748-funcspec.txt,v 1.4 2009/08/03 21:20:08 srik Exp $

                            RLI 6748

             NSR: BGP scale & performance improvements

                        Functional Specification

Author: Srikanth Seshadri <srik@juniper.net>
Contributors: Rex Fernando <rex@juniper.net>

Copyright (C) 2009, Juniper Networks, Inc.

NOTICE: This document contains proprietary and confidential
information of Juniper Networks, Inc. and must not be distributed
outside of the company without the permission of Juniper Networks
engineering.


1.  INTRODUCTION

{As necessary, present an overview of the feature, including:
- a general description of the feature
- who would use it
- why they'd use it
- a high level description of how it works and how one would use it

You MUST reference any relevant requirements, MRDs, PDDs, RLIs,
tracking PRs and/or external protocol definitions (RFCs, IETF drafts,
etc.) and reference documents here for traceability purposes.}

In order to improve the BGP NSR performance and scale, various approaches
were implemented that didn't offer the expected improvements. Please
refer to older functional/design specifications of RLI 6748 (checked into
this directory) regarding the shortcomings of the previous approaches.

The Master rpd generates a given update once and replicates it to all the
peers in a peer-group. The Backup has to process each of this update
individually and therefore the cpu utilization in the Backup rpd increases 
linearly with the number of peers in a peer-group.  The latest attempt was
to employ a hybrid approach in which the Master rpd sends 'Update-hints'
through a separate out-of-band channel to the Backup.  These hints contain
the set of peers to which the Master generates a give update. The Backup
would then process the Update-hint just once and note down the peers to which
this update was generated. When the associated real updates are snooped, the
Backup doesn't need to do any Update processing other than recording the peer
to which the Update was generated. When the Updates have been snooped for all
the peers in a given Update-hint, it can set the rib-out state for all the
peers in one-shot.

The above implementation didn't produce the expected results. The reason
was the send-side algorithm in the Master rpd wasn't working as designed.
The peers in a given peer-group were out-of-sync most of the time and
therefore the replication ratio was very poor. (between 1 and 2 on an average
peer-groups of 50 and even more). A fix was attempted for this, but
it turned out to be very hard to fine-tune this for all platforms in a short
period of time. This effort itself would require a separate RLI, if pursued.

The approach that seems to produce positive results is some modifications
in rpd to kernel interaction with respect handling writes to the TCP sockets
(that repsent BGP peers). There are also a few changes required in the
Kernel to scale to a higher number of peers.

    Target product        : JUNOS
    Target JUNOS release  : 10.1R1
    RLI                   : 6748
    NPI Program           : 326 JUNOS Non-Stop Routing (NSR)
    Tracking PR           : 290971

Referenced documents and RLIs

[SDCL]         - Socket Data Cache Layer (NSR: Kernel Performance Improvement), 
                 RLI 6386

[SDCL-IMPROV]  - Improvements to the SDCL approach described in 
                 sw-projects/os/nsr/bgp/rli-6748-funcspec-old.txt

[UPDATE-HINTS] - BGP NSR Update-Hints approach described in
                 sw-projects/os/nsr/bgp/rli-6748-funcspec-old2.txt

2.  FUNCTIONALITY

This RLI targets improvements in scale and performance. There are no
externally visible functional changes.

2.1 CLI Changes
---------------
There are no externally visible changes.

Certain performance statistics meant for monitoring and debugging can be
displayed using the

"show bgp statistics" 

command both in the Master and Standby REs.

2.2 Scale
---------
Currently supported scale for family inet:
    - 500 peers
    - 7 or 8 peer-groups
    - 500k prefixes
    - 1 million paths

Target scale with these changes:

There are two limiting factors for scaling and performance.

Memory: rpd can grow until 2GB of virtual and physical memory. By design,
the Backup RE consumes more memory than the Master to store RIB-OUT State.
This additional memory consumption is dependant on the number of prefixes
(Actives), number of peer groups.

CPU: The Backup rpd needs to do more work when compared to the Master rpd.

We've tested with the following scale for family inet:
    - 1000 peers
    - 7 or 8 peer-groups
    - 1 million prefixes
    - around 2 million paths

At this stage, the Backup does have remaining CPU cycles and some amount (.5G)
of memory. During systest, the peer scale can be increased to find out the
maximum that can be supported.

2.3 Target platforms
--------------------
Faster RE's with 4GB of RAM (eg: RE2000) would offer the best scale and
performance. This would allow rpd to consume the full 2GB of memory without
any impact due to the memory consumption of kernel and other processes.

The low-end platforms (eg: m10i) have been sanity tested for regressions.

3.  CAVEATS

L3VPN performance improvements are not specifically targeted. Depending
on the resources available, they could be tested and the belief is these
changes would have a positive effect on L3VPN scaling. But if additional
issues are discovered during the testing or if there aren't enough
resources for testing, addressing L3VPN scaling issues cannot be guaranteed 
as part of this RLI.


4.  OTHER REQUIREMENTS

None.


5.  IMPLEMENTATION DETAILS

When NSR is turned on, the TCP sockets are record-boundary based. If there
isn't enough space in the send-side socket buffer for a given write()
operation, the operation fails and the application buffers the data. For
every successful write() operation, the kernel treats that amount of bytes
written as one PDU (record) and replicates it to the Backup. There is
a fixed overhead cost associate for this replication. In addition to the
overhead in send-side, this includes a separate ACK from the Backup for
each replicated PDU and the associated processing. Its beneficial to
reduce this overhead by increasing the size of the PDU. This is currently
done by the BGP 'gather' module. This module itself gathers around
512 - 768 bytes worth before attempting a write.

5.1 Gather size
---------------
Increasing the gather size produced an excellent effect in reducing the
kernel cpu and interrupt utilization allowing more cycles for rpd or
other processes. Various sizes were tried out and the value of 8192 seems
to produce the best results. This value worked well both in low end (m10i)
and high end (M320, T640 utilizing RE types RE4, RE2000) platforms.

5.2 Send-side socket low-watermark
----------------------------------
When a socket gets send-blocked (write operation returns EAGAIN), the data
from the attempted write() is buffered until space becomes available. We
don't register a low-watermark since in the case of non-NSR, the TCP socket
is a stream and whenever the kernel tells the application that space is
available, the application can write some bytes (even though the complete
buffer may not be written out). In the NSR case, this approach is wasteful
since nothing can be written out if there isn't enough space available to
empty the complete buffer contents. Even though the current code would
try to set a low-watermark if the subsequent write operation (after a
write-ready callback) fails, it wasn't considering the data that could be
buffered within the gather buffer in doing so.

5.3 Gather timer writes
-----------------------
The gather timer-write fires periodically (every 250ms by default) and
attempts to flush the gather buffer. This needs to be hooked in with the
write-ready notifications. Otherwise, there could be quite a bit of wasted
system calls if we consider the scale. As an example, there could be 4 system
calls per second per peer. 1000 peers could mean 4000 system calls per second.

5.4 Kernel changes
------------------
In order to scale, there were a few neccsary kernel changes required. They
are all to do with various timeout values for acknowledgements expected
from the Backup rpd for various types of data synced. When a piece of
information needs to be replicated (eg. application socket data which
uses SDRL), this timer is started when the application writes the data
in the socket instead of when the data is actually sent to the Backup.
But there could be quite some delay in sending thsi data to the Backup
if the KKCM Socket (RE-RE kernel socket) itself is send blocked. The
approach taken for testing was to increase these timeout values so that
they don't cause false alarms and end up de-synchronizing the socket.

The following are the timers (timeout values in seconds):

                            old-value       new-value
JSR_IHA_MSG_TIMEOUT         1               10
SDRL_REPL_TIMEOUT           5               60
JSR_PRL_TIMEOUT_VAL_C       10              60

IHA  - used for initial session syncing
SDRL - socket send-side data replication
PRL  - receive side data replication

The kernel team will be fine-tuning these values and checking in the changes.

5.5 Keepalives
--------------
When keepalive timer expires, BGP forces a socket write even though there
could be data buffered; in other words, the socket is 'send-blocked' from 
rpd's point of view. One of the reasons for doing this is that rpd may not
have processed the write-ready notification for this socket yet due to
high cpu utiliazation and skipping the force write could end up flapping
the session if this peer wasn't serviced for a long time. There is no
change in semantics of this behavior.


5.6 Asynchronous socket unreplications
--------------------------------------
When the kernel decides to unreplicate a socket for any reason (eg: failure
of an Ack to arrive on time), this is an action initiated by the Kernel
and rpd today is unaware of this. From the Master rpd's perspective,
the communication to the peer is up. But its unware that the socket has been
unreplicated and state is not synchronized to the Backup.

Kernel provides asynchronous notifications for such events. rpd needs to
handle these and should attempt to replicate the peer again.

This should be considered in the future if such unreplications occur.
With the chosen timeout values, there haven't been any such occurances
during testing.

5.7 Future kernel optimizations
-------------------------------
The cpu utilization by the kernel especially during initial convergene or
when a 'clear bgp neighbor' is performed needs to be investigated for
any possible improvements. A sliding-window protocol could be considered
for reducing the Acks by the Backup.

5.11 Other alternatives considered
----------------------------------
Out of the may schemes considered, two are worth some discussion.

6.  PERFORMANCE

This RLI is directly related in increasing the scale and scale. The
below table shows two scenarios and compares the performance
in Non-NSR, Stock-NSR and the improved performance with these changes.

Clear bgp neighbor soft
-----------------------
* updates are refreshed to neighbors

            Master CPU util  %  Mem  Backup CPU %        Mem  Congerged
            Usr  Sys  Int  Idl  MB   Usr  Sys  Int Idl   MB    min:sec

non-nsr     49.2 7.1 13.5 28.4  xxx   - n/a -            n/a  11:15
non-nsr     48.0 6.0 13.2 31.0  692   - n/a -            n/a  10:20

Stock NSR   43.5 24.3 17.8 12.6 745  48.6 16.6 12.2 20.8 942  10:30
Stock NSR   44.1 23.7 17.7 12.7 745  47.3 17.2 12.5 21.2 942  10:30

fixed NSR   46.7 17.3 14.5 19.7 807  44.3 8.6  10.6 34.7 943  9:45
fixed NSR   45.6 18.6 14.3 19.6 xxx  45.1 9.1  10.7 33.3 xxx  9:45


Clear bgp neighbor
------------------
            Master CPU util  %    Backup CPU %       Congerged
            Usr  Sys  Int  Idl    Usr  Sys  Int Idl   min:sec

non-nsr     40.8 12.7 12.9 31.1    - n/a -             11:41
non-nsr     41.0 12.6 12.6 31.2    - n/a -             11:43

stock NSR   40.6 23.3 15.4 19.0   44.2 15.9 11.4 26.8  < 13
stock NSR   40.9 26.0 16.1 14.8   47.8 16.2 12.9 21.4  12:15

fixed NSR   39.5 20.2 13.8 24.5   46.0 12.0 11.7 28.9  10:55
fixed NSR   42.4 23.7 16.4 15.0   51.7 15.6 14.0 17.0  11:11
fixed NSR   44.2 21.8 16.2 15.5   47.6 12.2 11.9 26.7  xxx

7.  COMPATIBILITY ISSUES

None.

8.  SECURITY ISSUES

None.


9.  Graceful RE Switchover (GRES), Hobson or ISSU Impact 

The functionality will support GRES and will also be supported on TX.

There is no impact on GRES or TX as result of this functionality.

This feature will not impact ISSU.

10. NSR Impact

This functionality directly changes the NSR implementation.

The system should exhibit the same or improved P&S numbers.

11.  Platforms Supported

No impact.

12.  SDK Impact

None.

13.  NOTES

None.

14.  GLOSSARY

SDCL        Socket Data Caching Layer
P&S         Performance and Scaling
RT          Router Tester

15.  REVIEW COMMENTS

