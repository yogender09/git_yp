                 NSR: Kernel Performance Improvement 

                              RLI 6386

                       Functional Specification

          -----------------------------------------------------------

Author: Luc Bazinet <lbazinet@juniper.net>
Author: Alireza Assadzadeh <aassadza@juniper.net>
Author: Hariprasad Shanmugam <hshanmugam@juniper.net>


Copyright (C) 2008, Juniper Networks, Inc.

NOTICE: This document contains proprietary and confidential information of 
Juniper Networks, Inc. and must not be distributed outside of the company
without the permission of Juniper Networks engineering.

1.  INTRODUCTION

This functional specification describes the changes targeted for JUNOS
9.3 for RLI 6386 to improve performance and scaling (P&S) numbers for 
NSR. 

Although the scaling and performance requirements depend on various
factors such as workload, scenario, configuration, hardware, etc., there
is expectation that the current P&S numbers should be improved as they
are not meeting customer requirement and that customers are regularly
asking for better performance. Furthermore, NSR enabled P&S numbers are
compared with NSR disabled numbers which are used as a benchmark. The
goal is for the P&S numbers for these two cases to be close to each
other; in some cases 10% overheard has been noted as acceptable.

The primary consideration of this work is for BGP with peer groups. Other
routing protocols and JSR users may also benefit from this infrastructure
work.

One limitation for the BGP scenario is that during large burst update
event the router has high convergence time. This effectively reduces the
supported P&S numbers. This is the case that the router receives the
updates and has to update all its peers in the peer groups.

The analysis suggested that the bottleneck is within the backup RE. The
rpd on the backup RE is not draining the kernel replicated(snooped) socket 
buffers quickly. This then results in pushback from the backup RE to 
the master RE. This slows down progress on the master RE to send the BGP
updates to its peers. Refer to section 5 for more information.

Several options were considered to optimize the system in this situation.
The option that is implemented is described in this functional
specification (FS). Contrary to the title of the RLI and FS, there are
no changes to the kernel. The changes are made in rpd to help the kernel.

The Socket Data Cache Layer (SDCL) is a library that implements a socket
data buffering mechanism for NSR applications. This library can be used in NSR 
situations where the replicated data stream is too large to be held in memory.
The SDCL is able to cache incoming socket data to the disk, and pass it up to
the caller in time order, after an idle delay period.

SDLC is new and provided by this RLI. It is used by BGP in this release.
Other protocols and applications could use it in future releases.

NSR functionality has been provided several releases prior to JUNOS 9.2.
BGP was one of the first protocols that used NSR and benefited from it.

This functionality is tracked RLI 6386.

The URLs are:

https://deepthought.juniper.net/app/do/showView?tableName=RLI&record_number=6386&view=markup

RLI: 6386
Tracking PR: 276683

2.  FUNCTIONALITY

2.1 Changes to RPD Internal Behavior
------------------------------------

All the changes pertain to the rpd on the backup RE. The rpd processing
on the master RE is not modified.

The following describes the changes to rpd on the backup RE.

The rpd needs to drain the kernel socket buffers more quickly. This then
removes the pushback from the backup RE to the master RE through JSR.
This frees up the sockets on the master RE. As a result, on the master
RE, the rpd can push more data through the sockets.

In order for rpd on the backup RE to drain the socket more quickly, it
needs to read the data on the sockets with minimum amount of processing
them, since the extra processing is time consuming and is reducing the
throughput.

The rpd on the backup RE uses memory buffers within rpd to read the data
from the sockets. If the rpd memory buffers are exhausted, the hard-disk
is used as secondary storage. It is assumed that because of the large
volume of the data in scaling setup, the use of hard-disk will be
necessary.

2.2 CLI Changes
-----------------

The following CLI changes are added to enable SDCL for BGP and specify
some tunable parameters:

[edit protocols bgp nonstop-routing-cache]
    cache-element-size    Per-socket in-memory cache buf size, in kilobytes(1024).
    start_trigger_delay   Number of seconds to pause, before writing to disk.
    gulp-fairness-ratio   Number of times to fill in-memory buf, before a single-buf drain.
    end_trigger_delay     Number of seconds to wait for idle traffic, ik
    drain-fairness-ratio  Number of times to fill in-memory buf, before yielding.

Below are the permitted ranges, and defaults:

    cache-element-size  (range 4 .. 100)
                        (default 16)
                                                                                                                         
    start_trigger_delay (0 .. 1000)
                        (default 20)

    gulp-fairness-ratio (1 .. 100)
                        (default 8)

    end_trigger_delay   (1 .. 100)
                        (default 15)

    drain-fairness-ratio (1 .. 100)
                        (default 8)


This change SHOULD not negatively impact the P&S numbers. 

2.3 Socket Data Cache Layer (SDCL)
----------------------------------

The SDCL feature is enabled by passing an sdcl_setup_t to sdcl_init().
All the buffer sizes and wait times described below are configured by
values in the sdcl_setup_t.  Many of these parameters can be user-configurable 
CLI items, or they can also be hard-coded by the application.

The calling application must call sdcl_socket_init() for each send-snoop socket
that it wants to be cached by SDCL. The sdcl_socket_init() function allocates an
in-memory buffer and a creates a unique hard disk cache file for each NSR Tx-snoop 
socket that it is managing.

For purposes of example, let's say that a 16K buffer is allocated, which 
is four times larger than the (4K) bgp buffer used by rpd at the moment.

2.4 Cache Operating Modes
-----------------------------
At any given time for a group of sockets, SDCL is operatng in one of 
two cache modes. These are modes are:

1) memory-mode: priority is given to reduce the disk usage to the
                extent that all processing is in memory.

2) disk-mode: priority is given to reading from the socket which 
              results in storing packets to disk.

While the these modes are mutually exclusive, SDCL can switch between
these operating modes depending on the protocol data pattern and the 
configuration parameters.

When SDCL first starts, it acts as an in-memory pass-through (memory-mode), and 
only uses its in-memory buffer.  While the data coming in from the socket is
processed quickly enough, such that the in-memory buffer does not get full, all 
the socket data remains in-memory . (In our example, the in-memory buffer is four 
times larger than bgp's own 4k buffer. This permits short peaks of traffic
without activating disk-mode).

If the in-memory buffer gets full (start-trigger), the SDCL operating mode changes
to be file based (disk-mode). Once this mode is activated, all subsequent data is 
written to the hard disk, before the data is passed up to the caller. This way, if 
the caller cannot keep up, the hard disk cache file will grow.

SDCL remains in disk-mode until a configurable idle period is seen in the socket
data stream. This idle period is called the (end-trigger).

The occurence of the end-trigger, causes a task_job scheduling emphasis to
switch from keeping up with the incoming socket data stream (gulping), to one which 
favors the retrieval of the data by the application (draining).

All the data from the hard disk cache file is given to the caller in
time-order.

SDCL remains in disk-mode, until all the cached file data has been read by
the calling application. Once this occurs, the hard disk cache file space is reclaimed,
and SDCL returns to memory-mode.

3.  CAVEATS

None.

4.  OTHER REQUIREMENTS

Loosely speaking, the target performance and scaling number is to support
1M active routes with 500 BGP peers in peer groups on high end M/T
routers (M320 and T-series).

The burst update for this case can result in 15 GB of data to be received
by the router over 15 minutes.

For the high P&S setups, the router is required to have the maximum
amount of physical memory that is currently supported. This is 4GB.

5.  IMPLEMENTATION DETAILS


5.1 Design Assumptions
----------------------

 1) The calling application has an existing task_job (orig_task_job) that
    is of the following form:

      part 1: read data from the send-snoop socket
      part 2: process the data.
   
 2) The scheduling of this task_job is done by the task_set_receive()
    mechanism which creates the orig_task_job when data arrives on the socket.

5.2 Approach
------------

SDCL is implemented as interoperating task_jobs.

The SDCL approach is to substitute the task_job that is associated with
the send-snoop socket. This way, when data arrives on the socket, a 
new (sdcl_task_job) is created instead of orig_task_job.

This new task_job (sdcl_task_job) intercepts the socket data and places it in
the SDCL cache.

In order to pass the cached socket data back to the application, the 
orig_task_job is created periodically by the sdcl_task_job.

To retrieve the cached socket data, the orig_task_job needs to call
sdcl_cache_read(), rather than read().

For the bgp implementation,
  orig_task_job is bgp_standby_socket_read(), and
  sdcl_task_job is bgp_standby_sdcl_socket_read().

5.3 Design goals
----------------

 1) Use only one in-memory buffer for both task_jobs
    This minimizes applcation memory requirements when the SDCL feature
    is enabled.
   
 2) Try to use the SDCL in-memory cache as much as possible.
    (corollary: try to avoid using the file-cache, unless necessary)
    This minimizes SDCL cpu impact, while in quiescent or normal traffic 
   loads.

 3) Truncate/re-open hard disk cache file when empty.
    This minimizes the disk space requirements while in quiescent or
    normal traffic loads.


5.4 Details of Operation
------------------------

See Figure 1.

Data arrives on the send-snoop socket and creates sdcl_task_job, which 
reads that data into its in-memory cache buffer.

5.4.1 Memory-mode
------------------

If the buffer is not full, and there is no more socket data, then the
orig_task_job is created in order to process it. SDCL stays in this mode
until the in-memory cache buffer actually fills completely (start-trigger).

If the sdcl_task_job gets scheduled, and it read all the available data
on the socket without filling its buffer, it runs the orig_task_job.

In memory-mode, there is a 1:1 relationship between the sdcl_task_job
and the orig_task_job. Each job gets scheduled equally.

The only way to get out of memory-mode is to fill the SDCL in-memory buffer.

5.4.2 Disk-mode
----------------

If the buffer is full, then disk-mode starts, and the buffer is written to the
disk cache file.

There are two phases to disk-mode. The first is the gulp phase, where the
sdcl_task_job is mostly caching data to disk. The second phase is the drain
phase where the orig_task_job is mostly reading and processing the data.

5.4.3 Gulp Phase
-----------------

In this mode, the orig_task_job is scheduled much less frequently than the
sdcl_task_job, in order to read the send-snoop data from the socket as fast 
as possible.

However, since the scheduling of the orig_task_job contains the code to 
process the data, that job must be scheduled periodically to avoid application
timeouts, such as bgp flaps.

This continues until an idle period occurs in the data stream which is used as
the (end-trigger) to start favouring the scheduling of the orig_task_job.

5.4.4 Drain Phase
------------------

During this phase, SDCL is mostly draining the hard disk cache file data by
scheduling the orig_task_job.

Since SDCL is in disk-mode, if new data comes in, it goes to disk, and gets
processed in time-order with the existing data in the cache file.

The only way to get out of disk-mode is to schedule the orig_drain_job until
it has processed all the pending data in its SDCL cache file. Once there
is no more unread data in the cache file, the SDCL mode switches back to
memory-mode.


5.5 Characterization of the data flow
--------------------------------------

In the scenario for which SDCL was principally designed, there are several
hundred task_jobs simultaneously all processing incoming data on different sockets
(e.g. containing bgp updates).

For this situation to function properly, the task_jobs have to yield
periodically, otherwise they consume too much cpu time and other task_jobs
end up being starved.

5.5.1 Start-trigger
--------------------

Since SDCL does not have any foreknowledge of a large update burst
arriving on all its sockets, it uses the fact that its in-memory
buffer has filled as a trigger to start using the file-cache.

5.5.2 End-trigger
-----------------

Similarly, since SDCL does not have any foreknowledge of when the large
update burst has finished, it uses the fact that i/o on a socket has gone
idle for a (configurable) period of time. 


Figure 1: SDCL data flow
-------------------------

        (start-trigger)
             memory
             buffer 
              full
               |
|<--(memory)-->|      |<------------ (disk-mode)----------->|
     mode)     |
               V 
   in-memory     
|--------------| start
               | trigger
               | delay
   ^           |++++++|    gulp
   |                  |    phase
   |                  |-------------| end 
   |                                | trigger
   |                                | delay
 sdcl_task_job          ^           |++++++++|
 lets orig_task_job     |                    |    drain
 run every time all     |                    |    phase 
 the socket data        |            ^       |--------------|
 is read. Essentially   |            |                      | in-memory
 a pass-through.        |            |        ^             |------------- ...
                        |            |        |
                        |            |        |             ^
              sdcl_task_job          |        |             |
              runs most of the       |        |             |
              time, while letting    |        |             |
              orig_task_job run      |        |             |
              every time "n"         |        |             |
              in-memory buffers      |        |             |
              are read from the      |        |             |
              send snoop-socket.     |        |             |
              ("n" is                |        |             |
               gulp-fairness-ratio)  |        |             |
                                     |        |             |
                           Incoming data on   |             |
                           send snoop-socket  |             |
                           has gone idle for  |             |
                           "delay" seconds.   |             |
                             (end-trigger)    |             |
                                              |             |
                                              |             |
                                   orig_task_job            |
                                   runs most of the         |
                                   time, while yeilding     |
                                   everytime "n" in-memory  |
                                   buffers have been        | 
                                   read from the SDCL       |
                                   cache file.              |
                                   ("n" is                  |
                                    drain-fairness-ratio)   |
                                                            |
                                                 All the data in the
                                                 SDCL cache file has been 
                                                 passed to orig_task_job.
                                                 Clean up cache file.
                                                 Switch back to memory-mode.


5.6 Fairness Mechanisms
-----------------------

Due to the nature of task_job scheduling within rpd, a number of fairness
mechanisms were incorporated into the SDCL implementation.

5.6.1 Start-trigger Delay (on entry to disk-mode)
-------------------------------------------------

Issue:
------
In testing it was observed that, when an update burst begins, 
the distribution of task_jobs entering disk-mode is uneven.

With several hundred task_jobs, those that enter disk-mode
get too far ahead of those whose buffer has not filled yet,
and are still in-memory mode. 

The observed behavior is that some task_jobs are able to 
finish processing the entire update burst, while others are still in 
memory-mode. This behavior does not acheive the desired scheduling
distribution, as it does not adequately take advantage of the bgp 
standby cache and it also causes bgp flaps.

Solution:
--------

In order to force all task jobs to be more-or-less in the same
state, at the same time, a "pause" is done when switching from
memory-mode to disk-mode. (e.g. 20 seconds).

This permits all the task_jobs that are still in memory-mode 
to catch up.  Since those task_jobs that have entered disk-mode
have paused, all the other task_jobs that are participating in 
the update burst get a chance to continue running. This way,
they get a chance to fill up their in-memory buffer, and also
transition to disk-mode SDCL operation.

This is a tunable paramater. 

Since the period of time required to receive all the data
in the update burst is measured in minutes, this delay
does not end up increasing the overall completion time,
and does result in an equalization of the SDCL load.

In fact, it reduces the overall period of time for all the task_jobs
to complete, and is a factor in reducing bgp flaps.

However, if there are not very many peers, perhaps this pause 
would not be necessary. (The SDCL mplementation does not dynamically
adjust any of its tunable paramaters after they are set.)

5.6.2 Fairness ratios
----------------------

Recalling that disk-mode has two phases, gulping and draining, different
mechanisms are need to create fairness of scheduling.

5.6.2.1 Gulp-fairness-ratio
----------------------------

In order to periodically give some data to the calling application,
the sdcl_task_job must schedule the orig_task_job on a periodic basis.

While in disk-mode, in gulp phase, the sdcl_task_job terminates itself
and creates the orig_task_job in order to let it read some of the
pending data in the SDCL cache.

This exec()ing of task_jobs occurs every time "n" in-memory buffers are
filled from the incoming socket data stream ( i.e. the gulp-fairness-ratio ).

5.6.2.2 Drain-fairness-ratio
----------------------------

Once the incoming update burst has stopped, the idle delay
causes the disk-mode phase to switch to the drain phase.

In drain phase, the emphasis is on letting the orig_task_job
run in order to read and process the cached file data.

The fairness during the drain phase is done by returning
EBUSY to the caller everytime "n" in-memory buffers have
been given to the calling application. In which case, the
calling task_job is expected to simply return, effectivlely
yielding.

"n" is the drain-fairness-ratio value.

5.7 SDCL API
-------------

SDCL provides a set of API functions for the application to use the
functionality in its own context. Multiple applications using this 
library will be shielded appropriately as each SDCL initialization, 
will get thier own context of SDCL for a group of sockets.

The following are the API functions and its related definitions to 
be used by the application to use the library, followed by a sample 
implementation of BGP sockets using the SDCL library.


/**
 * External Definitions
 */ 

/**
 * Function pointers definitions for timer functions
 */
typedef void (*socket_read_timer_func) (task_timer *, time_t);
typedef void (*cache_read_timer_func)  (task_timer *, time_t);
typedef sdcl_sniff_action (*socket_sniff_func)
				  (sdcl_socket_info_t, char* );

/** 
 * @brief
 * Config params and callback functions common to a group of sockets.
 * Provided by the API user functions to SDCL.
 */
typedef struct _sdcl_setup {

    /* FS options */
    char *file_path;    /* directory location of sdlc files */

    /* tunning options */
    ushort cache_size;   /* memory buffer size */
    ushort start_trigger_delay;
    ushort gulp_fairness_ratio;
    ushort end_trigger_delay;
    ushort drain_fairness_ratio;

    /* handler functions for pausing */
    socket_read_timer_func socket_read_timer_method;
    cache_read_timer_func cache_read_timer_method;

    /* handler for sniffing data */
    socket_sniff_func socket_sniff_method;

} sdcl_setup_t;


/** 
 * @brief
 * Handle for a group of sockets (like all BGP sockets)
 * which have common properties & SDCL functionality is enabled .
 * Created by the SDCL and returned to the caller.
 */
typedef struct _sdcl_handle {
    /* opaque to API user */
} sdcl_handle_t;


/**
 * @brief
 * Per-socket SDCL structure
 */
typedef struct _sdcl_socket_info {
    /* opaque to API user */
} sdcl_socket_info_t;


/**
 * @brief
 * Different operations to be performed by the caller
 */
typedef enum {
    SDCL_SOCKET_READ_CLOSE_RESYNC,
    SDCL_SOCKET_READ_CLOSE_NORESYNC,
    SDCL_SOCKET_READ_CLOSE_TBD,
    SDCL_SOCKET_READ_SETUP_START_TIMER,
    SDCL_SOCKET_READ_SETUP_END_TIMER,
    SDCL_SOCKET_READ_EXEC_DRAIN
} sdcl_socket_postread_op;

/**
 * @brief
 * Different states for sockets
 */
typedef enum {
    SDCL_SOCKET_PAUSE = 1, 
    SDCL_SOCKET_HOLDOFF
} sdcl_socket_state;

/**
 * @brief
 * return values from sniff function
 */
typedef enum {
    SDCL_SOCKET_CONTINUE = 1, 
    SDCL_SOCKET_CLOSE
} sdcl_sniff_action;



/** 
 * API functions
 */ 

/**
 *----------------------------------------------------------------
 *  SDCL functions
 *------------------------------------------------------------------
 */

/** 
 * @brief
 * SDCL initialisation for a group of sockets.
 * validates and sets the configuration parameters from the caller 
 * and returns back a unique handle.
 *
 * @param[in] sdp  
 *    Config values
 * @param[out] sdh 
 *    valid handle, after successful initialization; NULL, 
 *    in case of error
 * @return
 *    @c EOK success; otherwise error
 */
status_t sdcl_init( sdcl_setup_t *sdp, sdcl_handle_t **sdh )


/** 
 * @brief
 * To modify the config values which were set earlier
 *
 * @param[in] sdp 
 *    pointer to config values to reconfigure
 * @param[in] sdh 
 *    handle (created during sdcl_init call)
 * @return
 *    @c EOK success; otherwise error
 */
status_t sdcl_reconfigure( sdcl_setup_t *sdp, sdcl_handle_t *sdh)


/** 
 * @brief
 * SDCL cleanup before closing the socket
 * free all resources related to this handle and clear the pointer
reference
 *
 * @param[out] sdh 
 *    address to pointer will be set to NULL 
 * @return
 *    @c EOK success; otherwise error
 */
status_t sdcl_teardown(sdcl_handle_t **sdh)


/** 
 * @brief
 * check to see if SDCL is enabled for this handle
 *
 * @param[in] sdh 
 *    handle
 * @return
 *     True or False
 */
boolean sdcl_is_enabled(sdcl_handle_t *sdh);


/**
 *----------------------------------------------------------------
 *  SDCL socket functions
 *------------------------------------------------------------------
 */

/** 
 * @brief
 * socket specific initialization - allocates resources needed for this
 * socket. Called once for every socket which needs to use the SDCL
 * functionality.
 * Caller sends in the address to a pointer location to get back the
sdcl_sock_info_t
 *
 * @param[in] sdh 
 *    handle
 * @param[out] sip 
 *    pointer to the address to store the created socket specific
 * @param[in] fds[] 
 *    fds are used as a unique value to name the temporary files
information
 *    NULL, incase of error
 * @return * @brief
 *    @c EOK success; otherwise error
 */
status_t sdcl_socket_init( sdcl_handle_t sdh, 
                           sdcl_socket_info_t **sip , 
                           int fds[] );


/** 
 * @brief
 * SDCL socket cleanup to free up all the resources allocated for 
 * this particular socket
 * @param[in] sip 
 *    socket specific information pointer
 * @return
 *    @c EOK success; otherwise error
 */
status_t sdcl_socket_teardown( sdcl_socket_info_t *sip )


/**
 * @brief
 * SET function for SDCL socket state 
 * @param[in] sip 
 *    socket specific information pointer
 * @param[in]
 *     state to be set
 * @return
 *     void
 */
void sdcl_set_socket_state (sdcl_socket_info_t *sip, 
                            sdcl_socket_state state);


/**
 *----------------------------------------------------------------
 *  SDCL socket I/O functions
 *------------------------------------------------------------------
 */

/** 
 * @brief
 * Reads data from the cache 
 * called to read the data stored in the cache
 * similar to read() call
 *
 * @param[in] sip
 *    socket specific information pointer
 * @param[in] buf
 *    pointer to data buffer to be filled in
 * @param[in] nbytes
 *    bytes to be read from the cache
 * @return
 *    num bytes successfully read from the cache; 0 - error 
 */
ssize_t sdcl_cache_read ( sdcl_socket_info_t *sip, 
                          void *buf, 
                          size_t nbytes);


/** 
 * @brief
 * Reads from the socket when there is data available
 * calling function has to take action based on the return opcode
 * either 
 *     - to kill this job and restart the cache read job
 *     - close the socket (user request or Error)
 *
 * @param[in] sip 
 *    socket specific information pointer 
 * @return
 *     opcode to indicate the next operation to be performed by caller
 */
sdcl_socket_postread_op sdcl_socket_read( sdcl_socket_info_t *sip );


/**
 *----------------------------------------------------------------
 * SDCL set/get functions:
 *------------------------------------------------------------------
 */

These accessor functions encapsulate implemntation of sdcl_socket_info_t

/**
 * @brief
 * set the start_trigger_delay value
 * 
 * @param[in] sip
 *    socket specific information pointer
 * @param[in] tip
 *    task_timer pointer
 */
void 
sdcl_set_start_trigger_delay (sdcl_socket_info_t *sip, task_timer *tip);


/**
 * @brief
 * get the start_trigger_delay value
 * 
 * @param[in] sip
 *    socket specific information pointer
 * @return
 *    task_timer pointer; NULL if no task_timer exists 
 */
task_timer *
sdcl_get_start_trigger_delay (sdcl_socket_info_t *sip);


/**
 * @brief
 * set the start_trigger_state value
 *
 * @param[in] sip
 *    socket specific information pointer
 * @param[in] state
 *    state to be updated
 */
void
sdcl_set_start_trigger_state (sdcl_socket_info_t *sip, int state);


/**
 * @brief
 * set the end_trigger_delay value
 *
 * @param[in] sip
 *    socket specific information pointer
 * @param[in] tip
 *    task_timer pointer
 */
void 
sdcl_set_end_trigger_delay (sdcl_socket_info_t *sip, task_timer *tip);

/**
 * @brief
 * get the end_trigger_delay value
 *
 * @param[in] sip
 *    socket specific information pointer
 * @return
 *    task_timer pointer; NULL if no task_timer exists 
 */
task_timer *
sdcl_get_end_trigger_delay (sdcl_socket_info_t *sip);


/**
 * @brief
 * set the end_trigger_state value
 *
 * @param[in] sip
 *    socket specific information pointer
 * @param[in] state
 *    state to be updated
 */
void
sdcl_set_end_trigger_state (sdcl_socket_info_t *sip, int state);


5.8 Sample API usage
---------------------

1. Application has to call sdcl_init() with the structure sdcl_setup_t 
initialized. when sdcl_init() returns it provides a handle back to 
application, if init is successful.

  sdcl_setup_t bgp_sdcl_setup_params;
  sdcl_handle_t bgp_sdcl_handle;
  status_t ret_val;

  /* file path should exist - SDCL would not create the directory */
  strcpy(bgp_sdcl_setup_params.file_path, "/var/tmp/bgp");

  bgp_sdcl_setup_params.cache_size   = 2048;
  bgp_sdcl_setup_params.start_trigger_delay  = 15;
  bgp_sdcl_setup_params.gulp_fairness_ratio  = 4;
  bgp_sdcl_setup_params.end_trigger_delay    = 15;
  bgp_sdcl_setup_params.drain_fairness_ratio = 4;
 

  /* timer functions */
  bgp_sdcl_setup_params.socket_read_timer_method = bgp_socket_read_timer_method;
  bgp_sdcl_setup_params.cache_read_timer_method  = bgp_cache_read_timer_method;
  bgp_sdcl_setup_params.socket_sniff_method      = bgp_socket_data_sniff;

  ret_val = sdcl_init(bgp_sdcl_setup_params, &bgp_sdcl_handle);
  if (ret_val != EOK)
    /* Error */


2. When a BGP socket is created and when it requires 
   the SDCL functionality:

  sdcl_socket_info_t *bgp_sock1_info; /* for socket1 */

	

  if (sdcl_is_enabled(bgp_sdcl_handle)) {
    ret_val = sdcl_socket_init(bgp_sdcl_handle, 
				bgp_sock1_fds, 
 				&bgp_sock1_info);
    if (ret_val != EOK)
      /* Error */
  }


3. Application reads the data out of the socket

  in function,
  bgp_standby_socket_read(...){

    ...
    if (bgp_sdcl_enabled_for_this_socket(sock1_fds)) {
     sdcl_cache_read(bgp_sock1_info, buf, 1024);

    } else {
     read(..); // direct socket read
    }

    /* handle the SDCL error EBUSY also in addition to other ones
    switch(errno)
    {
	case EBUSY: 
	  if (SDCL enabled on this socket) {
	    // don't worrry - be happy
            break;
	  }
    }
  }


4. Timer functions 

  Two timer function pointers are to be provided during the sdcl_init()

  bgp_socket_read_timer_method(...)  {
    sip = /* get the stored sdcl_socket_info_t from task context */
    sdcl_set_socket_state(sip, SDCL_SOCKET_PAUSE_EXPIRED);
  }


  bgp_cache_read_timer_method (...)  {
    /* get the stored sdcl_socket_info_t from task context */
    sdcl_set_socket_state(sip, SDCL_SOCKET_HOLDOFF_EXPIRED);
  }


5. Early detection function
  
  sdcl_sniff_action bgp_socket_data_sniff(sip, tmp_buf) {

    if (the data has some info to close the socket)
      return SDCL_SOCKET_CLOSE;
     
    return (SDCL_SOCKET_CONTINUE);
  }



6. Application task_set_receive SDCL task_job, 

  in function,
  bgp_standby_sdcl_socket_read(){

    /* get the stored sdcl_socket_info_t from task context */
   sdcl_socket_info_t *sip
   sdcl_socket_postread_op status;

   /* Call the main SDCL socket read handler. */
   /* BTW: this calls sdcl_sniff_action bgp_socket_data_sniff() to 
           check if there is a need for a quick out.
    */

    status = sdcl_socket_read( sip );

    /* Call some support code. */
    switch(status)
    {
	case SDCL_SOCKET_READ_CLOSE_RESYNC: 
    	    bgp_standby_peer_close(bnp, BGPEVENT_CLOSED, TRUE);
    	    return;

	case SDCL_SOCKET_READ_CLOSE_NORESYNC: 
    	    bgp_standby_peer_close(bnp, BGPEVENT_CLOSED, FALSE);
    	    return;

	case SDCL_SOCKET_READ_CLOSE_TBD: 
            /*need to add a case for a close, reading the the last "n" bytes. */
            /* IMPLEMENT */
            break;

	case SDCL_SOCKET_READ_SETUP_START_TIMER:
            /*
             * Disable Rx on socket. We do not want gulp job to wake up too soon
             * The Pause timeout handler re-enables Rx. 
             * The gulp job will not know it was off.
             */
            task_socket_disable_receive(tp); 

            task_job_delete( bpeer->bsp_ss_job );
            bpeer->bsp_ss_job = NULL;

            tip = sdcl_get_start_trigger_delay( sip );
            if (!tip) {
                tip = task_timer_create_oneshot(tp, 
                                                "SDCL start trigger timer", 0, 
                                                start_trigger_delay, 0, 
                                                bgp_socket_read_timer_method,0);

                sdcl_set_start_trigger_delay( sip, tip );
            } else {
                task_timer_set_oneshot(tip, start_trigger_delay, 0); 
            }
            sdcl_set_start_trigger_state(sip, SDCL_SOCKET_START_TRIGGER_ACTIVE);
            return;

	case SDCL_SOCKET_READ_SETUP_END_TIMER:
            task_job_delete( bpeer->bsp_ss_job );
            bpeer->bsp_ss_job = NULL;

            tip = sdcl_get_end_trigger_delay( sip );
            if (!tip) {
                tip = task_timer_create_oneshot(tp, 
                                                "SDCL end trigger timer", 0, 
                                                end_trigger_delay, 0, 
                                                bgp_cache_read_timer_method,0);                                        
                sdcl_set_end_trigger_delay( sip, tip );
            } else {
                task_timer_set_oneshot(tip, end_trigger_delay, 0); 
            }
            sdcl_set_start_trigger_state( sip, SDCL_SOCKET_END_TRIGGER_ACTIVE );
            return;

	case SDCL_SOCKET_READ_EXEC_DRAIN: 
            task_job_delete( bpeer->bsp_ss_job );
            bpeer->bsp_ss_job = NULL;

            /* 
             * Setup the drain job to give the gulp data to the caller.
             * Note: the drain job cannot exit with data still in gulp buffer !
             */
            newjp = task_job_create_background(tp, TASK_JOB_PRIO_4, 
                                               "BGP outbound read",
                                                bgp_standby_socket_read, NULL);                                                          
            bpeer->bsp_ss_job = newjp; 
            break;
    }
  }


5.8 Cache management
---------------------
The SDCL uses a two-layered caching approach to store the data from the 
socket. First layer would be the in-memory cache and second layer would 
be the hard disk. The cache in both cases is on a per-socket basis.

In-memory cache is implemented for each socket on top of the socket
layer in the application's context and its size is determined by the
user application during the sdcl_init(). Under normal circumstances,
this is the cache which will be used if the incoming data rate is not
higher than the processing rate of the application.

If the in-memory cache is not emptied when more data arrives in the
socket,then SDCL will switch to the second cache automatically. Then on,
any data arriving in the socket will be cached to end of the file. When
the application gets ahead and reads all the data from the hard disk cache
file and after a deterministic period, SDCL will switch to in-memory cache
only mechanism. In this case, a file is created for each socket in a
directory application prefers. Application passes on this information to
SDCL via sdcl_init() along with the MAX file size.

It is application's responsibility to determine if the sizes specified
for the cache for so many sockets will be available or not. But, SDCL
will take of the corner cases if it were to happen, in the following
way:

Disk Full:- 
SDCL will throttle the socket until SDCL is able to add data to the
file. In this case, the onus is on the file system to get more space.

File size overflow:- 
In cases where the data in the socket exceed the max file size, the SDCL
will throttle the socket and let the application drain the entire file, 
before it adds additional data to the file.



6.  PERFORMANCE

There is no performance degradation as a result of this feature.

1) The allocation on in-memory buffers by SDCL increases
the application's memory footprint.

2) In the case where there are few peers during an update burst,
the fairness time delay after start-trigger provides no add 
benefit, since the delay is intended for the case where many
peers are active in the update burst simultaneously.

7.  COMPATIBILITY ISSUES

This feature is disabled on platforms without hard-disk.

8.  SECURITY ISSUES

None.

9. Graceful RE Switchover (GRES), Hobson Impact

The functionality will support GRES and will also be supported on TX.

There is no impact on GRES or TX as result of this functionality.

Kernel JSR functionality is not changed.

10.  NSR Impact

This functionality directly changes the NSR implementation.

The system should exhibit the same or improved P&S numbers.

11.  Platforms Supported

This feature is disabled on platforms without hard-disk.

12.  SDK Impact

None.

13.  NOTES

None.

14.  GLOSSARY

P&S - Performance and Scaling
JSR - Juniper Socket Replication

15.  REVIEW COMMENTS

See the audit trail of the RLI tracking PR 276683.
