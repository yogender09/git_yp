$Id: unit_test_plan.txt,v 1.5 2007/08/30 02:43:13 kjain Exp $


Copyright (C) 2004, Juniper Networks, Inc.

NOTICE: This document contains the proprietary and confidential
information of Juniper Networks, Inc., and must not be
distributed outside of the company without the
permission of Juniper Engineering.

[This document is a template for a standard unit test plan for use in
Juniper engineering projects.  To use this document just change the
contents of appropriate sections (marked with {}).  ANYTHING NOT
MARKED 'AS NECESSARY' MUST BE INCLUDED.  If you do not need to fill
out a section, leave it blank or mark it n/a -- do not delete it.

1.  INTRODUCTION

This document describes the unit test plan for kernel socket replication 
infrastructure required for NSR (Non Stop Routing)

RLIs: 2709 2710 2754 2755
PR: 59525 
Design specs: 

http://cvs.juniper.net/cgi-bin/viewcvs.cgi/sw-projects/os/nsr/kernel/kernel_replication_spec.txt

http://cvs.juniper.net/cgi-bin/viewcvs.cgi/sw-projects/os/nsr/kernel/iha_design.txt

http://cvs.juniper.net/cgi-bin/viewcvs.cgi/sw-projects/os/nsr/kernel/sdrl_design.txt

http://cvs.juniper.net/cgi-bin/viewcvs.cgi/sw-projects/os/nsr/kernel/psrm_design.txt

http://cvs.juniper.net/cgi-bin/viewcvs.cgi/sw-projects/os/nsr/kernel/prl_design.txt

http://cvs.juniper.net/cgi-bin/viewcvs.cgi/sw-projects/os/nsr/kernel/kkcm_design.txt

2.  SETUP

The unit testing for kernel socket replication infrastruture is conducted using a sample application running on the primary RE, which connects to a peer on an external JUNOS box. The backup version of the same sample application runs on the secondary RE and snoops on the primary application's socket communication using juniper socket replication (jsr) semantics.

The setup looks as follows -


                                       JUNOS DUAL-RE PLATFORM (R1)
 ===========================================================
|                                                           |
|                                                           |
|              _____                  _____                 |
|             /     \                /     \                |
|      Pri    |     |                |     | Sec            |
|      Sample |     |                |     | Sample         |
|      App    |     |                |     | App            |
|             \_____/                \_____/                |
|                                                           |
|          *---------------*      *--------------*          |
|    (RE1) | Primary       |______| Secondary    | (RE2)    |
|          |    RE         |      |   RE         |          |
|          |               |      |              |          |
|          *---------------*      *--------------*          |
|                                                           |
 ===========================================================

                           +
                           +
                           +
                           +
                           +                           
                           + TCP/UDP connection to the Primary Sample App.
                           +                                                      
                           +                           
                           +
                         _____
                        /     \
                        |     |
                        |     |  Peer Application
                        \_____/

                    ---------------------
                    |                   |
                    | JUNOS             |
                    |    (B1)           |
                    |                   |
                    ---------------------                          


2.1  CONFIGURATION

Graceful RE Switchover (GRES) is configured and active on R1. 

R1 and B1 are reachable (route exists) from each other. 

The same IP address should be available to both REs as a result of switchover. This means if Primary sample application on (primary RE) RE-1 binds to a local IP address of IP-1 and uses it to connect to the peer application, then after failover the same IP address IP-1 should be available to RE-2. The requirement allows RE-2 to migrate the connection. In practical terms, this rules out using fxp0 addresses which differ across the 2 REs for socket replication


3. FUNCTIONAL TEST CASES (SOCKET REPLICATION FULL FEATURE)

 Goal:  
 
 Test Specific Setup: 
 
 Test Steps:
 
 Success Criteria:
   
 Result:  


3.1 Handle alloc and Secondary Socket pair creation

 Goal: Handles can be allocated on primary/secondary RE and secondary socket
 pair created to snoop on primary's socket communication.   
 
 Test Specific Setup:
 
    Used coneyisland and propecia in following setup

                          FE
           coneysisland --------------------------- propecia
           (5.5.5.5/31)                            (5.5.5.4/31) 

 Test Steps:
 1. Run jsrutil on coneyisland master RE as a server app

    $ ./jsrutil -a jsr_echo_server

 2. Run jsrutil on coneyisland backup RE as snooping app

    $ ./jsrutil -a jsr_echo_server

 3. Run jsrpeer on propecia to establish 5 connections and send a
    single message of 100 bytes to master RE (5.5.5.5). Master RE
    will echo it back.

    $ ./jsrpeer 5.5.5.5
 
 Success Criteria: 
 1. On the master RE primary handles are created for the 5 connections

Peer fd(8): New conn: From 5.5.5.4:1857, To 5.5.5.4:16000
Peer fd(8), allocated primary handle 0x100001d00000000 
Peer fd(9): New conn: From 5.5.5.4:4408, To 5.5.5.4:16000
Peer fd(9), allocated primary handle 0x100001e00000001 
Peer fd(10): New conn: From 5.5.5.4:2960, To 5.5.5.4:16000
Peer fd(10), allocated primary handle 0x100001f00000002 
Peer fd(11): New conn: From 5.5.5.4:4235, To 5.5.5.4:16000
Peer fd(11), allocated primary handle 0x100002000000003 
Peer fd(12): New conn: From 5.5.5.4:3150, To 5.5.5.4:16000
Peer fd(12), allocated primary handle 0x100002100000004 
 
 2. On the backup RE secondary handles are created for the two snoop fds

ORE fd(5): Sec rcv_fd(6), snd_fd(7), pri_handle(0x100001d00000000)  sec_handle(0
x100000d00000000)
ORE fd(5): Sec rcv_fd(8), snd_fd(9), pri_handle(0x100001e00000001)  sec_handle(0
x100000e00000001)
ORE fd(5): Sec rcv_fd(10), snd_fd(11), pri_handle(0x100001f00000002)  sec_handle(0x100000f00000002)
ORE fd(5): Sec rcv_fd(12), snd_fd(13), pri_handle(0x100002000000003)  sec_handle(0x100001000000000)
ORE fd(5): Sec rcv_fd(14), snd_fd(15), pri_handle(0x100002100000004)  sec_handle(0x100001100000001)

 Result: Pass

3.2 Socket replication setup (Initialization Handshake) 
 
 Goal: Socket replication initialization handshake involving SDRL, PSRM/PRL and
 IHA takes place when replication is enabled on a socket  
   
 Test Specific Setup: Same as 3.1

 Test Steps: Same as 3.1

 Success Criteria:
 1. Trace logs/socket returns ready for read/writes on primary

 Result: Pass

3.3 Socket data replication (Receive Side)

 Goal: Incoming data packets are replicated to the backup rcv snoop socket  
 
 Test Specific Setup: Same as in 3.1
 
 Test Steps: Same as in 3.1
 
 Success Criteria:

 1. Backup RE shows that it has received the replicated messages on Rcv
    and Snd fds

Peer Rcv fd(6): Received new message header, Msg Num 0
Peer Rcv fd(6): Msg 0: Read 112 bytes : verify off
Peer Snd fd(7): Msg 0: ECHO MESSAGE: NOT VERIFIED: RE1
Peer Snd fd(7): Msg 0: Read 112 bytes : verify off
Peer Snd fd(7): Msg 0: Send and Recv data payload: MATCH: 100 bytes
Peer Rcv fd(8): Received new message header, Msg Num 0
Peer Rcv fd(8): Msg 0: Read 112 bytes : verify off
Peer Snd fd(9): Msg 0: ECHO MESSAGE: NOT VERIFIED: RE1
Peer Snd fd(9): Msg 0: Read 112 bytes : verify off
Peer Snd fd(9): Msg 0: Send and Recv data payload: MATCH: 100 bytes
Peer Rcv fd(10): Received new message header, Msg Num 0
Peer Rcv fd(10): Msg 0: Read 112 bytes : verify off
Peer Snd fd(11): Msg 0: ECHO MESSAGE: NOT VERIFIED: RE1
Peer Snd fd(11): Msg 0: Read 112 bytes : verify off
Peer Snd fd(11): Msg 0: Send and Recv data payload: MATCH: 100 bytes
Peer Rcv fd(12): Msg 0: Read 112 bytes : verify off
Peer Rcv fd(12): Received new message header, Msg Num 0
Peer Snd fd(13): Msg 0: ECHO MESSAGE: NOT VERIFIED: RE1
Peer Snd fd(13): Msg 0: Read 112 bytes : verify off
Peer Snd fd(13): Msg 0: Send and Recv data payload: MATCH: 100 bytes
Peer Rcv fd(14): Received new message header, Msg Num 0
Peer Rcv fd(14): Msg 0: Read 112 bytes : verify off
Peer Snd fd(15): Msg 0: ECHO MESSAGE: NOT VERIFIED: RE1
Peer Snd fd(15): Msg 0: Read 112 bytes : verify off
Peer Snd fd(15): Msg 0: Send and Recv data payload: MATCH: 100 bytes

 Result: Pass

3.4 Socket data replication (Send Side)

 Goal: Outgoing data records are replicated to the backup snd snoop socket  
 
 Test Specific Setup: Same as 3.3
 
 Test Steps: Same as 3.3
 
 Success Criteria: Same as 3.3. Send side data has been received on Snd fds.
   
 Result: Pass

3.5 TCP state replication 
  
 Goal: TCP state is replicated successfully between primary and secondary
 
 Test Specific Setup: Same as 3.3
 
 Test Steps: Same as 3.3
 
 Success Criteria: netstat -V output was manually inspected
   
 Result: Pass

3.6 Socket and TCP connection Failover
 
 Goal: Socket and TCP state can be successfully failed over to the secondary 

 Test Specific Setup: Same as 3.3
 
 Test Steps:

 1. Run jsrutil on coneyisland master RE as a server app

    $ ./jsrutil -a nsr_server

 2. Run jsrutil on coneyisland backup RE as snooping app

    $ ./jsrutil -a nsr_client

 3. Run jsrpeer on propecia to establish 5 connections and send 100
    byte messages continuously to master RE (5.5.5.).  Master RE
    will echo it back.

    $ ./jsrpeer -n 5 -m 0 5.5.5.5
 
 4. On backup RE effect a switchover through CLI
   
    cli> request chassis routing-engine master acquire 
 
 Success Criteria: Peer communication continues uninterrupted between peer
 app and jsrutil on backup RE (new master) even after switchover. There is
 a few seconds break in communication when switchover happens, but 
 communication continues without failure.
   
 Result: Pass

3.7 Primary RE Failure

 Goal: Primary RE failure allows the secondary application to merge and migrate
       sockets/TCP-connection.
 
 Test Specific Setup: 

None.  Setup is as described in section 2.
 
 Test Steps:

1.  Master and backup RE are running jsrutil.
2.  Peer is running jsrpeer.
3.  Open up separate windows to look at the syslogs on both machines.  Set
    the logging level on both REs to LOG_DEBUG for both the syslog and for
    JSR log messages.
4.  While messages are being exchanged, I log onto the master RE's console and
    reset it, simulating a master RE failure.
5.  Interpret results.
 
 Success Criteria:

The secondary application should merge its replicated receive and send 
sockets and continue communication with the peer, without error and 
significant delay.

 Results:

The secondary application does continue communication with the peer; however,
there is a delay of several seconds after the master RE goes down, before
communication is resumed.  This is because of the chassisd mastership switch
detection delay, and is expected.

The jsrpeer process completed successfully, without error.

syslog from the backup RE during and after the master RE goes down:

Sep 22 11:42:35  simba1 /kernel: jsr_sdrl_send_msg: SDRL sending message as follo
ws-
Sep 22 11:42:35  simba1 /kernel: IPC HDR - TYPE: 1, SUBTYPE: 4, VER: 1, LEN 32   
Sep 22 11:42:35  simba1 /kernel: SDRL ACK MSG- TYPE SDRL RECORD REPLICATE, HANDLE
: 72057598332895232, REJ: 0, SEQ: 4, DLEN 312
Sep 22 11:42:35  simba1 /kernel: jsr_prl_ipc_msg_handler: received IPC message: t
ype 3, subtype 0, length 72
Sep 22 11:42:35  simba1 /kernel: jsr_prl_recv_replicate_msg(): received PRL repli
cate message: handle 0x100000100000000 seqnum 0x19 type 3 subtype 0 version 1 len
gth 72
Sep 22 11:42:35  simba1 /kernel: jsr_prl_send_ack_msg(): sending PRL acknowledgem
ent message: handle 0x100000100000000 reason 0 rcvbufsize 0 type 3 subtype 1 vers
ion 1 length 28
Sep 22 11:42:35  simba1 /kernel: sent PRL acknowledgement message of length 28
Sep 22 11:42:38  simba1 chassisd[2804]: No response from the other routing engine
 for the last 2 seconds.
Sep 22 11:42:38  simba1 /kernel: mastership: routing engine 1 becoming master
Sep 22 11:42:38  simba1 chassisd[2804]: Keepalive timeout of 2 seconds expired.  Assuming RE mastership.
Sep 22 11:42:39  simba1 /kernel: mastership: timeout becoming master mstr_conf=6d
Sep 22 11:42:39  simba1 /kernel: rdp keepalive expired, connection dropped - src 
0:1025 dest 1:1020
Sep 22 11:42:40  simba1 /kernel: mastership: routing engine 1 becoming master   
Sep 22 11:42:40  simba1 /kernel: TNP: deleting neighbor 4 from interface fxp1.   
Sep 22 11:42:40  simba1 /kernel: TNP: deleting neighbor 1 from interface fxp1.   
Sep 22 11:42:40  simba1 ksyncd[2815]: after 0 attempts connect returned error: No
 route to host
Sep 22 11:42:41  simba1 /kernel: mastership: timeout becoming master mstr_conf=6d
Sep 22 11:42:41  simba1 ksyncd[2815]: after 1 attempts connect returned error: No
 route to host
Sep 22 11:42:42  simba1 /kernel: mastership: routing engine 1 becoming master
Sep 22 11:42:42  simba1 ksyncd[2815]: after 2 attempts connect returned error: No
 route to host
Sep 22 11:42:43  simba1 /kernel: mastership: timeout becoming master mstr_conf=6d
Sep 22 11:42:43  simba1 ksyncd[2815]: after 3 attempts connect returned error: No
 route to host
Sep 22 11:42:44  simba1 /kernel: mastership: routing engine 1 becoming master
Sep 22 11:42:44  simba1 ksyncd[2815]: after 4 attempts connect returned error: No
 route to host
Sep 22 11:42:45  simba1 /kernel: mastership: timeout becoming master mstr_conf=6d
Sep 22 11:42:45  simba1 ksyncd[2815]: after 5 attempts connect returned error: No
 route to host
Sep 22 11:42:46  simba1 /kernel: mastership: routing engine 1 becoming master
Sep 22 11:42:46  simba1 ksyncd[2815]: after 6 attempts connect returned error: No
 route to host
Sep 22 11:42:47  simba1 /kernel: mastership: timeout becoming master mstr_conf=6d
Sep 22 11:42:47  simba1 ksyncd[2815]: after 7 attempts connect returned error: No
 route to host 
Sep 22 11:42:48  simba1 /kernel: mastership: routing engine 1 becoming master 
Sep 22 11:42:48  simba1 ksyncd[2815]: after 8 attempts connect returned error: No
 route to host
Sep 22 11:42:49  simba1 /kernel: mastership: timeout becoming master mstr_conf=6d
Sep 22 11:42:49  simba1 ksyncd[2815]: after 9 attempts connect returned error: No route to host
Sep 22 11:42:50  simba1 /kernel: mastership: routing engine 1 becoming master
Sep 22 11:42:50  simba1 ksyncd[2815]: after 10 attempts connect returned error: N
o route to host   
Sep 22 11:42:51  simba1 /kernel: mastership: timeout becoming master mstr_conf=6d
Sep 22 11:42:51  simba1 ksyncd[2815]: after 11 attempts connect returned error: N
o route to host
Sep 22 11:42:52  simba1 /kernel: mastership: routing engine 1 becoming master
Sep 22 11:42:52  simba1 ksyncd[2815]: after 12 attempts connect returned error: N
o route to host
Sep 22 11:42:53  simba1 /kernel: mastership: timeout becoming master mstr_conf=6d
Sep 22 11:42:53  simba1 ksyncd[2815]: after 13 attempts connect returned error: N
o route to host
Sep 22 11:42:54  simba1 /kernel: mastership: routing engine 1 becoming master
Sep 22 11:42:54  simba1 /kernel: TNP: adding neighbor 1 to interface fxp1.
Sep 22 11:42:54  simba1 /kernel: pfe_peer_set_timeout: set timeout 0xc03c1fa7
Sep 22 11:42:54  simba1 /kernel: jsr_kkcm_cleanup: reason 0
Sep 22 11:42:54  simba1 /kernel: jsr_iha_cleanup: JSR IHA uninitialization comple
ted successfully
Sep 22 11:42:54  simba1 /kernel: jsr_sdrl_cleanup: Cleanup Reason 0
Sep 22 11:42:54  simba1 /kernel: PSRM global state cleaned up because of reason 0
Sep 22 11:42:54  simba1 /kernel: PRL global state cleaned up because of reason 0 
Sep 22 11:42:54  simba1 /kernel: chassisd pid 2804 syscall 3 ran for 650.795 ms  
Sep 22 11:42:54  simba1 /kernel: pfe_peer_alloc: type 1, index 0, old state Valid
 new state Reopening mastership 1
Sep 22 11:42:54  simba1 /kernel: pfe_peer_clear_timeout: clearing timo 0xc03c1fa7
Sep 22 11:42:54  simba1 /kernel: pfe_peer_set_timeout: set timeout 0xc03bd76b
Sep 22 11:42:54  simba1 /kernel: pfe_listener_connect: conn established: listener
 idx=0, tnpaddr=0x2
Sep 22 11:42:54  simba1 /kernel: pfe_msg_peer_info: type 1, index 0, old state Re
opening new state Online mastership 1
Sep 22 11:42:54  simba1 /kernel: pfe_peer_clear_timeout: clearing timo 0xc03bd76b
Sep 22 11:42:54  simba1 /kernel: pfe_peer_alloc: type 0, index 0, old state Init
new state Opening mastership 1
Sep 22 11:42:54  simba1 /kernel: pfe_peer_set_timeout: set timeout 0xc03bd76b
[...]

Syslog on the master RE contains no unexpected errors as well, before it is
brought down.

3.8 Secondary RE Failure

 Goal:

Secondary RE failure causes unreplicate and data communication to proceed on the 
primary without a hitch.

 Test Specific Setup: 

None.  Setup is as described in section 2.
 
 Test Steps:

1.  Master and backup RE are running jsrutil.
2.  Peer is running jsrpeer.
3.  Open up separate windows to look at the syslogs on both machines.  Set 
    the logging level on both REs to LOG_DEBUG for both the syslog and for 
    JSR log messages.
4.  While messages are being exchanged, I log onto the backup RE's console and
    reset it, simulating a backup RE failure.
5.  Interpret results.
 
 Success Criteria:

The primary application should continue communication with the peer with little 
to no delay and/or errors.

 Results:

The primary application does continue communication with the peer; however, 
there is a 2 second or so delay after the backup RE goes down, before 
communication is resumed.  This is because of the chassisd mastership switch 
detection delay, and is expected.

The jsrpeer process completed successfully, without error.

There is an unexpected PRL replicate message timeout error that occurs.  I 
will look into this, though it does not seem to affect the correctness or flow 
of traffic from the peer.

syslog from the master RE during and after the backup RE goes down:

Sep 22 11:34:49  simba /kernel: sent PRL replicate message of length 384
Sep 22 11:34:49  simba chassisd[2823]: No response from the other routing engine for the last 2 seconds. 
Sep 22 11:34:51  simba /kernel: rdp keepalive expired, connection dropped - src 1:1020 dest 5:1025
Sep 22 11:34:51  simba /kernel: peer_inputs: soreceive() error 64
Sep 22 11:34:51  simba /kernel: pfe_send_failed(index 1, type 7), err=32
Sep 22 11:34:51  simba /kernel: peer_socket_close: type 7, index 1, old state Online new state Valid mastership 1
Sep 22 11:34:51  simba /kernel: pfe_peer_set_timeout: set timeout 0xc03c1fa7
Sep 22 11:34:51  simba /kernel: TNP: deleting neighbor 5 from interface fxp1.
Sep 22 11:34:51  simba craftd[2825]:  Minor alarm set, Loss of communication with Backup RE
Sep 22 11:34:51  simba alarmd[2824]: Alarm set: RE color=YELLOW, class=CHASSIS, reason=Loss of communication with Backup RE 
Sep 22 11:34:51  simba craftd[2825]: forwarding display request to chassisd: type = 4, subtype = 43 
Sep 22 11:34:53  simba /kernel: jsr_prl_send_replicate_msg(): sending PRL replicate message: handle 0x100000100000000 type 3 subtype 0 version 1 length 384 
Sep 22 11:34:53  simba /kernel: sent PRL replicate message of length 384
Sep 22 11:34:56  simba /kernel: jsr_unreplicate unreplicating handle 0x100000100000000, due to error 60, msg PRL replicate message timed out
Sep 22 11:34:56  simba /kernel: jsr_sdrl_unrepl_socket: Unreplicate error 60 on handle 72057598332895232
Sep 22 11:34:56  simba /kernel: Unreplicated handle 72057598332895232, error 0, reinject 0

Syslog on the backup RE contains no unexpected errors as well, before it is 
brought down.

3.9 Async rtsock notifications [TBD]

 Goal:

 Test Specific Setup: 
 
 Test Steps:
 
 Success Criteria:

3.10 Unreplicate Call

 Goal:Sockets can be unreplicated w/o causing interruption to peer communication 
 Test Specific Setup: 
         1.Modified sample app (juniper/usr.bin/jsrutil/jsrutil)
            to call explicit unreplicate on a connection after 6 packets/messages 
            have been replicated.
 
         2. fenway is a M40e dual RE box connected to pnc across a T1 link     
                        
                          T1
           fenway --------------------------- pnc
           (5.5.5.5/31)                      (5.5.5.4/31) 

         3. fenway1 is the primary RE and fenway is the secondary/backup RE

 
 Test Steps:
        1. On fenway1 (primary RE) -
                root@fenway1% ./jsrutil -a nsr_server
        
        2. On fenway (backup RE) -
                root@fenway% ./jsrutil -a nsr_client
        
        3A. On pnc (peer) -
                root@pnc% ./jsrpeer -n 1 -w 500 -m 10 -l 300 5.5.5.5

          
        3B. Try the same with 3 socket connections  -
                root@pnc% ./jsrpeer -n 3 -w 500 -m 10 -l 300 5.5.5.5

 Success Criteria:
   1. Primary RE (fenway1) continues through first 6 packets, does an unreplicate and then
      works on next 4 packets -

root@fenway1% ./jsrutil -a nsr_server
NSR is enabled 
Peer server socket fd(5) = socket(inet, tcp) created
ORE server socket fd(6) = socket(inet, tcp) created
ORE fd(7): New connection: From 10.0.0.4,3473 To 10.0.0.4,17000
Peer fd(8): New conn: From 5.5.5.4:3892, To 5.5.5.4:16000
Peer fd(8), allocated primary handle 0x100000800000000 
Peer fd(8): Replication request.. 
Peer fd(8): Replication request started
ORE fd(7): Peer Msg 0: Wrote replication message 32 bytes
ORE fd(7): Peer Msg 0: Received new message header
ORE fd(7): Peer Msg 0: Received replication ack, 36 bytes
Peer fd(8): jsr_replicate() issued, pri_handle 0x100000800000000, sec_handle 010000
0500000000
Peer fd(8): Received new message header, Msg Num 0
Peer fd(8): Msg 0: Read 312 bytes: verify off 
Peer fd(8): Msg 0: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 1
Peer fd(8): Msg 1: Read 312 bytes: verify off 
Peer fd(8): Msg 1: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 2
Peer fd(8): Msg 2: Read 312 bytes: verify off 
Peer fd(8): Msg 2: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 3
Peer fd(8): Msg 3: Read 312 bytes: verify off 
Peer fd(8): Msg 3: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 4
Peer fd(8): Msg 4: Read 312 bytes: verify off 
Peer fd(8): Msg 4: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 5
Peer fd(8): Msg 5: Read 312 bytes: verify off 
Peer fd(8): Msg 5: Echo wrote 312 bytes: unverified

Peer fd(8): Unreplicated!!!

Peer fd(8): Received new message header, Msg Num 6
Peer fd(8): Msg 6: Read 312 bytes: verify off 
Peer fd(8): Msg 6: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 7
Peer fd(8): Msg 7: Read 312 bytes: verify off 
Peer fd(8): Msg 7: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 8
Peer fd(8): Msg 8: Read 312 bytes: verify off 
Peer fd(8): Msg 8: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 9
Peer fd(8): Msg 9: Read 312 bytes: verify off 
Peer fd(8): Msg 9: Echo wrote 312 bytes: unverified
Peer fd(8): Remote side closed connection
Peer fd(8): closing fd..
Peer fd(8), handle(0x100000800000000): destroying context 
Peer fd(8): freeing handle 0x100000800000000

   2. Backup RE (fenway) gets first 6 messages replicated (on both input and output side) -

root@fenway% ./jsrutil -a nsr_client
NSR is enabled 
ORE client socket fd(5) = socket(inet, tcp) created
ORE fd(5): Connected to (10.0.0.5:17000) from (10.0.0.5:3473)
ORE fd(5): Peer Msg 0: Received new replication message header
ORE fd(5): Peer Msg 0: Received replication request, 32 bytes
ORE fd(5): Sec rcv_fd(6), snd_fd(7), pri_handle(0x100000800000000)  sec_
handle(0x100000500000000)
ORE fd(5): Sec rcv_fd(6) snd_fd(7): Replication reply formed 
Peer Rcv fd(6): Received new message header, Msg Num 0
Peer Rcv fd(6): Msg 0: Read 312 bytes : verify off 
Peer Snd fd(7): Msg 0: ECHO MESSAGE: NOT VERIFIED: RE1
Peer Snd fd(7): Msg 0: Read 312 bytes : verify off 
Peer Snd fd(7): Msg 0: Send and Recv data payload: MATCH: 300 bytes
Peer Rcv fd(6): Received new message header, Msg Num 1
Peer Rcv fd(6): Msg 1: Read 312 bytes : verify off 
Peer Snd fd(7): Msg 1: ECHO MESSAGE: NOT VERIFIED: RE1
Peer Snd fd(7): Msg 1: Read 312 bytes : verify off 
Peer Snd fd(7): Msg 1: Send and Recv data payload: MATCH: 300 bytes
Peer Rcv fd(6): Received new message header, Msg Num 2
Peer Rcv fd(6): Msg 2: Read 312 bytes : verify off 
Peer Snd fd(7): Msg 2: ECHO MESSAGE: NOT VERIFIED: RE1
Peer Snd fd(7): Msg 2: Read 312 bytes : verify off 
Peer Snd fd(7): Msg 2: Send and Recv data payload: MATCH: 300 bytes
Peer Rcv fd(6): Received new message header, Msg Num 3
Peer Rcv fd(6): Msg 3: Read 312 bytes : verify off 
Peer Snd fd(7): Msg 3: ECHO MESSAGE: NOT VERIFIED: RE1
Peer Snd fd(7): Msg 3: Read 312 bytes : verify off 
Peer Snd fd(7): Msg 3: Send and Recv data payload: MATCH: 300 bytes
Peer Rcv fd(6): Received new message header, Msg Num 4
Peer Rcv fd(6): Msg 4: Read 312 bytes : verify off 
Peer Snd fd(7): Msg 4: ECHO MESSAGE: NOT VERIFIED: RE1
Peer Snd fd(7): Msg 4: Read 312 bytes : verify off 
Peer Snd fd(7): Msg 4: Send and Recv data payload: MATCH: 300 bytes
Peer Rcv fd(6): Received new message header, Msg Num 5
Peer Rcv fd(6): Msg 5: Read 312 bytes : verify off 
Peer Snd fd(7): Msg 5: ECHO MESSAGE: NOT VERIFIED: RE1
Peer Snd fd(7): Msg 5: Read 312 bytes : verify off 
Peer Snd fd(7): Msg 5: Send and Recv data payload: MATCH: 300 bytes

   3. PEER sends and receives data w/o interruption 

root@pnc% ./jsrpeer -n 1 -w 500 -m 10 -l 300 5.5.5.5
Num Connections : 1 
Remote peer port: 16000 
Message Length  : 300 
Num Messsages   : 10 
Address Family  : inet 
Message Verify  : Off 

Connecting to host 5.5.5.5:16000...
Conn 0: fd 3: Connected to (5.5.5.5:16000) from (5.5.5.5:3892)
Conn 0: Msg 0: Wrote 312 bytes: verify off 
Conn 0:Msg 0: Read 312 bytes: NOT VERIFIED: ECHO DATA MATCHES: RE1: Time 0.20s
Conn 0: Msg 1: Wrote 312 bytes: verify off 
Conn 0:Msg 1: Read 312 bytes: NOT VERIFIED: ECHO DATA MATCHES: RE1: Time 0.8s
Conn 0: Msg 2: Wrote 312 bytes: verify off 
Conn 0:Msg 2: Read 312 bytes: NOT VERIFIED: ECHO DATA MATCHES: RE1: Time 0.8s
Conn 0: Msg 3: Wrote 312 bytes: verify off 
Conn 0:Msg 3: Read 312 bytes: NOT VERIFIED: ECHO DATA MATCHES: RE1: Time 0.8s
Conn 0: Msg 4: Wrote 312 bytes: verify off 
Conn 0:Msg 4: Read 312 bytes: NOT VERIFIED: ECHO DATA MATCHES: RE1: Time 0.8s
Conn 0: Msg 5: Wrote 312 bytes: verify off 
Conn 0:Msg 5: Read 312 bytes: NOT VERIFIED: ECHO DATA MATCHES: RE1: Time 0.8s
Conn 0: Msg 6: Wrote 312 bytes: verify off 
Conn 0:Msg 6: Read 312 bytes: NOT VERIFIED: ECHO DATA MATCHES: RE1: Time 0.6s
Conn 0: Msg 7: Wrote 312 bytes: verify off 
Conn 0:Msg 7: Read 312 bytes: NOT VERIFIED: ECHO DATA MATCHES: RE1: Time 0.6s
Conn 0: Msg 8: Wrote 312 bytes: verify off 
Conn 0:Msg 8: Read 312 bytes: NOT VERIFIED: ECHO DATA MATCHES: RE1: Time 0.6s
Conn 0: Msg 9: Wrote 312 bytes: verify off 
Conn 0:Msg 9: Read 312 bytes: NOT VERIFIED: ECHO DATA MATCHES: RE1: Time 0.6s
Conn 0: Summary: Sent 10 messages of 300 bytes each, closing connection
Finished with 0 connections. Exiting...
root@pnc% 


3.11 Socket close/teardown

 Goal:Socket close/teardown clears up replication state.

 Test Specific Setup: 
        1. fenway is a M40e dual RE box connected to pnc across a T1 link
                
                          T1
           fenway --------------------------- pnc
           (5.5.5.5/31)                      (5.5.5.4/31) 

        2. fenway1 is the primary RE and fenway is the secondary/backup RE
       
 Test Steps:
        1. Run a replicated setup with peer connections from pnc to fenway1 (replicated on fenway) 

        2. Kill primary/secondary applications 

 Success Criteria:
        1. Peer (pnc) sends unlimited packets on a replicated setup -
                root@pnc% ./jsrpeer -n 1 -w 500 -m 0 -l 300 5.5.5.5

        2. Secondary app on fenway is killed, causing a close/teardown

                netstat -V -f inet before close -

                root@fenway% netstat -V -f inet | grep 5.5.5.5
                        tcp4       0    312  5.5.5.5.16000          5.5.5.4.1972           ESTABLISHED

                
                netstat -V -f inet after close -
       
                root@fenway% netstat -V -f inet | grep 5.5.5.5
                         tcp4       0    312  5.5.5.5.16000          5.5.5.4.1972           FIN_WAIT_1

        

        3. Primary app - Peer communication continues uninterrupted -
                
Peer fd(8): Msg 3: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 4
Peer fd(8): Msg 4: Read 312 bytes: verify off 
Peer fd(8): Msg 4: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 5
Peer fd(8): Msg 5: Read 312 bytes: verify off 
Peer fd(8): Msg 5: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 6
Peer fd(8): Msg 6: Read 312 bytes: verify off 
Peer fd(8): Msg 6: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 7
Peer fd(8): Msg 7: Read 312 bytes: verify off 
Peer fd(8): Msg 7: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 8
Peer fd(8): Msg 8: Read 312 bytes: verify off 
Peer fd(8): Msg 8: Echo wrote 312 bytes: unverified
Peer fd(7): Remote side closed connection
Peer fd(7): closing fd..
ORE connection fd(7) has been closed
ORE fd(7), context destroyed 
Peer fd(8): Received new message header, Msg Num 9
Peer fd(8): Msg 9: Read 312 bytes: verify off 
Peer fd(8): Msg 9: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 10
Peer fd(8): Msg 10: Read 312 bytes: verify off 
Peer fd(8): Msg 10: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 11
Peer fd(8): Msg 11: Read 312 bytes: verify off 
Peer fd(8): Msg 11: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 12
Peer fd(8): Msg 12: Read 312 bytes: verify off 
Peer fd(8): Msg 12: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 13
Peer fd(8): Msg 13: Read 312 bytes: verify off 
Peer fd(8): Msg 13: Echo wrote 312 bytes: unverified
Peer fd(8): Received new message header, Msg Num 14
Peer fd(8): Msg 14: Read 312 bytes: verify off 
Peer fd(8): Msg 14: Echo wrote 312 bytes: unverified
Peer fd(8): Remote side closed connection
Peer fd(8): closing fd..
Peer fd(8), handle(0x100000f00000000): destroying context 
Peer fd(8): freeing handle 0x100000f00000000

3.12 Bulk Data Transfer

 Goal:Transfer data over replicated sockets for a long time

 Test Specific Setup: 

   Same as 3.6
 
 Test Steps:

   Same as 3.6
 
 Success Criteria:No memory/mbuf leaks

 Test Results: Pass

3.13 Multiple simultaneous replicated sockets

 Goal: To test multiple replicated socket connections at the same time

 Test Specific Setup:  Same as 3.3
 
 Test Steps: Same as 3.3
 
 Success Criteria: Multiple socket connections should simultaneously be
 supported.

 Results: Pass

3.14 UDP socket replication

 Goal:Ensure that socket data replication and failover works for UDP sockets

 Test Specific Setup: 
          1.Modified mini sample app (juniper/usr.bin/jsrutil/jsrutil_mini, jsrpeer_mini)
            to use UDP sockets.
          2. fenway is a M40e dual RE box connected to pnc across a T1 link
                
                          T1
           fenway --------------------------- pnc
           (5.5.5.5/31)                      (5.5.5.4/31) 

          3. fenway is the primary RE and fenway1 is the secondary/backup RE

 Test Steps:
 1. Run mini peer app on pnc -

       root@pnc% jsrpeer_mini 1277

 2. Run mini sample app on primary RE -

        root@fenway%   ./jsrutil_mini 5.5.5.4 1277 0
        PRIMARY SOCKET

        Primary handle is 0x1b010001 

       
 3. Run mini sample app on backup RE -
        root@fenway1% ./jsrutil_mini 5.5.5.4 1277 1
        SECONDARY SOCKET

 
 Success Criteria:
  1. UDP send and rcv data is replicated on fenway1 (backup) -

  Secondary handle is 0x2000001 
rfd 6, sfd 7
 SEND SNOOP SOCKET:(32 bytes): Client Message 0: NSR is alive..
 READ SNOOP SOCKET: (33 bytes): Server Message 24: NSR is alive..
 SEND SNOOP SOCKET:(32 bytes): Client Message 1: NSR is alive..
 READ SNOOP SOCKET: (33 bytes): Server Message 25: NSR is alive..
 SEND SNOOP SOCKET:(32 bytes): Client Message 2: NSR is alive..
 READ SNOOP SOCKET: (33 bytes): Server Message 26: NSR is alive..
 SEND SNOOP SOCKET:(32 bytes): Client Message 3: NSR is alive..
 READ SNOOP SOCKET: (33 bytes): Server Message 27: NSR is alive..
 SEND SNOOP SOCKET:(32 bytes): Client Message 4: NSR is alive..
 READ SNOOP SOCKET: (33 bytes): Server Message 28: NSR is alive..
 SEND SNOOP SOCKET:(32 bytes): Client Message 5: NSR is alive..
 READ SNOOP SOCKET: (33 bytes): Server Message 29: NSR is alive..
 SEND SNOOP SOCKET:(32 bytes): Client Message 6: NSR is alive..
 READ SNOOP SOCKET: (33 bytes): Server Message 30: NSR is alive..
 SEND SNOOP SOCKET:(32 bytes): Client Message 7: NSR is alive..
 READ SNOOP SOCKET: (33 bytes): Server Message 31: NSR is alive..
 SEND SNOOP SOCKET:(32 bytes): Client Message 8: NSR is alive..
 READ SNOOP SOCKET: (33 bytes): Server Message 32: NSR is alive..
 SEND SNOOP SOCKET:(32 bytes): Client Message 9: NSR is alive..
 READ SNOOP SOCKET: (33 bytes): Server Message 33: NSR is alive..
 SEND SNOOP SOCKET:(33 bytes): Client Message 10: NSR is alive..
 READ SNOOP SOCKET: (33 bytes): Server Message 34: NSR is alive..
 SEND SNOOP SOCKET:(33 bytes): Client Message 11: NSR is alive..

 2. After a switchover and merge, secondary application is able to migrate the socket and 
   UDP connection as active.
  
 On Fenway -    
    regress@fenway> request chassis routing-engine master switch 
        Toggle mastership between routing engines ? [yes,no] (no) yes 

        Resolving mastership...
        Complete. The other routing engine becomes the master.

 On Fenway1 - <Data is recv'ed from the peer>

  Message recv'ed on merged socket - Server Message 35: NSR is alive..
Message recv'ed on merged socket - Server Message 36: NSR is alive..
Message recv'ed on merged socket - Server Message 37: NSR is alive..
Message recv'ed on merged socket - Server Message 38: NSR is alive..
Message recv'ed on merged socket - Server Message 39: NSR is alive..
Message recv'ed on merged socket - Server Message 40: NSR is alive..
Message recv'ed on merged socket - Server Message 41: NSR is alive..
Message recv'ed on merged socket - Server Message 42: NSR is alive..
Message recv'ed on merged socket - Server Message 43: NSR is alive..
Message recv'ed on merged socket - Server Message 44: NSR is alive..
Message recv'ed on merged socket - Server Message 45: NSR is alive..
Message recv'ed on merged socket - Server Message 46: NSR is alive..
 

 On pnc - <Peer is able to receive data post merge


Here is the message number 36: Client to Server - socket has been merged 0!!
Here is the message number 37: Client to Server - socket has been merged 1!!
Here is the message number 38: Client to Server - socket has been merged 2!!
Here is the message number 39: Client to Server - socket has been merged 3!!
Here is the message number 40: Client to Server - socket has been merged 4!!
Here is the message number 41: Client to Server - socket has been merged 5!!
Here is the message number 42: Client to Server - socket has been merged 6!!
Here is the message number 43: Client to Server - socket has been merged 7!!
Here is the message number 44: Client to Server - socket has been merged 8!!
Here is the message number 45: Client to Server - socket has been merged 9!!
Here is the message number 46: Client to Server - socket has been merged 10!!
Here is the message number 47: Client to Server - socket has been merged 11!!


3.15 Stress Tests

 Goal:

Test that socket replication activities do not cause any memory leaks or 
watchdog panics.

There are plenty of other stress tests that can be run.  However, we just 
want to test the common datapaths, as additional testing is beyond the 
scope of the UTP.

3.15.1	Basic peer traffic replication stress test.

 Test Specific Setup: 

None.  Setup is as described in section 2.
 
 Test Steps:

1.  Master and backup RE are running jsrutil.
2.  Peer is running jsrpeer with -m 0 (infinite messages).
	./jsrpeer -n 1 -w 500 -m 0 -l 300 5.5.5.5
3.  Open up separate windows to look at the syslogs on both machines.  Set
    the logging level on both REs to LOG_DEBUG for both the syslog and for  
    JSR log messages.
4.  Let run for 24 hours.
5.  Interpret results.
 
 Success Criteria:

* R1 does not crash due to a watchdog panic or out of memory condition.
* R1 does not leak any kernel memory, either mbuf or kmalloc.
* R1 is still fully operational and interactive after the test is finished.
* R1 does not drop or lose traffic from the peer.  All traffic from the peer 
  should be fully replicated to both sides.

 Results:

  Started on 10:19 AM on 9/23/05

vmstat -m from master RE before test starts:

netstat -m from master RE before test starts:

vmstat -m from backup RE before test starts:

netstat -m from backup RE before test starts:


vmstat -m from master RE after test ends:

netstat -m from master RE after test ends:

vmstat -m from backup RE after test ends:

netstat -m from backup RE after test ends:

3.15.2  Open/close stress test.

 Test Specific Setup:

None.  Setup is as described in section 2.

 Test Steps:

1.  Master and backup RE are running jsrutil.
2.  Peer is running jsrpeer with -m 0 (infinite messages).
        ./jsrpeer -n 1 -w 500 -m 0 -l 300 5.5.5.5 <extra args>
3.  Open up separate windows to look at the syslogs on both machines.  Set
    the logging level on both REs to LOG_DEBUG for both the syslog and for
    JSR log messages.
4.  Let run for 24 hours.
5.  Interpret results.

 Success Criteria:

* R1 does not crash due to a watchdog panic or out of memory condition.
* R1 does not leak any kernel memory, either mbuf or kmalloc.
* R1 is still fully operational and interactive after the test is finished.
* R1 does not drop or lose traffic from the peer.  All traffic from the peer
  should be fully replicated to both sides.

 Results:

  Started on XX:XX XM on 9/XX/05

vmstat -m from master RE before test starts:

netstat -m from master RE before test starts:

vmstat -m from backup RE before test starts:

netstat -m from backup RE before test starts:

 
vmstat -m from master RE after test ends:

netstat -m from master RE after test ends:
 
vmstat -m from backup RE after test ends:

netstat -m from backup RE after test ends:
 
3.16 Replication of Variable Length Message Sizes

 Goal: To test the replication of large message sizes

 Test Specific Setup: Same as 3.1
 
 Test Steps: 
 1. Run jsrutil on coneyisland master RE as a server app

    $ ./jsrutil -a nsr_server

 2. Run jsrutil on coneyisland backup RE as snooping app

    $ ./jsrutil -a nsr_client

 3. Run jsrpeer on propecia to establish 5 connections and send a
    single message of 100 bytes to master RE (5.5.5.5). Master RE
    will echo it back.

    $ ./jsrpeer 5.5.5.5
 
 4. Repeat above with differnet length sizes (option -l ).

    $ ./jsrpeer -m 0 -l 200 5.5.5.5
    $ ./jsrpeer -m 0 -l 2000 5.5.5.5

 Success Criteria: Data replication works correctly for different
 packet length.

3.17 Replication for connections/data in non-default Routing Instances (and Logical Routers) [TBD]

 Goal:

Test that socket replication is functional and without error when the connection 
is between a CE and PE in a routing-instance.

 Test Specific Setup: 

Setup is as described in section 2, with the following addendums:

* PE is the dual-RE box running the jsrutil applications.
* CE is the peer running jsrpeer.
* Add config to both REs on the PE to make it a PE in a routing-instance.
 
 Test Steps:

1.  Master and backup RE are running jsrutil.
2.  Peer is running jsrpeer.
3.  Open up separate windows to look at the syslogs on both machines.  Set
    the logging level on both REs to LOG_DEBUG for both the syslog and for
    JSR log messages.
4.  Interpret results.
 
 Success Criteria:

Replication of data from the peer should occur between the applications on 
the REs.

 Results:


4. "Socket Data Replication Layer" Specific Test Cases

4.1 Sockets are *not* (read/write) ready during init handshake

 Goal: Replicated sockets should not be read/write ready while init handshake is in progress. 
 
 Test Specific Setup: 
 
 Test Steps:
           a. Start replication on a primary socket.
           b. Do a select for readability/writability on the socket 
             OR
           b. Try to read or write to the socket.

 Success Criteria: Select should not return till the init handshake between primary and secondary
 RE is completed OR socket read/write should fail (for non blocking sockets with EWOULDBLOCK) or block (for blocking sockets) till init handshake is completed
   
 Result:  
   Select returns only when replication handshake is over and read/writes block till init handshake completes


4.2 Init time socket data

 Goal:Any data present in the primary socket's rcv/snd buffer at the time replication is initiated is replicated to the backup RE as part of the (SDRL) initialization handshake.
 
 Test Specific Setup: 
 
 Test Steps:
          a. Send a message to the peer application from the primary application
          b. Dont read back the response
          c. Start socket replication with the secondary
          d. Once the sockets are ready (init handshake is complete), read back the newly created socket pair on secondary RE
 
 Success Criteria: Server's response to the message should be sync'ed to the backup as part of init handshake and available for reading by the secondary application.
         
 Result:  Data is available to the secondary app.

4.3 TCP/UDP data reinjection

 Goal: TCP/UDP data is reinjected into the transport layer in primary after getting an ACK/NACK from 
       secondary or a timeout.

 Test Specific Setup: 
 
 Test Steps:
        1. Replicate data across REs 
        2. Monitor the state of the TCP PCB on primary and debug logs tracing replication activity        
 
 Success Criteria:
        1. netstat -V -f inet shows progress in the state of the TCP connection
        2. Peer gets all data
        3. Debug logs indicate that data is getting reinjected


4.4 State of socket replication buffer

 Goal:Replication buffer doesnt accumulate data over time as replication continues

 Test Specific Setup: 

        
 Test Steps:
        1. Replicate data across REs 
        2. Monitor state of replication buffer by looking at sr_stats

 Success Criteria:
        Size of socket replication buffer stays within limits of un'acked data.


5. "Packet Replication Layer/Protocol State Replication Module" Specific Test Cases

5.1	PRL

Resource leaks and crashes are covered in the more general sections above.
A lot of the tests below are very white-box by necessity.

5.1.1	PRL global initialization occurs when NSR is enabled

 Goal:

Global state for PRL should be initialized when NSR is enabled on either the 
master or backup REs.

 Test Specific Setup:

None.  Setup is as described in section 2.

 Test Steps:

1.  Open up separate windows to look at the syslogs on both machines.  Set
    the logging level on both REs to LOG_DEBUG for both the syslog and for
    JSR log messages.
2.  Activate NSR on both the master and backup REs.  They do not have to 
    happen at the same time, as the events should be independent of each other.
3.  Interpret results.

 Success Criteria:

* Break into the kernel using gdb and inspect all the PRL global variables 
  to ensure that they are set.
* Check the syslog to see if "PRL global state initialized" is printed in there.

 Results:

5.1.2	PRL per-socket initialization occurs when a socket is enabled 
	for replication

 Goal:
 
Per-socket state for PRL should be initialized when NSR is enabled on either the
master or backup REs, and the socket is enabled for replication.
 
 Test Specific Setup:

None.  Setup is as described in section 2.

 Test Steps:
 
1.  Open up separate windows to look at the syslogs on both machines.  Set
    the logging level on both REs to LOG_DEBUG for both the syslog and for
    JSR log messages.
2.  Activate NSR on both the master and backup REs.  They do not have to
    happen at the same time, as the events should be independent of each other.
3.  Activate replication on the test socket on both REs.
4.  Interpret results.
 
 Success Criteria:
 
* Break into the kernel using gdb and inspect all the PRL per-socket variables 
  to ensure that they are set.
* Check the syslog to see if "PRL per-socket state initialized for socket 
  0x<address>" is printed in there.
 
 Results:

5.1.3	PRL per-socket teardown occurs when a socket is closed or merged

 Goal:
 
Per-socket state for PRL should be destroyed when a replicated socket is 
closed or merged, on either the master or backup REs.

 Test Specific Setup:

None.  Setup is as described in section 2.

 Test Steps:

1.  Open up separate windows to look at the syslogs on both machines.  Set
    the logging level on both REs to LOG_DEBUG for both the syslog and for
    JSR log messages.
2.  Activate NSR on both the master and backup REs.  They do not have to
    happen at the same time, as the events should be independent of each other.
3.  Activate replication on the test socket on both REs.
4.  Merge or close the replicated socket.
5.  Interpret results.

 Success Criteria:

* Break into the kernel using gdb and inspect all the PRL per-socket variables
  to ensure that they are cleared/deallocated.
* Check the syslog to see if "PRL per-socket state cleaned up for socket
  0x<address>" is printed in there.

 Results:

5.1.4	PRL replicates incoming traffic from the peer to the secondary

 Goal:

On a replicated socket, PRL should replicate the incoming traffic from the 
peer from the primary socket to the secondary socket.

 Test Specific Setup:

None.  Setup is as described in section 2.

 Test Steps:

1.  Open up separate windows to look at the syslogs on both machines.  Set
    the logging level on both REs to LOG_DEBUG for both the syslog and for
    JSR log messages.
2.  Activate NSR on both the master and backup REs.  They do not have to
    happen at the same time, as the events should be independent of each other.
3.  Activate replication on the test socket on both REs.
4.  Send data from the peer.
5.  Interpret results.

 Success Criteria:

* Incoming traffic should appear at both the primary and secondary receive 
  socket buffers.  The data should be exactly the same.
* Incoming traffic should be replicated without excessive delay.
* Check the syslog to see if PRL replication message logs are printed in there.

 Results:

5.1.5	PRL timeout occurs after not receiving ack from secondary socket

 Goal:
  
On a replicated socket, PRL should timeout and unreplicate the socket if 
an ack for a PRL replicate message is not received in time (as set by 
the net.jsr.prl.timeout_value sysctl.
 
 Test Specific Setup:

None.  Setup is as described in section 2.
 
 Test Steps:
 
1.  Open up separate windows to look at the syslogs on both machines.  Set
    the logging level on both REs to LOG_DEBUG for both the syslog and for
    JSR log messages.
2.  Activate NSR on both the master and backup REs.  They do not have to
    happen at the same time, as the events should be independent of each other.
3.  Activate replication on the test socket on both REs.
4.  Send data from the peer.
5.  Do not allow secondary kernel to send ack for PRL replicate message 
    (will need a special debug kernel to do this).
6.  Interpret results.

 Success Criteria:

* The primary socket should unreplicate after the timeout occurs.
* Check the syslog to see if PRL replication timeout message logs are printed 
  in there.

 Results:

5.1.6	PRL does not fill up the syslog with too many messages in 
	default log setting

 Goal:

PRL should not pollute the syslog with too many messages on the default 
log setting.

 Test Specific Setup:

None.  Setup is as described in section 2.

 Test Steps:

1.  Open up separate windows to look at the syslogs on both machines.  Set
    the logging level on both REs to LOG_DEBUG for both the syslog and for
    JSR log messages.
2.  Activate NSR on both the master and backup REs.  They do not have to
    happen at the same time, as the events should be independent of each other.
3.  Activate replication on the test socket on both REs.
4.  Send data from the peer.
5.  Look at syslog to interpret results.

 Success Criteria:

* The syslog should not contain a lot of messages from PRL (for instance, 
  no per-packet messages should be in there).

 Results:

5.2	PSRM

Resource leaks and crashes are covered in the more general sections above.
A lot of the tests below are very white-box by necessity.

5.2.1	PSRM global initialization occurs when NSR is enabled

 Goal:
 
Global state for PSRM should be initialized when NSR is enabled on either the
master or backup REs.
 
 Test Specific Setup:

None.  Setup is as described in section 2.

 Test Steps:
 
1.  Open up separate windows to look at the syslogs on both machines.  Set
    the logging level on both REs to LOG_DEBUG for both the syslog and for
    JSR log messages.
2.  Activate NSR on both the master and backup REs.  They do not have to
    happen at the same time, as the events should be independent of each other.
3.  Interpret results.
 
 Success Criteria:
 
* Break into the kernel using gdb and inspect all the PSRM global variables 
  to ensure that they are set.
* Check the syslog to see if "PSRM global state initialized" is printed in there.
 
 Results:

5.2.2	PSRM per-socket initialization occurs when a socket is enabled 
	for replication

 Goal:

Per-socket state for PSRM should be initialized when NSR is enabled on either the
master or backup REs, and the socket is enabled for replication.

 Test Specific Setup:

None.  Setup is as described in section 2.
 
 Test Steps:
 
1.  Open up separate windows to look at the syslogs on both machines.  Set
    the logging level on both REs to LOG_DEBUG for both the syslog and for
    JSR log messages.
2.  Activate NSR on both the master and backup REs.  They do not have to
    happen at the same time, as the events should be independent of each other.
3.  Activate replication on the test socket on both REs.
4.  Interpret results.
 
 Success Criteria:

* Break into the kernel using gdb and inspect all the PSRM per-socket variables
  to ensure that they are set.
* Check the syslog to see if "PSRM per-socket state initialized for socket 
  0x<address>" is printed in there.
 
 Results:

5.2.3	PSRM per-socket teardown occurs when a socket is closed or merged

 Goal:

Per-socket state for PSRM should be destroyed when a replicated socket is
closed or merged, on either the master or backup REs.

 Test Specific Setup:

None.  Setup is as described in section 2.

 Test Steps:

1.  Open up separate windows to look at the syslogs on both machines.  Set
    the logging level on both REs to LOG_DEBUG for both the syslog and for
    JSR log messages.
2.  Activate NSR on both the master and backup REs.  They do not have to
    happen at the same time, as the events should be independent of each other.
3.  Activate replication on the test socket on both REs.
4.  Merge or close the replicated socket.
5.  Interpret results.

 Success Criteria:

* Break into the kernel using gdb and inspect all the PSRM per-socket variables
  to ensure that they are cleared/deallocated.
* Check the syslog to see if "PSRM per-socket state cleaned up for socket
  0x<address>" is printed in there.

 Results:

5.2.4	PSRM replicates protocol metadata from the primary to the secondary 
	socket before and during a merge on the secondary socket, or a close 
	on the primary socket

 Goal:
  
On a replicated socket, PSRM should replicate protocol metadata from the 
primary to the secondary socket before and during a merge on the secondary 
socket, or a close on the primary socket.
 
 Test Specific Setup:

None.  Setup is as described in section 2.
 
 Test Steps:
 
1.  Open up separate windows to look at the syslogs on both machines.  Set
    the logging level on both REs to LOG_DEBUG for both the syslog and for
    JSR log messages.
2.  Activate NSR on both the master and backup REs.  They do not have to
    happen at the same time, as the events should be independent of each other.
3.  Activate replication on the test socket on both REs.
4.  Send data from the peer.
5.  Merge the secondary socket, or close the primary socket.
6.  Interpret results.

 Success Criteria:

* When traffic from the primary socket is sent to the peer, the tcpcb should 
  be updated accordingly.
* When the close or merge occurs, selected tcpcb and inpcb fields should be 
  replicated from the primary to the secondary socket.  The other fields should 
  be initialized to proper default values on the secondary.
* Check the syslog to see if PSRM replication message logs are printed in there.
 
 Results:

5.2.5	PSRM timeout occurs after not receiving ack from other socket

 Goal:
 
On a replicated socket, PSRM should timeout and unreplicate the socket if 
an ack for a PSRM message is not received in time (as set by the 
net.jsr.psrm.timeout_value sysctl.

 Test Specific Setup:

None.  Setup is as described in section 2.

 Test Steps:

1.  Open up separate windows to look at the syslogs on both machines.  Set
    the logging level on both REs to LOG_DEBUG for both the syslog and for
    JSR log messages.
2.  Activate NSR on both the master and backup REs.  They do not have to
    happen at the same time, as the events should be independent of each other.
3.  Activate replication on the test socket on both REs.
4.  Send data from the peer.
5.  Do not allow secondary kernel to send ack for PSRM replicate message 
    (will need a special debug kernel to do this).
6.  Interpret results.

 Success Criteria:

* The primary socket should unreplicate after the timeout occurs.
* Check the syslog to see if PSRM replication timeout message logs are printed   
  in there.

 Results:

5.2.6	PSRM does not fill up the syslog with too many messages in 
	default log setting

 Goal:
PSRM should not pollute the syslog with too many messages on the default 
log setting.

 Test Specific Setup:

None.  Setup is as described in section 2.

 Test Steps:

1.  Open up separate windows to look at the syslogs on both machines.  Set
    the logging level on both REs to LOG_DEBUG for both the syslog and for
    JSR log messages.
2.  Activate NSR on both the master and backup REs.  They do not have to
    happen at the same time, as the events should be independent of each other.
3.  Activate replication on the test socket on both REs.
4.  Send data from the peer.
5.  Look at syslog to interpret results.
 
 Success Criteria:

* The syslog should not contain a lot of messages from PSRM (for instance, 
  no per-packet messages should be in there).
  
 Results:

6. "Initialization And Handle Allocation" Specific Test Cases

7. "Kernel to Kernel Communication Module" Specific Test Cases



8.  BOUNDARY TEST CASES

8.1 Out of Memory Test
8.2 Thousands of sockets test
8.3 Out of socket buffer space test        


9.  REGRESSION TEST CASES

{As necessary.  As with functional tests but for regression tests.
Include references to regression requests, who is performing them,
etc.}

10.  INTEROP TEST CASES

{As necessary.  As with functional tests but for interoperability.
Include references to any specs or details regarding the interop
target.}

11.  MIGRATION & COMPATIBILITY TEST CASES

{As necessary.  As with functinoal tests but for migration and
compatibiilty tests (any tests needed to show forwards/backwards
compatibility, to show existing configs will not break, to verify
upgrades and downgrades, etc.}

12.  TEST COVERAGE REMAINING

{Describe all test coverage areas which will be needed on the feature
which are *not* covered by this document.  Identify test inputs for
systest or other groups to proceed with.}

13.  DEFECTS REMAINING

{List PRs filed at dev complete for test failures from this plan
which were not fixed by dev complete.}
