$Id: l2vpn-vpls-design.txt,v 1.4 2006/09/24 18:44:42 bhupesh Exp $

		    NSR support for L2VPN and VPLS

			       RLI 3169

		       Design Specification
		
		Bhupesh Kothari (bhupesh@juniper.net)

Copyright (C) 2006, Juniper Networks, Inc.

NOTICE: This document contains proprietary and confidential
information of Juniper Networks, Inc. and must not be distributed
outside of the company without the permission of Juniper Networks
engineering.


1. INTRODUCTION

    
    The overall goals and requirements are described in
    [NSR-L2VPN]. This document describes the design to add NSR support
    for L2VPN in RPD only. There are three components in the system
    that hold key pieces of data that the L2VPN module in RPD depends
    on. The key pieces of information are:

    - signaling state: information contained in L2VPN NLRI such as
      labels. 

    - VT or LSI IFLs allocated on the primary for each remote PE in
      case of VPLS.


    A brief description of each component that holds the key
    information is described below.

    1. L2VPN: This of course is the component which implements both
       L2VPN and VPLS functionality. All the state related to VC
       connection, CE interfaces and dynamic interfaces is maintained
       by L2VPN.

    2. BGP: BGP will replicate the update messages that contain L2VPN
       NLRI on the backup RE. This will be used to reconstruct the
       label blocks for the peers in the same VPN, along with other
       information such as label block range and offset.

    3. Kernel: VT or LSI IFLs will be replicated by the kernel on the
       backup RE. This will be used to reconstruct the VT or LSI IFLs
       for the remote peers. 


2. HIGH LEVEL DESIGN


    The existing NSR infrastructure provides two libraries that can be
    used to replicate or reconstruct data on the backup RE. The two
    libraries provide a set of APIs that applications can use to
    mirror or replicate data.

    1. Mirroring API: This provides a set of APIs that applications
       can use to register data on the primary RE that needs to be
       mirrored on the backup RE. It provides APIs to add, delete and
       modify mirrored data on the backup RE. More details are in
       [MAPI]. 

    2. Juniper Kernel Socket Replication (JSR): This provides a set of
       APIs that applications on the primary RE can use to do socket
       replication. The idea is to have the secondary see all the
       protocol messages sent to and received from the network peers
       over a socket. The kernel currently provides the support for
       socket replication only for TCP. Details are provided in
       [JSR]. 


    An application can also opt to exchange information between the
    primary and backup RE on its own via sockets. It will the
    application's responsibility to maintain the socket for
    communication and the reliable delivery of the messages.

    L2VPN module relies on kernel to replicate all the IFL information
    present on the primary RE to the backup RE. L2VPN modules also
    relies on BGP to replicate all the messages it receives from and
    sends to its network peer on the backup RE. BGP puts all the
    messages with for L2VPN address family in the routing table
    <instance>.l2vpn.0. It will be sufficient for L2VPN to reconstruct
    everything that it needs on the backup RE from the kernel and BGP
    installed routes in <instance>.l2vpn.0 and BGP's rib-out. There is
    no need to either use MAPI or an internal socket to exchange any
    additional data between the primary and the backup RE.

    Note that there might be a need to use MAPI to support P2MP for
    VPLS. Since P2MP for VPLS is not yet supported, it is too early to
    speculate what all data will be needed to mirror on the backup
    RE. Once the design for P2MP is complete, it will be more clear
    and it might even turn out that L2VPN module would be able to
    reconstruct data related to P2MP on the backup RE based on the
    replicated state elsewhere (tag module, or even kernel). For this
    project there is nothing that will be added for P2MP. Any work
    required later for P2MP would be an add-on and would not require
    any rework of any NSR code done as part of this RLI.


3. DETAIL DESIGN


    As mentioned in Section 2, L2VPN module on the primary is not
    replicating data on its own. Thus the changes in the L2VPN module
    acting as the master will be minimal. Most of the changes
    required for the master role will involve setting up the
    initialization vectors differently than the standby. 

    The rest of this section details how the routes and state from the
    kernel are created or replicated on the backup RE. 


3.1 SIGNALING STATE

    The signaling state that needs to be created on the backup RE is:

    - Route Distinguisher

    - Site ID

    - Site Offset

    - Label block size

    - Label base

    - Status Vector


    The RD and site ID values are taken from what is configured on the
    backup RE. The site offset is calculated based on the site
    ID. Status vector is also calculated based on the VC and local
    interface state on the backup RE. The label block and label base
    are reconstructed from the information contained in the BGP
    rib-out. 

    L2VPN module adds routes in routing tables mpls.0 and
    <instance>.l2vpn.0. The routing table mpls.0 contains the
    pseudo-wire routes. BGP adds VPN routes in routing table
    <instance>.l2vpn.0 that it learns from its network peer.


3.1.1 Signaling state received from peers


    BGP creates targets specific rib-groups and add <instance>.l2vpn.0
    to its list of secondary rib list. The primary rib is the
    bgp.l2vpn.0. When BGP receives updates from its peer, it imports
    L2VPN address family routes in bgp.l2vpn.0 and also adds the
    routes to the secondary rib. L2VPN module will process the routes
    added by BGP in the secondary rib. 

    There is no change on the backup when processing routes in
    <instance>.l2vpn.0 that are added by BGP. 


3.1.2 Signaling state advertised to peers

 
    On the primary, L2VPN module add routes to <instance>.l2vpn.0
    which BGP picks and advertises to its peers. On the backup, the
    L2VPN module will reconstruct these routes based on the
    information in the BGP rib-out. L2VPN module will still add its
    routes to <instance>.l2vpn.0 but would do so based on the
    information in BGP L2VPN NLRI that the primary created.

    The L2VPN code will be hooked via function vector
    bgpaf_standby_outmetrics. 

    
3.1.3 Pseudowire routes


    The are two Pseudowire routes that are added in mpls.0 for each
    remote PE.

	   my-label -> IFL
	   IFL      -> inh (label stack has the peer-label)

    The label 'my-label' is what the primary advertised. This is
    reconstructed from the BGP rib-out. 

 
    The IFL in VPLS case would be either VT or LSI. For L2VPN, the IFL
    will be that of the actual interface (CE side and core
    side). For VPLS, L2VPN will learn about the dynamic IFL on the
    backup from the kernel. 

    The indirect nexthop 'inh' is created by L2VPN based on the
    indirect nexthop passed to it by BGP.  The label 'peer-label' is
    contained in the VPN route that BGP added in <instance>.l2vpn.0.
    

3.1.4 Labels


    There are two label allocation scheme currently in our
    implementation. One for VPLS and other for L2VPN. The primary
    requirement for the label allocation is that the backup should be
    able to allocate the exact same label blocks as primary has
    allocated for the remote sites. 


3.1.4.1 L2VPN    

    The label allocation in L2VPN is done with the primary goal of
    minimizing label wastage. Since a site in L2VPN does not have to
    be connected to all the other sites, label block sizes cannot be
    predicted. A constant label block size can waste a lot of label
    space. This document won't describe the complete label allocation
    for L2VPN, but it describes the issues related to NSR.

    From the NSR perspective, what is needed is a way to allocate
    particular label blocks on the backup RE. For L2VPN, the label
    block sizes can be different and in almost all cases are
    different. Thus a block allocation library needs to support
    allocation of arbitrary block sizes, in addition to specific label
    values. In VPLS, the blocks are always of the same size (currently
    eight) and thus the ID allocation can be simple and very efficient
    both in managing label space, and in creation and deletion of
    blocks. With arbitrary sizes, the label space can become
    fragmented and this is can be damaging if the label space is
    limited. The LSI block allocation library currently supports all
    the requirements except for arbitrary sizes, which it only
    supports partially. This library would be enhanced to fully
    support the allocation of any arbitrary size.

    The label space carved out for the block allocation on the router
    is 800,000 - 899,999. This is currently shared by VPLS and
    L2VPN. Thus a new ID space will be created for this label
    space. Any application that needs label blocks from this global
    space would need to use the new block library with the new
    identifier.

    The maximum label block size for L2VPN will be reduced to 256 from
    the current 512. This really has no functional impact. The reason
    to do this is because the block library supports internal pages of
    size 256. This can be easily changed to 512, however there is no
    harm in keeping 256 and most likely 256 is a more optimal value
    for the label space (less wastage: once a page is allocated for a
    particular block size, it will only allocate block size of that
    size. Only exception to this rule is for arbitrary block size
    which internally would still be managed based on one block
    size).


3.1.4.2 VPLS

    
    VPLS behaves differently with LSI and VT interfaces. VPLS sends
    label information to the kernel at the time of LSI creation. This
    was required for graceful restart. LSI label space is very limited
    and thus it is an absolute requirement to reuse labels (labels
    in use before the failure). 

    VPLS does not add label information when VT interfaces are
    created. So for graceful restart, after the failure, we just
    simply start allocating new labels. Note that since there is no
    prior knowledge of what labels were used, in all likelihood the
    labels allocated after the failure will not be the same as
    before. This implies that after the failure we could swap labels
    between different VPNs (and thus forwarding one customer's traffic
    to another). 

    The graceful restart behavior ties with the NSR. For NSR it is a
    requirement to resue the labels on the backup RE. However, it is
    not required to send the label information to the kernel for VT
    interfaces as the label information is contained in the BGP
    messages on the backup RE. It would however be better for code
    manageability to have one behavior for the both VT and LSI, if the
    broken case as explained above in itself is not sufficient as a
    valid reason to do so. 
    
    VPLS will now use the block library to allocate labels for VT as we
    need to allocate particular label blocks on the backup RE.


3.2 L2VPN IN STANDBY ROLE

    
    L2VPN has three input events to deal with on the backup RE -- CLI,
    callback during BGP rib-out processing, and lastly the kernel IFL
    notifications in case of VPLS. The three events can happen in any
    order on the backup RE.

    - CLI: The backup RE in steady state will process the CLI events
      as it does normally. However, when it comes to allocating labels
      and VT and LSI IFLs in case of VPLS, backup RE does behave
      differently than the primary RE. If L2VPN does not have the
      replicated information that it needs to allocate labels and
      VT/LSI IFLs, it needs to defer the processing of those instances
      until it has the necessary information (state transition to
      "pending/deferred". No state machine, so set a bit and put the
      instance on a work queue -- Patricia trie with key as instance
      ID and local site ID)

    - Callback from BGP: L2VPN will save the information from BGP
      rib-out and will trigger the re-evaluation process of any
      instance that is in the deferred state.

    - VT/LSI IFL notification: L2VPN already has the capability to
      save the information from the kernel in its internal data
      structures during a graceful-restart. This event now on the
      backup RE will trigger the re-evaluation process of any instance
      that is in the deferred state.


    Note that there is yet another event -- route flash for
    <instance>.l2vpn.0.  BGP adds routes in <instance>.l2vpn.0. This
    does not need any special handling.

    Primary and backup RE will have different function vectors to
    allocate labels. As explained in Section 3.1, same labels that are
    used on primary are allocated on the backup. For VPLS, there will
    be another function vector for allocating VT and LSI IFLs. 



3.3 RACE CONDITIONS

    In a steady state, there is no possibility of any race
    condition. During the switchover time, there are cases that needs
    to be reviewed.

    1. Possibility of pseudowire routes not being added by L2VPN

       If L2VPN fails to add pseudowire routes that exist in kernel,
       kernel would delete those routes once RPD sends the route
       updates to the kernel. If the kernel replicated a pseudowire
       routes, it implies that the primary added the routes. It also
       implies that PFE has been programmed with those routes. L2VPN
       will add the pseudowire routes if (i) CLI is in sync with the
       primary (ii) L2VPN has the label and IFL in case of VPLS.  
       
       Now there are three cases to consider.

       a) L2VPN will have the incoming label if BGP has replicated the
          BGP rib-out on the backup. If kernel on the primary fails to
          replicate the BGP message on the backup RE, L2VPN will not
          be able to allocate the same label as primary. L2VPN will
          allocate a new label in such a case. As for the allocation
          of IFL, there are two cases to consider. If the kernel
          replicated the IFL state on the backup RE, L2VPN will reuse
          the IFL, i.e. it will allocate a new label and will
          associate the newly allocated label with the IFL already
          present in the kernel. If the IFL state also was not
          replicated on the backup RE, then there is no other option
          but to allocate new label and a new IFL. 

       b) L2VPN will have the outgoing label if the BGP has replicated
          the incoming messages on the backup. The primary would
          create a pseudowire route when it processes the incoming
          message. Thus the creation of pseudowire route on the
          primary does not happen until the message has been
          processed. The incoming message on the backup will be
          replicated much "prior" to the creation of the pseudowire
          route on the primary.

       c) BGP rib-out has been replicated and kernel on the primary
          failed to program the PFE and also failed to replicate the
          information on the backup kernel.

	  In such a case, the network peer would receive our side of
	  label and would program its hardware accordingly and could
	  potentially start sending traffic. Since we failed to
	  program our side of hardware, traffic would be
	  dropped. However, L2VPN will create the pseudowire routes
	  with the labels that primary advertised. When RPD sends
	  these to kernel, kernel would simply create those routes. So
	  we would be able to recover correctly as the labels that
	  were advertised are the ones used after the failover. In
	  other words, we would simply create a new Pseudowire route
	  with the label that was advertised to the peers.

	During the switchover, BGP will drain its queue of outgoing
	messages. This will create the rib-out which will be in sync
	with the network peer. L2VPN will be called in to process each
	of the outgoing message. However, L2VPN will not act on these
	messages right away but instead will spawn a job to do its
	work. In such a scenario, where BGP has the information in its
	rib-out and kernel has the pseudowire routes, L2VPN might fail
	to add the pseudowire route during its initialization
	switchover process. If L2VPN fails to add pseudowire routes,
	kernel will delete those. However, this will be corrected as
	soon as the L2VPN gets a chance to work on the background
	job. Alternative would be to process everything right
	away. This can be feasible only if there are small number of
	instances that needs to be worked on. 


    3.  After the switchover, L2VPN will recalculate the CCC status
        and will send the status down to the kernel. Thus there is no
        potential of any mismatch of CCC status before and after the
        switchover. 


4  WORK ITEMS

    This section roughly lists the coding items coming out of the
    design. 

      1. make the block allocation library more generic. (labels, ifls,
         etc.). Take the LSI specific stuff (which most likely is just
         variable names) out of the library. 

      2. enhance the block allocation library to deal with arbitrary
         block sizes as efficiently as possible. This is needed due to
         variable L2VPN label block sizes.

      3. make the l2vpn code use the block allocation library for
         allocating label block. This is needed to allocate
         particular label blocks.

      4. make the VPLS use the block allocation library for allocating
         label blocks for VT interfaces. This is needed to allocate
         particular label blocks.

      5. See if VPLS can add label stuff as part of the VT interface
         creation. This will make the behavior same for LSI and
         VT. Not related to NSR but will make the code cleaner,
         especially now due to (4). 

      6. all the initialization stuff needed for primary and backup --
         different function vectors. Switchover function should simply
         change the vectors to be for primary.

      7. deal with 3 cases listed in Section 3.2. @@@ need to expand
         this as bulk of work is here.

      8. calculate BGP out metrics. callback to l2vpn code needs to be
         here.



APPENDIX
---------

    Current problems are listed below. Depending on the priority,
    fixes migh be made as part of the NSR work (status, if fixed or
    not, is listed below for each problem).
	
    P1. Label merging algorithm for L2VPN: The algorithm itself is OK
        but the implementation is not quite correct. It works and will
        always come up with a label block size to allocate with an
        offset. But the code could be made to follow the
        intention. For example, a local remote site with ID 1 and two
        remote sites with ID 3 and 5, according to the algorithm we
        should only allocate one label block of size 4 with offset 3
        so that it covers both remote sites - 3 and 5. However, the
        implementation will allocate two label block, each with size
        of 2. The logic to select if we need to merge label blocks to
        the right or to the left is not correct and is not clear what
        it is based on. Efficiency could be improved. It always
        iterates nine times (max block size can be 512 - 9 bits) every
        time it has some label block to work on irrespective of how
        many label blocks are present. So if we have just one label
        block, we would still iterate 9 times, even though there is
        nothing to merge with. In other cases, if we have already
        merged label blocks, we would continue to iterate and fail to
        merge any further (since there is nothing to merge with) until
        we hit the count of nine.

    S.  Rewrite. 
	[NOT DONE].


    P2. There is a label range that is advertised to the peers. There
        is also a label block size that is local to the host. For some
        reason the range is not equal to the size. It makes no sense
        to have these two different. Say if we allocate a label block
        size of 8. A remote site with a ID 5 is the max of all the
        remote IDs. So our range will be five. If a site with ID 6 is
        added, we would readvertise the message with the expanded
        label range. This is unnecessary because we already allocated
        labels. If we hadn't allocated labels, say to be more
        efficient for label space, then it would be OK to again
        allocate and readvertise. But if we already allocated labels,
        it makes no sense to not tell our peers about our range that
        is less than what we have already allocated. In other words,
        range should be equal to the label block size. It is also
        confusing in VPLS case to actually see a range other than 8
        even though we always allocate block size of 8.

    S.  Make the size equal to the range. 
	[NOT DONE].

    P3. There is no mechanism in place that makes sure that hardware
        is programmed and only then send advertisements to our network
        peers. As soon as a PE receives an advertisement from its
        network peer, it will install the pseudowire route and will
        start sending traffic. If the hardware on the remote peer is
        not yet programmed, the remote peer will drop traffic until
        the hardware gets programmed. Worse, if a PE fails to program
        the hardware correctly, we would blackhole traffic
        indefinitely as there is no feedback of the failure back to
        the control plane.

    S.  Not a VPLS specific problem. The infrastructure work to pass
	the hardware state is done elsewhere.
	[NOT DONE].
 
    P4. VPLS always sends an ADD for the IFL during graceful
	restart. Kernel sends an error (EXISTS) as the IFL alreayd
	exists. This should be fixed.

KRT Request: send len 100 v86 seq 0 ADD iflogical index 0 devindex 4 flags 8010 gen 0
KRT Request: send iflogical ADD: error: File exists (17)
KRT Request: send len 96 v82 seq 0 ADD iffamily index 0 devindex 4 af 45 flags 0 gen 0
KRT Request: send iffamily ADD: error: File exists (17)

    S.  Avoid sending IFLs adds for IFLs that already exist in the
	kernel. This applies to both the graceful restart and NSR case. 
	[DONE].


    P5. RPD_KRT_VPLS_IFL_MODIFY is called indefinitely. This starts to
	happen right after restart pending goes to complete. A quick
	look at the code shows that we are adding the timer
	repeatedly.

KRT Request: send len 76 v82 seq 0 CHANGE iffamily index 71 devindex 0 af 45 flags 0 gen 0
KRT Request: send iffamily CHANGE: error: ENOENT -- Item not found (2)
RPD_KRT_VPLS_IFL_MODIFY: Unable to modify VPLS-related state: -1 (errno 2)

     S. This can happen if an inteface is configured under VPLS
        instance and the interface does not have VPLS encapsulation
        configured. Avoid sending a VC update for an interface that 
        does not have VPLS encaps.
	[NOT DONE].  


    P6. VPLS shares the subunit space for both VT and LSI.

    S.  Seperate out the LSI and VT subunit space. Add LSI_VPLS as
	another application interested in allocating unqiue IDs. 
	[DONE].


    P6. Maintain temporal ordering of operations on label space on the
	standby. [rpd-nsr-coders archive has the full description]

    S.  Not yet decided on the solution.
        [NOT DONE]   



REFERENCES


[JSR]	      Juniper Socket Replication API
	      sw-projects/os/nsr/kernel/socket_replication_api.txt

[MAPI]	      Mirroring API
	      sw-projects/os/nsr/nsr-infrastructure.txt
