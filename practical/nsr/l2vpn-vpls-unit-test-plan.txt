$Id: l2vpn-vpls-unit-test-plan.txt,v 1.6 2007/11/22 10:36:42 srik Exp $

Copyright (C) 2006, Juniper Networks, Inc.

NOTICE: This document contains the proprietary and confidential
information of Juniper Networks, Inc., and must not be
distributed outside of the company without the
permission of Juniper Engineering.

1.  INTRODUCTION

     This document describes the unit test plan to test NSR support
     for L2VPN and VPLS. The NSR support for L2VPN depends on the BGP
     NSR functionality. In addition, NSR support for VPLS depends on
     the GRES support for VPLS. 

     Tracking RLI for this feature is 3169 and the tracking PR is 72239.

     Tracking RLI for BGP NSR is 2711 and tracking RLI for GRES for
     VPLS is 2043.

     Tracking RLI for BGP VPLS is 6289 and tracking RLI for L2VPN using
     LDP is 6292.

    The RLI also need to solve the problem of transient and permanent
    configuration inconsistencies between the Primary and Standby which 
    would lead to lot of race conditions as well as scale issues with BGP. 
    The approach taken to solve this was two fold:

        a. Force "system commit synchronize" to be set when nonstop-routing
           is configured so that configuration would be the same in the
           Primary and Standby.
        b. Synchronize the operation of Primary and Standby in terms of
           instance data structure creation, reading the associated 
           route-target configs and sending of BGP route-refresh messages
           to PE peers. This is described in detail in [1].
           The introduction is also presented here:

    1. Primary process SIGHUP first. It process delete of instances. It also
       adds instances and marks the new RIBs as OUT_SYNC. Instance changes are
       also processed the same way as it is currently.
    2. BGP processes delete route-targets only.
    3. Send primary_commit_done message to Standby.
    4. Standby waits for this message and SIGHUP before proceeding to process
       config.
    5. Standby processes the commit in the regular manner.
    6. Sends Standby_commit_done message to Active.
    7. Primary resets the OUT-SYNC bit in the ribs.
    8. BGP also processes route-target additions. It registers this rib for
       all groups within the instance. If the table is OUT_SYNC, it defers
       registering PE groups (which are in master instance).
    9. As part of (7), rt infra invokes registered callbacks (BGP as an example)
       that goes and registers all PE groups  with this rib
    10. We've deferred registering PE groups until Standby has also created
        the RIB and hence deferred sending PE updates automatically until the
        Standby is ready to snoop and insert those updates into instance ribs.
    11. auto-rd instances have been taken care of and an asynchronous callback
        provided which is invoked when the rd value gets assigned.
    12. route-refreshes happen during step (8) which means the Standby has
        created all instances RIBs by that time.
    13. Route-refreshes are tracked as written in the document so that the 
        Standby can figure out if a route-refresh needs to be sent after 
        Switchover.

2.  SETUP


           (pro5-c)
      PE2  -- P -- PE3
    (pro5-e)  |  (pro5-g)
	      |
	      PE1
           (pro5-d)


    PE1 and PE2 are dual RE systems.


Sample configuration is provided below. LDP is used to set up full
mesh of LSP among the PE routers. 

PE1: pro5-d
===========

routing-instances {
    green {
        instance-type l2vpn;
        interface t3-0/3/2.0;
        interface t3-0/3/2.1;
        interface so-1/0/2.0;
        interface t3-0/3/2.2;
        route-distinguisher 3:3;
        vrf-target tar:1:1;
        protocols {
            l2vpn {
                encapsulation-type frame-relay;
                site three {
                    site-identifier 3;
                    interface t3-0/3/2.0 {
                        remote-site-id 1;
                    }
                    interface t3-0/3/2.2 {
                        remote-site-id 2;
                    }
                }
                site four {
                    site-identifier 4;
                    interface t3-0/3/2.1 {
                        remote-site-id 1;
                    }
                }
                site mh-two {
                    site-identifier 2;
                    interface so-1/0/2.0 {
                        remote-site-id 1;
                    }
                }
            }
        }
    }
    vpls-green {
        instance-type vpls;
        interface fe-0/0/2.0;
        route-distinguisher 10.255.245.53:1;
        vrf-target target:11111:1;
        protocols {
            vpls {
                site-range 10;
                site green1 {
                    site-identifier 1;
                }
            }
        }
    }
    vpls-red {
        instance-type vpls;
        interface fe-0/0/2.1;
        route-distinguisher 10.255.245.53:2;
        vrf-target target:11111:2;
        protocols {
            vpls {
                site-range 10;
                site red1 {
                    site-identifier 1;
                }
            }
        }
    }
}



PE2: pro5-e
============

routing-instances {
    green {
        instance-type l2vpn;
        interface so-0/1/0.0;
        interface so-0/1/1.0;
        interface so-0/1/1.1;
        route-distinguisher 4:4;
        vrf-target tar:1:1;
        protocols {
            l2vpn {
                encapsulation-type frame-relay;
                site one {
                    site-identifier 1;
                    interface so-0/1/0.0 {
                        remote-site-id 2;
                    }
                    interface so-0/1/1.0 {
                        remote-site-id 3;
                    }
                    interface so-0/1/1.1 {
                        remote-site-id 4;
                    }
                }
            }
        }
    }
    vpls-green {
        instance-type vpls;
        interface ge-0/2/0.0;
        interface fe-0/0/3.0;
        interface fe-0/0/2.0;
        route-distinguisher 10.255.245.54:1;
        vrf-target target:11111:1;
        protocols {
            vpls {
                site-range 10;
                site green2 {
                    site-identifier 2;
                }
            }
        }
    }
    vpls-red {
        instance-type vpls;
        interface ge-0/2/0.1;
        interface fe-0/0/3.1;
        interface fe-0/0/1.0;
        route-distinguisher 10.255.245.54:2;
        vrf-target target:11111:2;
        protocols {
            vpls {
                site-range 10;
                site red2 {
                    site-identifier 2;
                }
            }
        }
    }
}

PE3: pro5-g
===========

routing-instances {
    green {
        instance-type l2vpn;
        interface so-1/1/3.0;
        interface so-1/1/3.1;
        route-distinguisher 6:6;
        vrf-target tar:1:1;
        protocols {
            l2vpn {
                encapsulation-type frame-relay;
                site two {
                    site-identifier 2;
                    site-preference 200;
                    interface so-1/1/3.0 {
                        remote-site-id 1;
                    }
                    interface so-1/1/3.1 {
                        remote-site-id 3;
                    }
                }
            }
        }
    }
    vpls-green {
        instance-type vpls;
        interface ge-0/1/0.0;
        route-distinguisher 10.255.245.56:1;
        vrf-target target:11111:1;
        protocols {
            vpls {
                site-range 10;
                site green3 {
                    site-identifier 3;
                }
            }
        }
    }
    vpls-red {
        instance-type vpls;
        interface ge-0/1/0.1;
        route-distinguisher 10.255.245.56:2;
        vrf-target target:11111:2;
        protocols {
            vpls {
                site-range 10;
                site red3 {
                    site-identifier 3;
                }
            }
        }
    }
}



    The complete configuration files are located at:
    /homes/bhupesh/test_setup/configs/vpls/pro5/all/




3.  FUNCTIONAL TEST CASES

Useful commands and traceoptions
--------------------------------
- Traceoptions can be enabled under protocol l2vpn/vpls that traces
  l2vpn/vpls specifics
- under routing-options traceoptions, the hidden flag "commit-synchronize"
  can be enaled that traces synchronized commit specifics. The hidden
  flag "commit-synchronize" under protocols bgp traceoptions can enabled
  for the same purpose. Its better to have the same trace filenames under
  "routing-options" and "protocols bgp" when enabling the "commit-synchronize"
  flag.

    It is recommended that the order be followed while testing as it
    would uncover basic problems early on.


3.1.  Label repository creation from BGP messages on standby RE.
----------------------------------------------------------------
	
    A label respository is created on the standby by snooping BGP
    messages that primary is advertising. Any advertisements that 
    primary sends to its peers should be replicated in this repository. 
    The output of 'show l2vpn replication label' will dump the repository
    contents on the standby. 

    Note that this repository is common for both L2VPN and
    VPLS as both use BGP for signaling. 
    
    Run the following test with at least two instance of each type -
    l2vpn and vpls.

1.  Add new instance. 
    Test Steps:
	1. Add a new l2vpn/vpls instance on the primary.
	2. Commit.
        3. Check whether the connection state is Up.
    Success Criteria:
        1. The connection state on the Standby should be Up.
        2. Entries should be present in the instance.l2vpn.0 table.
	3. The output of 'show l2vpn replication label' displays the label
           information for the RD corresponding the newly added instance
           on the primary.
    Results: PASS

    Note: Standby BGP creates phantom entries for every l2vpn prefix
    advertised to a PE neighbor. There are no phantom entries present on
    the Primary. They can be displayed using the "hidden" option in
    the "show route" command. Example:

    {backup}[edit]
    regress@pro9-b1# run show route table l2vpn1.l2vpn.0 hidden 

    l2vpn1.l2vpn.0: 6 destinations, 8 routes (6 active, 0 holddown, 2 hidden)
    + = Active Route, - = Last Active, * = Both

    101.0.0.0:1:101:1/96                
                        [Phantom/4294967295] 02:40:34
                          Unusable
    101.0.0.0:1:101:201/96                
                        [Phantom/4294967295] 02:40:34
                          Unusable


2.  Delete existing instance.
    Test Steps:
	1. Delete l2vpn/vpls instance on primary.
	2. Commit.
        3. The deleted l2vpn circuits shouldn't be displayed.
    Success Criteria:
        1. The deleted l2vpn circuits shouldn't be displayed.
        2. instance.l2vpn.0 table should be deleted. Use the command:
           show route table instance.l2vpn.0
	3. The output of 'show l2vpn replication label' does not display
           the entry for the RD corresponding to the deleted instance on
	   the primary. 
    Results: PASS

    Note: The instance.l2vpn.0 table doesn't go away immediately in the
    Standby. This is because the BGP "NSR optimization cache" is holding a
    reference to the phantom entries. The cache entries are cleaned up
    approximately 2 minutes later. The instance.l2vpn.0 tale is also
    removed at that time. This is by design and the normal behavior.

    Immediately after a instance delete operation, when we do a show on
    the Standby's instance table, there won't be any routes displayed.
    If we use the "hidden" option, the phantom entries are displayed.

    Example: (immediately after instance delete)
    regress@pro9-b1# run show route table l2vpn2.l2vpn.0 

    l2vpn2.l2vpn.0: 2 destinations, 2 routes (0 active, 0 holddown, 2 hidden)

    {backup}[edit]
    regress@pro9-b1# run show route table l2vpn2.l2vpn.0 hidden 

    l2vpn2.l2vpn.0: 2 destinations, 2 routes (0 active, 0 holddown, 2 hidden)
    + = Active Route, - = Last Active, * = Both

    101.0.0.0:2:102:1/96                
                        [Phantom/4294967295] 00:18:14
                          Unusable
    101.0.0.0:2:102:201/96                
                        [Phantom/4294967295] 00:18:14
                          Unusable

    {backup}[edit]


    after approx 2 minutes:
    {backup}[edit]
    regress@pro9-b1# run show route table l2vpn2.l2vpn.0 hidden    
    error: Routing Process: No routing tables matching specification.


3.  Change route-distinguisher.
    Test Steps:
        1. Change the route-distinguisher under an instance.
        2. Commit.
        3. instance.l2vpn.0 table shows new prefixes for the connections
           with the new rd-value. The old prefixes (with old rd-value) have
           been deleted.
        4. Check if the connections come up.
	3. Look at the output of 'show l2vpn replication label'.
    Success Criteria:
        1. The Standby's instance.l2vpn.0 table gets modified in the same way.
        2. The connection state should match the Primary.
        3. The output of 'show l2vpn replication label' 
           confirms the new label advertisement for the RD just added on
           the primary.
    Results: PASS


4.  Change site ID.
    Test Steps:
	1. Change site ID for a l2vpn/vpls instance on primary.
        2. Commit.
    Success Criteria:
	The output of 'show l2vpn replication label' displays the
	label advertisement for the new site ID. The advertisement for
	the old site ID is deleted from the repository.
    Results: PASS


5.  Delete site
    Test Steps:
	1. Delete site from a l2vpn/vpls instance on primary.
        2. Commit.
    Success Criteria:
	The output of 'show l2vpn replication label' confirms that the
	label advertisement for the deleted site in no longer in the
	repository. 
    Results: PASS


6.  Add site.
    Test Steps:
	1. After test # 5, add a new site for the same l2vpn/vpls
	instance on the primary.
        2. Commit.
    Success Criteria:
	The output of 'show l2vpn replication label' displays the new
	label advertisement for the newly added site.
    Results: PASS


7.  Configure 'no-tunnel-services'
    Test Steps:
	1. Configure 'no-tunnel-serives' for a VPLS instance on the
	primary. 
        2. Commit.
    Success Criteria:
	The output of 'show l2vpn replication label' displays new
	label values due to LSI label range. 
    Results: PASS

	
8.  Delete 'no-tunnel-services'
    Test Steps:
	1. Continuing test #7,  delete 'no-tunnel-serives' for the same VPLS
	instance on the primary. 
        2. Commit.
    Success Criteria:
	The output of 'show l2vpn replication label' displays new
	label values as LSI label space is no longer in use.
    Results: PASS
	

3.2. Auto-rd instances
----------------------

    The rd-value for these instances are generated using a combination of the 
    route-distinguisher-id configured and the kernel-id. l2vpn instances
    don't have a kernel-id since they aren't forwarding instances. For
    these instances, the Primary rpd generates an "id" and this "id" is
    synced to the Standby so that the Standby can generate the same rd-value
    as the Primary.

    Note: For these tests, a route-distinguisher-id value needs to be 
          configured under routing-options.

1. Add new instance
        Same procedure as documented in section 3.1.
    Results: PASS

2. Delete existing instance
        Same procedure as documented in section 3.1.
    Results: PASS

3. Configure route-distinguisher
        1. Configure a route-distinguisher value under a routing instance.
        2. Check to see if prefixes are present with the new-rd value in
           instance.l2vpn.0 table.
        3. Check if connections come up.
    Success Criteria:
        1. The Standby's instance.l2vpn.0 table gets modified in the same way.
        2. The connection state should match the Primary.
        3. The output of 'show l2vpn replication label' 
           confirms the new label advertisement for the RD just added on
           the primary.
    Results: PASS

4. Delete route-distinguisher
        1. Delete the route-distinguisher configured on a routing instance.
        2. Check to see if the prefixes are present with the new-rd value
           (auto-generated) in instance.l2vpn.0 table.
        3. Check if connections come up.
    Success Criteria:
        1. The Standby's instance.l2vpn.0 table gets modified in the same way.
        2. The connection state should match the Primary.
        3. The output of 'show l2vpn replication label' 
           confirms the new label advertisement for the RD just added on
           the primary.
    Results: PASS


3.3  IFL repository creation from kernel state. 
-----------------------------------------------
	
    This applies to VPLS only as L2VPN does not create dynamic
    IFLs. The output of 'show vpls replication dynamic-interfaces'
    will dump the repository contents on the standby. 

    The following set of tests in this section should be run once with
    'no-tunnel-services' configured (LSI) and once without it (VT). 

    Run the following test with at least two VPLS instances.
    
1.  Add new instance.
    Test Steps:
	1. Add a new VPLS instance on the primary.
        2. Commit.
    Success Criteria:
	'show vpls replication dynamic-interfaces' displays the new
	LSI/VT on the standby.
    Results: PASS


2.  Delete existing instance.
    Test Steps:
	1. Delete the instace created in step #1.
        2. Commit.
    Success Criteria:
	'show vpls replication dynamic-interfaces' confirms the
	deletion on the LSI/VT on standby. 
    Results: PASS


3.  Delete site.
    Test Steps:
	1. Delete a site for an existing VPLS instance.
        2. Commit.
    Success Criteria:
	'show vpls replication dynamic-interfaces' confirms the
	deletion of the LSI/VT on standby.
    Results: PASS
	

4.  Change site ID.
    Test Steps:
	1. Continuing test #3, add the site back. 
        2. Commit.
	3. 'show vpls replication dynamic-interfaces' 
	4. Change the site ID.
        5. Commit.
	6. 'show vpls replication dynamic-interfaces' 
    Success Criteria:
	'show vpls replication dynamic-interfaces' at step #3 confirms
	the addition of new LSI/VT on standby. 'show vpls replication
	dynamic-interfaces' at step #6 confirms the deletion of the
	LSI/VT for the old site ID and the addition of the LSI/VT for
	the changed site ID.
    Results: PASS
	

7.  Add or delete 'no-tunnel-services'
    Test Steps:
	1. Either add or delete 'no-tunnel-services' on the primary as
	the case might be. 
        2. Commit.
    Success Criteria:
	'show vpls replication dynamic-interfaces' displays the new
	LSI/VT due to the change in configuration on the primary.
    Results: @
	

8.  Change Route Distinguisher.
    Test Steps:
	1. Change the RD of a VPLS instance on the primary.
        2. Commit.
    Success Criteria:
	'show vpls replication dynamic-interfaces' confirms the
	deletion of the old LSI/VT and addition of the new LSI/VT for
	the new RD value. 
    Results: PASS
	

9.  Offline/online Tunnel PIC
-----------------------------
    Test Steps:
	1. Do 'request chassis pic pic-slot <n> offline/online'
    Success Criteria:
	'show vpls replication dynamic-interfaces' confirms the
	deletion/addition of the VT IFLs.
    Results: @

	
3.4 PW status
-------------
	
    The output of 'show l2vpn/vpls connection' will show the
    connection status along with other information such as label
    base and IFL subunits. 

    After confirming that repositories are in sync with the primary
    BGP advertisemnts and IFL state, the PW status for the instances
    configured on the standby needs to be checked. 


3.4.1 PW status due to configuration changes on primary.
--------------------------------------------------------

    The list below provides the set of basic tests. The steps are same
    or similar to the ones already outlined in Section 3.1 and 3.2 and
    are thus omitted here to avoid unnecessary repetition. Configure
    at least two instances of each type - l2vpn and vpls.

    The success criteria for each of the test below is that the
    connection status on the standby is same as what primary has.

1.  add/delete existing instance.
2.  add/delete site.
3.  change site ID.
4.  add/delete 'no-tunnel-services'.
5.  add/delete/change route-distinguisher.

6.  clear bgp session with PE neighbor

        For this particular test case, we can have two PE-RRs
        as neighbors. This case is different than deleting the instance or
        a site since BGP doesn't send out withdraws to PE peers since the
        connection itself goes down. In order for l2vpn on the Standby to
        clean its state, BGP informs l2vpn when it a l2vpn prefix is no longer
        advertised to any PE peers (since all of them are down).

    a:
    Test Steps:
	1. Bring down one bgp session.
        2. Prefixes are still present in the instance.l2vpn.0 table.
        3. PW connection state is unaffected.
    Success Criteria:
        1. Prefixes are still present in the Standby's instance.l2vpn.0 table.
        2. labels remain unchanged in the Standby.
        3. PW connection state is unaffected in the Standby.

    b:
    Test Steps:
	1. Bring down the other bgp session.
        2. Prefixes are still present in the Primary instance.l2vpn.0 table.
        3. PW connection state is down.
    Success Criteria:
        1. The labels for the PW connections are cleaned up in the Standby.
        2. PW connection state is down in the Standby.
        3. Standby l2vpn cleans up the prefixes in instance.l2vpn.0 tables
           show route instance.l2vpn.0 table on the Standby shows empty.
        4. BGP cleans the phantom entries from instance.l2vpn.0 table
           in the Standby.

    c:
    Test Steps:
	1. Bring up both the bgp sessions.
        2. PW connections come up.
    Success Criteria:
        1. PW connections come up in the Standby.
        2. The labels for the PW connections are learnt by the Standby.
        3. Prefixes get added in Standby's instance.l2vpn.0 table.

7.  clear ldp nbr.
8.  deactivate/activate local CE facing interface
9.  change encapsulation for CE facing interface (applies to L2VPN only). 
10. add/delete multi-homing site

  
    +---+---+---+---+---+---+---+---+------+
      1   2   3   4   5   6   7   8   9  10
    +---+---+---+---+---+---+---+---+------+
      P   P   P   P   P   P   @   @   @  @


3.4.2 PW status due to configuration changes on remote peer
-----------------------------------------------------------

    The list below provides the set of basic tests. The steps are same
    or similar to the ones already outlined in Section 3.1 and 3.2 and
    are thus omitted here to avoid unnecessary repetition. Configure
    at least two instances of each type - l2vpn and vpls.  The success
    criteria for each of the test below is that the connection status
    on the standby is same as what primary has.

    Run the following on the remote peer and look at 
    'show l2vpn/vpls connection' on both primary and standby.
    
1.  add/delete existing instance.

    Results:
        Add:    Pass
        Delete: Fail. Standby rpd on DUT cores. (PR 231234)
        
2.  add/delete site.
3.  change site ID.
4.  add/delete 'no-tunnel-services'.
5.  add/delete/change route-distinguisher.
6.  clear bgp session
7.  clear ldp nbr.
8.  deactivate/activate local CE facing interface 
9.  change encapsulation for CE facing interface (applies to L2VPN only). 
10. reduce site range to something below the local site ID configured
    on DUT.	   
    
    
	
Results:
	
    +---+---+---+---+---+---+---+---+---+---+
      1   2   3   4   5   6   7   8   9   10
    +---+---+---+---+---+---+---+---+---+---+
      F   @   @   @   P   @   @   @   @   @

    
    
3.5 Switchover 
--------------
    
3.5.1 Switchover in consistent state
------------------------------------

    This section lists the tests that should be performed for testing
    RE switchover.
    
1.  Test Steps:
	
	1. Configure multiple l2vpn and vpls instances on the master. 
	
        2. Commit.

	3. Compare the 'show l2vpn/vpls connections' on master and
	   standby and make sure the PW status match. If not, then
	   look at the standby IFL and label repository state, routes
	   in instance.l2vpn.0 and in bgp.l2vpn.0

	4. Do 'show route table bgp' and 'show route table instance'
	   on standby. Start sending traffic across l2vpn and vpls
	   connections. 
	
	5. Do 'request chassis routing-engine master acquire' on
	   standby.

	6. Do 'show route table bgp' and 'show route table instance'
	   on the new master. Make sure that the routes are still in
	   the routing tables. Check the age of the routes to make
	   sure routes were not deleted during switchover.

	7. Check the PW status. It should show the same state as prior
	   to switchover. 

    Success Criteria:
	No traffic loss and no flaps in PW connection state. 	
	
    Results: PASS
    

3.5.2 Switchover with multihoming sites.
----------------------------------------

    Run the test in Section 3.4.1 with an addition of a multihoming
    site for VPLS. The success criteria remains the same as in Section
    3.4.1 . 

    Results: @


3.5.3  Switchover multiple times.
---------------------------------

    Run the tests in Section 3.4.1, 3.4.2, and 3.4.3 multiple times.
    The success criteria remains the same as in these sections.


    Results: PASS

	
3.6  Cross functional test cases
--------------------------------


3.6.1  VPLS termination in L3VPN
--------------------------------	
   
    Results: @

3.6.2. 	L2ckt termination into VPLS
-----------------------------------
 
    Note that L2ckt is not yet NSR ready. After a switchover, L2ckt
    connections will be dropped. However, VPLS connections should
    stay up. 
	
    Results: @

3.6.3  Restart routing immediate on primary
-------------------------------------------

   Test Steps:
       1. Configure multiple l2vpn/vpls instances on all the PEs. 

       2. Run 'show l2vpn/vpls connection' on the primary and on the
	  standby. The output shows the connectin in UP state.

       3. Run 'restart routing immediate' on primary.

       4. The old l2vpn/vpls connections go away and the primary
          reestablishes all the l2vpn/vpls connections after the
          restart.  Do 'show l2vpn/vpls connection'.
 
    Success Criteria:

        The standby should reestablish all the l2vpn/vpls connections
        after the primary restart. 'show l2vpn/vpls connection', 'show
        route table <foo>', 'show route table mpls' match both on
        primary and standby.

        Implementation detail: When Standby BGP cleans up RIB-OUT state, it
        notifies l2vpn which would result in Standby l2vpn cleaning up its
        state and removing the previously advertised prefixes from
        Standby's instance.l2vpn.0 table. When the Primary BGP re-establishes
        the session(s) with PE peers and advertisements are sent out,
        Standby l2vpn rebuilds its state.

        Note: NSR is lost when Primary rpd restarts. There is no Switchover
        to maintain NSR.
	
    Results: FAIL

    For VPLS, connections go into NP state.


3.6.4  Restart routing on standby
---------------------------------	

   Test Steps:
       1. Configure multiple l2vpn/vpls instances on all the PEs. 

       2. Run 'show l2vpn/vpls connection' on the primary and on the
	  standby. The output shows the connectin in UP state.

       3. Run 'restart routing' on standby.

    Success Criteria:

        The standby should reestablish all the l2vpn/vpls connections
        after the restart. 'show l2vpn/vpls connection', 'show route
        table <foo>', 'show route table mpls' match both on primary
        and standby.

	
    Results: PASS



3.6.5  Restart routing on remote peer
-------------------------------------

   Test Steps:
       1. Configure multiple l2vpn/vpls instances on all the PEs. 

       2. Run 'show l2vpn/vpls connection' on the primary and on the
	  standby. The output shows the connectin in UP state.

       3. Run 'restart routing' on the remote peer.

    Success Criteria:

        The primary and stadby reestablishes all the l2vpn/vpls
        connections after the restart on the remote network peer.
        'show l2vpn/vpls connection', 'show route table <foo>', 'show
        route table mpls' match both on primary and standby.

    Implementation detail:
        This is similar to BGP's PE sessions going down which would end
        up in Primary and Standby BGP cleaning state. See details in
        testcase 6 in section 3.4.1.
	
    Results: PASS


3.6.6 Route-Reflector configuration
-----------------------------------
1.  Change BGP from a PE to PE-RR.

    Success Criteria:
        1. PW connection state is unaffected.
        2. labels remain unchanged in the Standby.
	
    Results: PASS

2.  Change BGP from PE-RR to PE.

    Success Criteria:
        1. PW connection state is unaffected.
        2. labels remain unchanged in the Standby.
        3. Prefixes are still present in the Standby's instance.l2vpn.0 table.
	
    Results: PASS


3.6.7 Reboot Standby
--------------------

3.6.8. Deactivate PE neighbor(s).
---------------------------------
    Test Steps:
        1. deactivate PE neighbor(s) under "protocols bgp"
        2. All PW connections go down
        3. Primary RE has local routes in instance.l2vpn.0 tables.
    Success Criteria:
        1. PW connections go down in the Standby.
        2. All prefixes in the Standby instance.l2vpn.0 tables are removed.
        3. Standby L2vpn cleans up its label/dynamic interface state.
	
Results: PASS

3.6.9. Deactivate nonstop-routing
---------------------------------
1. deactivate

    Test Steps:
        1. With nonstop-routing configured and both rpd's up, perform
           "deactivate routing-options nonstop-routing".
        2. commit
    Success Criteria:
        1. Standby rpd exits
        2. PW state doesn't change on the Primary rpd.

    Results: PASS

2. activate

    Test Steps:
        1. After the above test (deactivate), perform
           "activate routing-options nonstop-routing".
        2. commit
    Success Criteria:
        1. Standby rpd starts up and rsyncs state with Primray.
        2. PW state is replicated to Standby

    Results: PASS

3. change configuration

    Test Steps:
        1. After above test (activate), make changes to the config
           (either add/delete routing instances).
        2. commit
    Success Criteria:
        1. Commit is successful.
        2. PW state for the added/deleted instances are replicated to Standby.

    Results: PASS

	
4. BOUNDARY TEST CASES 
 
    The purpose here is to test the control plane functionality. Its a 
    non-goal to test forwarding plane (traffic) for the affected instances/
    sites.

1. Restart Primary rpd in the middle of a commit

   Results: PASS

2. Restart Standby rpd in the middle of a commit

   Results: PASS

3. RE Switchover during the middle of a commit

   Results: PASS

4. Mix/match multiple operations (add/delete/change) in a single commit

   Results: PASS

5. Delete a routing instance and commit followed by an immediate add of
   the same routing-instance and commit.

   Results: PASS

6. Deconfigure "protocols bgp" and commit.

   Results: PASS

7. Deconfigure "protocols" and commit.

   Results: PASS

Success Criteria:
    After the restart/switchover the new commit is effective on both
    the Primary and Standby. In other words, PW newly created/changed
    connections are in the Up state and the deleted PWs are no longer
    present.

Results: PASS
	
5. REGRESSION TEST CASES 
 
    Enough code has been changed in the following areas that require
    retest of the existing feature set.


    o VPLS Label and subunit allocation

      The underlying block ID library has gone some changes. This
      impacts any application that was using block ID library which
      until now is only VPLS and L2VPN for label and subunit
      allocation for dynamic IFLs.

    o L2VPN label allocation

      Until now L2VPN was not using block ID library to allocate label
      blocks. For NSR, this has been changed and now L2VPN also uses
      block ID library to allocate label blocks. 

    o Graceful-Restart for VPLS

      Both IFL and label repository has been changed. 

    o Subunit space for VT and LSI has been seperated.

      This was a bug introduced in 7.6. Since the VT and LSI subunit
      spaces have been seperated, the LSI and VT IFLs could now have
      identical subunits as they are coming out of their own subunit
      space. 

 
6. INTEROP TEST CASES 
 
None. 

	
7. MIGRATION & COMPATIBILITY TEST CASES 
 
None. 

	
8. TEST COVERAGE REMAINING 
 
   All tests with Results mared as '@' are incomplete or half
   done. Some of these marked with '@' are only done for L2VPN and not
   for VPLS. 
	

	
9. DEFECTS REMAINING 


10. Review Comments

	
References
----------
[1]. sw-projects/os/nsr/instance_sync_issue.txt
