$Id: pim-nsr-designspec.txt,v 1.3 2008/04/02 16:58:59 ravis Exp $ tag:

                     NSR: PIM stateful replication
                     Design Specification

                     Ravi Singh <ravis@juniper.net>


Copyright (C) 2007, Juniper Networks, Inc.

NOTICE: This document contains proprietary and confidential
information of Juniper Networks, Inc. and must not be distributed
outside of the company without the permission of Juniper Networks
engineering.



1. INTRODUCTION

This document describes the design for NSR stateful replication for PIM.
The goals and functionality supported for this RLI are described in the
functional specification [PIM-NSR].

This document starts by briefly mentioning the various approaches considered
for PIM NSR and the strengths/drawbacks of each approach. Thereafter the
document goes on to describe the design details based on the chosen approach
for PIM NSR.




2. DESIGN OVERVIEW

The central aim of the NSR exercise is to make the failover as seamless as
possible. However, there can be no guarantees made in terms of packet losses.

Thus, the backup needs to be able to take-up participation in the
control protocols as closely as possible to where the old-master left-off.
However, it is not possible to ensure that the backup will be in lock-step
with respect to where the master left off. Thus there will be many cases
where there will be packet losses.


2.0 DESIGN PHILOSOPHY
1. Impact on single-RE code to be kept to a minimum.
2. Separation of NSR code from single-RE code: this is to ensure that the
   changes being made for NSR do not impact the single-RE code. It is NOT
   acceptable for NSR to break single-RE PIM functionality under any cases.


2.1 APPROACHES - CONSIDERED; AND CHOSEN


2.1.1 Do nothing special for NSR for PIM: 
Since PIM & IGMP are periodic protocols, most of the control state would 
eventually recover after failover, not doing NSR for PIM & IGMP would not have 
any lingering long-term adverse effects. In this approach, graceful-restart 
could be used to maintain forwarding state for mcast flows that are alreading 
passing through the router. Any new mcast flows hitting the router would not 
be handled until the graceful restart period is over - this is the major 
drawback of this approach.


2.1.2 Purely tandem approach - socket replication approach: 
In this approach, backup rpd builds it PIM state based on the incoming 
PIM traffic from neighbors & sniffs PIM control traffic sent by the master
rpd. The backup rpd does not send out any PIM control pkts. However, it
still runs its state machines independently of the master.

This approach would rely on a pair of replicated sockets, where the 
kernel socket replication provides support for the backup rpd to sniff the
PIM pkts received from the nbrs and PIM pkts sent by the master by sniffing
on the receive & send replicated sockets, respectively.

Additionally, the backup rpd would be able to see the route and nexthop
changes made by the master rpd due to kernel route replication.

The kernel socket replication code guarantees that the replicated 
receive-socket buffer on the backup shall be populated with the received 
pkt before the socket-buffer on the master. However, this does not and can 
not provide any guarantees about the order in which the backup and master 
rpds process the contents from the socket buffers.
Additionally, the kernel socket replication code guarantees that the 
send-socket buffer on the backup rpd's replicated send socket shall be 
reliably populated before the relevant msg from the master rpd's replicated 
send-socket buffer is actually transmitted.

While the kernel socket replication by itself shall provide the contents of
the raw packets to the backup, it would be highly onerous for the backup to
keep track of PIM control state the master has or has not sent out.
This is further aggravated in setups where there likely will be config
mismatches.

For protocols using a TCP-based-transport where the configs are guaranteed to
be the same, for the backup to determine the exact state that the master 
sent out before a failover is somewhat simplified due to having a shared &
reliable sequence number between the 2 REs for the replicated send-socket.

However, this is not an easy proposition for PIM which uses an unreliabe
raw IP transport. Also, config mismatches are very much a real issue.
Thus, the socket-replication approach is not a tenable solution. Nor is any
other approach that relies on socket replication along with any other
scheme.

If PIM were to use socket replication for NSR, it would suffer from the
following major drawbacks:
          - Not know deterministically about the control state that the master
            has already sent to the neighbors.
          - Out-of-sync scheduling: tandem approach would rely on each of the
            2 rpds running their own state machines based on the received
            control packets. Due to local (pre-RE) scheduling constraints,
            the backup could actually be ahead of the master in its processing
            code & thus may not send out some control state until 1
            re-transmission timeout.
          - Not being able to handle config mismatches.

2.1.3 State synchronization by mirroring:
Master rpd syncs condensed state info to the backup RE based on which the 
backup generates the relevant state. The backup RE does not generate any 
control pkts. Nor does it receive any PIM control packets from the neighbors.

The master and the backup use certain pre-determined message types to
replicate state that the backup itself is not capable of generating.

The master rpd sends to the backup ALL state that it believes that the backup
rpd needs. Even if the backup rpd's config does not allow it to use the state
received from the master to populate its data structures, it stores the
PIM state received from ther master. Later, when (if) the config and the
interface/instance state allows the backup to install the received state into
its PIM data structures, it shall.

Additionally, whenever the config on the backup rpd allows it to process any
of the state received from the master that it could not use before (due to a
lack of config), it uses that state to build its data structures.
Similar high-level logic shall apply to state received by the backup from the
master that was unusable due to unicast routing state being different on the
backup with respect to that on the master. Any unicast routing changes on the
backup that make the previously-unusuable state (received from the master)
usable now, shall be used to populate the data structures on the backup.

At switch-over, the backup shall re-evaluate the unresolved state (state
received from the master that the backup found unusable due to config or due
to unicast routing) and delete any still-unresolvable state.

Additionally, at switch-over the backup shall start its state machines based
on the replicated state it had received from the master rpd before it went
offline.

In some instances, mirroring the entire protocol packets as seen by the master
might be the most efficient way of replicating state to the backup.

Whatever sync state that still exists on the master in its mirror 
queues would be lost on failover.

The mirroring approach has been chosen over the socket replication approach.




3. DESIGN - Summary

PIM on the backup does not do the following:
    - Send any PIM control packets on the interfaces.
    - Receive any PIM control packets on the interfaces.
    - Create any dynamic interfaces (pe/pd/mt).
    - Install any routes into the forwarding table.
 
PIM on the backup does the following:
    - Install routes in inet.1. However these are not installed in the
      forwarding table. At any time, inet.1 on the backup contains the routes
      that were installed by the master (replicated to the backup by GRES
      kernel replication) and the routes added by the backup into inet.1. 
      The backup also installs the routes into inet.1 so that the krt 
      infrastructure can do the route updation at switchover, if needed. 
      At failover, the krt infrastructure code walks the inet.1 routes &
      compares the routes installed by the old master and the routes installed
      by the new-master. Depending on the differences between the new-master's
      inet.1 route-set and the old-master's inet.1 route-set, the krt
      infrastructure does the right thing in terms of adding/deleting or
      updating the routes in the forwarding table.
    - Receives the replicated PIM control state from the master & uses that
      to populate its data structures, if relevant as per its config and as 
      per its unicast routing decisions.

This section describes the PIM NSR design by breaking the whole design into
sub-sections and dealing with each component of PIM in terms of processing
    - before failover - on the master and on the backup; and
    - after failover - this shall be only for the old-backup RE (and
      what has now become the new master).

Each sub-section of the detailed design describes how config mismatches are
handled. Special processing for handling config mismatches is required only on
the backup. Such processing shall be performed by the backup both before a
failover and also after a failover. Each sub-section shall independently
describe, if needed, the processing needed to deal with config mismatches.

Additionally, a later sub-section describes the post-failover processing
again - this time in collation with the post-failover processing for all
sub-components.


4. Modules
4.1 PIM mirror module (New)
  Manages the following:
   - Creation/deletion of PIM-NSR context:
       + The master creates the PIM-NSR context only if NSR is enabled. When
         NSR is disabled, the master destroys the NSR context by using a job to
         first explicitly delete the replicated state on the backup.
         The master uses the PIM-NSR context as a 
       + The backup creates the PIM-NSR context even if PIM is not enabled. The
         backup rpd would exist only if NSR is configured. However, there is a
         possibility that PIM on the master might get configured & PIM on the
         master might try to mirror state to the backup before PIM has been
         configured on the backup. To handle this situation, a place-holder is
         needed on the backup to store the replicated state. The PIM-NSR 
         context acts as this place-holder on the backup.
   - Registration of MAPI callbacks.
   - Initial state sync & subsequent resyncs of state to the backup:
         + ISS job: this job is needed to replicate the state to the backup 
           when the mirror connection goes. It is not a good idea to rely on
           the getnext() callback from MAPI as:
             - It does not provide a way for ensuring ordering
               between initial/resync replication of various clients
               that have inter-dependencies with each other.
             - It does not have a way to postpone the initial-sync/reync 
               for a MAPI client until after the client is ready to 
               actually start the initial-sync/resync. Using the 
               getnext() mechanism would cause a MAPI client to indicate
               no state to sync if the getnext() handler is called
               before the client is ready to replicate any state.
               Using an ISS job provides a way to solve the above 2 issues.
               However, for the ordering issue, the interdependent MAPI 
               clients' data could still show up out of order in the specific
               cases where the Resync of RDB state to the backup is needed 
               when the mirror connection goes down & comes back up due to 
               any of following:
                        - Backup rpd restart.
                        - TCP connection between the 2 rpds going down for
                          any reason.

 - Replication of state involves using the MAPI callbacks to provide the
   encoded messages for transmission.
 - Encoding for replicatin messages has the following characteristics:
        + The encoding of messages uses TVLs that are either of base types or 
          are byte streams. This is to ensure that ISSU will not have any 
          issues going forward.

4.2 PIM RDB module (New)
  This module manages the PIM replication database (RDB) & manages its 
  maintenance on the master & on the backup. This module is closely coupled 
  with the PIM mirror module which syncs the RDB state to the backup.

  Manages the following:
   - Creation of the RDB on the master when NSR is enabled.
   - Deletion of the RDB on the master when NSR is disabled (This is after the
     master has replicated the DELETION of the RDB entries to the backup).
   - Manages Resolving/Unresolving of RDB entries. 
       + Backup: This is mostly relevant on the backup - just because the 
         master sends some state, does not mean that that state can be used 
         by the backup immediately.
         The backup may not have the config or the interface/route/RP state 
         just yet which would make the specific replicated state useful 
         immediately.
       + Master: This becomes relevant on the master when the underlying 
         data structure that an RDB entry represents has been deleted. During 
         the time when the master is waiting for the MIRROR_DELETE callback 
         to complete, the RDB entry (in question) is unresolved pending 
         deletion on the completion of the MIRROR_DELETE() callback from MAPI.

  The RDB module contains code providing replication-services to the following
  PIM modules:
     - Neighbor: 
     - RP mechanisms (BSR/AutoRP): each RDB-entry correponds to a router 
       that sent us some BSR/AutoRP message.
     - JoinPrune maintenance (JP module): each RDB-entry corresponds to a nbr 
       that has sent some JP state. The actual JPs sent by the nbr are glued 
       into the RDB-entry as an entry in the Ptree.

 RDB creation job:
     - When NSR is enabled, this job creates RDBs for each of the PIM modules
       relying on the PIM RDB module for support.

 RDB deletion job:
     - When NSR is disabled, this job mirror_delete()s the RDB s (if the mirror
       connection is still up) & thereafter deletes them.

4.3 PIM infrastructure module:
    PIM NSR can be enabled in 3 different ways:
      - Enabling NSR in a previous commit & PIM in a subsequent one. This
        correspods to enabling PIM on a router already configured for NSR.
      - Enabling PIM in a previous commit & NSR in a subsequent one. This
        corresponds to a router running PIM (& having much of the PIM control
        state & multicast route state already present) having NSR enabled 
        on it.
      - Enabling PIM & NSR in the same commit: interally this maps to 
        a router having PIM configured on a router already running NSR. 
        However, in this specific case there will only be minimal PIM 
        control state present.

   
          
4.3 PIM interface module
    The PIM interfaces (pifs) are created on the master as usual.
    On the backup, the pifs are created just as they would be on the backup 
    with some minor differences. Ksyncd replicates the ifl state to the backup.
    The backup then uses the same code as the master to create the pif & its 
    self-nbrs.
    
    The backup does not create any dynamic interfaces. For the dynamic ifls 
    that the master creates, the backup creates pifs if its config so allows. 
    This is similar to how things happen for the graceful restart case when 
    the newly starting rpd discovers remnant dynamic ifls that were created 
    by PIM.

4.4 PIM neighbor module
    The backup does not receive any PIM pkts on the wire. So, it waits for the
    master to replicate the non-self-nbr state to the backup.
    
    Self-nbr creation is closely tied to the creation of the PIM interfaces 
    (pifs).
    The backup independently creates its self-nbrs for ifls on which it has PIM
    configured & which are UP.

    The master replicates the following neighbor state:
     - Hello pkts from non-self nbrs: these are input into the hello processing
       code on the backup to create the nbrs on the backup.
       Whenever a non-self-nbr updates it hello (makes come Hello packet 
       change), the entire new hello msg is replicated to the backup for it 
       to update its view of the neighbor.
     - For self-nbrs: the master replicates the self-hellos. This is to ensure
       the generation-ID for the self-nbrs on the 2 REs remain the same.
       Else at swithchover, the external neighbors would delete the JoinPrune
       state sent by the NSR router to them. This would cause unnecessary data
       latency due to having to re-establish the JP state.

   The backup disables the timers to time-out the neighbors.
   These are enabled only at switchover.

4.5 Join/Prune (JP) management module
  - JP state needs to maintained in sync between the 2 REs to ensure that
    after failover, the mroute oif-state will remain the same [barring other
    JP replication is done only for the SSM/SM modes.

For dense mode, no explicity JP state is maintained. Rather, only the prune 
state is maintained inside sg_ditable inside the sg_node.

In order to efficiently replicate the JP state only the JP state deltas are
replicated to the backup. Initially, when the connection between the 2 rpds
goes up & when PIM NSR is configured, all of the existing JP state is 
replicated to the backup. Subsequently, whenever a Join or a Prune entry is 
added or deleted, such an operation is replicated to the backup.

The backup's PIM RDB module has support built in to handle unresolved JP 
entries.
 
Rather than define our own messaging for encoding the replicated JP state, we
reuse the PIM standard Join/Prune message. Based on the existing data 
structures we recreate the PIM standard JP message that a downstream 
neighbor would have sent to us had it intended to send us the Join/Prune 
state that we are trying to replicate to the backup. Using the standard PIM JP 
messages ensures future-proofness of these messages which is important for 
ISSU.

Reusing the standard PIM JP messages requires the state to replicated on a 
per-nbr basis (The standard PIM JP messages are sent by a "specific PIM
 neighbor").
Thus, from the point of view of the PIM Mirror module it is really dealing with
PIM RDB module's JP subcomponent. This subcomponent is made up of JP-RDB 
entries which are per-neighbor that has sent us JP state. When the state has 
to be replicated to the backup, it is done by efficiently creating the 
standard PIM JP message from the pim_rxjp state that was received from this 
neighbor.

Recreating the simulated PIM JP messages from a downstream neighbor:
  - We create a radix_tree of pim_jpgroups & the pim_rxjps to be mirrored are
    appropriately threaded into each pim_jpgroup. A walk of the radix tree &
    walking the threads in each pim_jpgroup in the tree provide the relevant
    data to contruct the JP packet to be handed off to MAPI.

 
 

4.6 BSR/AutoRP modules
    For each of BSR & AutoRP, the master maintains a copy of the last packet
    of a type received from another router. When the backup rpd comes up,
    it plays these packets to the backup rpd. The backup rpd passes these
    packets into its relevant processing code & learns the RPSets.

    When the master receives a newer packet of a type [(BSR/CRP) for BSR
    and RPAnnounce/RPMapping for AuoRP] from an existing sender, it updates
    the last pkt stored if required. If the updation was done, the master
    also provides a copy of the updated message to the backup.

    The above mechanism ensures that the backup maintains the same RPSet
    as learnt through AutoRP & BSR.

    
4.7 PIM Rosen module
    To list details later.x


4.8 PIM SFSM module
    To list details later.
    
    The intent of this section is to list down how things would work on
    a NSR router configured in a specific role in the PIM domain.
    This also accounts for the data-triggered state changes that need to 
    happen on the backup to keep the state in sync.

    IGMP functionality:
        Backup does not have any IGMP group state of its own.
        Race conditions between:
             + Backup: Arrival of simulated PIM join from master & creation of
                       relevant self-nbr.
             + Master: IGMP triggered dynamic join state creation & static
                       join state creation for the same (s,g)/(*,g).
                    
        - IGMP dynamic joins:
        - IGMP static joins:
    SSM:
        The master replicates all the pim_rxjp state to the backup. This
        ensures that the backup has all of the relevant Join/Prune state.

        FirstHop Router:
        Intermediate Router
        LastHop Router:

        In all 3 of the above cases, the backup rpd would trigger an internal
        resolve. This would cause the same route installed on the backup as
        was installed by the master.

        Possible issues:
          - Iif on backup is not the same as iif on master (Due to unicast
            routing being out of sync):
          - Pim_rxjp for first downstream join showed up after the internally
            triggered resolve on the newly added phantom route: The resolve
            would end up creating an mroute with an Mdiscard nexthop. This
            would eventually on processing of the first pim_rxjp would get
            converted to a proper forwarding multicast nexthop.

    SM:
        FirstHop Router:

        Intermediate Router:
        Cases:
        1. SPT switch triggered before the arrival of data on SPT [When
            RPF'(RP) == RPF'(src)]:
            Master has already switched to SPT before backup rpd came up:
                   
            Master switches to SPT after backup rpd came up:
                   Backup does the same if it has the same rxjp database &
                   the same conditions [RPF'(RP) == RPF'(src)].
                   If the relevant (S,G) pim_rxjp shows up after the faked
                   resolve, the backup would have missed the SPT switch & it
                   would not have shifted to the SPT.
                   
        2. SPT switch triggered by the arrival of data on SPT:
            Master has already switched to SPT before backup rpd came up:
                   The master would replicate all the rxjp state to the backup
                   & when ISS is done the backup would trigger internal
                   resolves on all the phantom routes. The resolve coming on
                   the rpf-if(Src) will ensure that the master 

            Master switches to SPT after backup rpd came up:
        3. .

        LastHop Router:
            Master has already switched to SPT before backup rpd came up:
            Master switches to SPT after backup rpd came up:
            
        RP:


    Assert state:
    No extra work has been done for Asserts in an NSR environment.
       - Downstream asserts merely remove an oif from an OIL. The effort & 
         complexity to replicate Assert state is not worth extra pay-off. 
         Due to not replicating the assert state, the backup would not have 
         any downstream assert. Upon failover, this would cause the new-backup 
         to start forwarding traffic on oifs where the old-master was an
         Assert loser. This situation would get resolved quickly when the
         actual assert winner reinstantiates its Assert state on seeing a
         data packet forwarded by the new master.
       - Upstream assert state: An upstream assert causes the RPF' to be
         modified. Not replicating the upstream state to the backup will
         cause the new-master to continue to send its joins to its RPF nbr.
         

4.9 PIM Dense Mode

    NSR for dense mode relies not on explicit replication of all the SG
    events, rather on the quick regeneration of state on the backup after a
    switchover based on the presence of an mroute on the backup due to a
    route addition by the old master.

    The base inputs for generating SG state in DM are the list of neighbors &
    the presence of local receivers. The PIM mirror module replicates the
    neighbor state to the backup.
    When a backup rpd comes up, after initial state replication has completed,
    it generates an internal resolve on each multicast route installed by the
    master (phantom routes).
    Additionally, the backup does not timeout the mroutes installed.
    This ensure that the backup has routes for all the dense groups that
    the master has routes. This helps to reduce pkt losses for (SG)s due to
    time to resolve.

    The backup does not know of the following:
       - Any prunes sent by downstream neighbors.
       - Any subsequent Grafts sent by the downstream neighbors.

    The backup maintains the oif list for each SG based on the list of
    downstream neighbors.

    At switchover, the new master does the following for each SG:
       - Does a prune echo on each unpruned oif. This ensures that it quickly
         re-learns the prune state as appropriate & prunes the relevant oifs.
       - At switchover, IGMP would have learnt the presence of local receivers
         & the new master uses this to update its oif list for the mroute.

    Optimizations possible in the future:
    The backup rpd could track every change in the nexthop (corresponding to
    the oif state) & accordingly update its oif state for the dense SG & the
    corresponding backup route. At switchover, the new master 

4.10 RPD KRT module
1.  Routes:
    Changes are to adds/delete a phantom mroute on the backup when the master
    adds/deletes a multicast route.
    There are hooks in the add/delete handlers to create/delete the 
    appropriate multicast routes on the backup by triggering an internal 
    resolve on the backup.

    When the master changes a multicast route, the corresponding phantom 
    route on the backup has its ktsi info updated appropriately.

2.  Nexthops:
    On the backup it is possible for nexthops to exist that have a kref-count 
    of 0.

    Decoding nexthop ops on the backup (in response to the operations on 
    the nexthops by by the master):
     - INDIRECTS pointing to MULTIRT:
       Act on the RTM_* messages from the kernel for indirect nexthops 
       pointing to MULTIRT forwarding nexthops.
  
       Even if these were ignored, things would work fine in *most cases* as 
       the nexthop is fetched from the kernel whenver a RTM_ADD/CHANGE is 
       processed for a route pointing to the nexthop in question. This 
       similarly creates the nexthop & sticks it in the nh-nhid-table & also 
       in the per-AF mcast nhop-radix-tree.
       However, there is a chance that a failover might happen before a 
       RTM_CHANGE for a route is sent to the backup rpd . In this case 
       ignoring the RTM_ADD for the new nexthop (which happens before the 
       RTM_CHANGE for the route), would cause the new master-rpd to lose 
       track of the new nexthop that the master rpd created before dying.
  
   -  MULTIRT:
      These are maintained by the kernel & rpd need not track the 
      creation/updation of these.

   Backup: When the kref-count of a multicast nexthop reaches 0 (due to the
   master not having any multicast routes that reference this nexthop or 
   equivalently the backup not having any phantom routes that reference this 
   nexthop), remove it from the nh-nhid table & set its nhid to 0. However, 
   the nexthop itself is continued to be maintained.


4.11 RPD OS module:
     - Initialize the module init handlers for PIM & MC on the backup.
     - Initialize the module-var-init handlers for PIM, IGMP & MC on the 
       backup.
       On the backup, IGMP/MLD do not run. So, module_init handlers are not
       needed. However, module_var_init handlers initialize some global vars 
       that are used by PIM.
     - Initialize the hooks so that the PIM status will show up in the 
       output of "show task replication".

4.12 IGMP module:
   - Init module handler for the igmp/mld task is not registered on the
     backup. This is to keep the backup from running the IGMP state machines.
     (Code change for this is in rpd/os/task.c -> sent as a separate file for
     review).
   - var_init module handler for igmp/mld is registered even on the backup, as
     this initializes some sockaddrs (eg all_routers etc which are used by 
     PIM).

   - At RE swtichover, the new master would automatically have its 
     init/var_init handlers updated for mld/igmp correctly to 
     mld_init/igmp_init & mld_var_init/igmp_var_init respectively.
   - igmp_init()/mld_init() gets called internally when an ifl is auto-enabled
     for IGMP/MLD due to PIM (& other mcast proto) config. This is now done
     selectively only on the master.
   - At RE switchover, the global MC task triggers re-evaluation 
     of auto-enabling for PIM interfaces (Sent separately as updated review 
     for rpd/mc).
   - PIMv1 uses IGMP msgs. The backup rpd should just return if igmp_send_raw
     or mld_send_raw is called.

4.13 MC module:
   - At RE swithchover, interfaces need to be evaluated for auto-enabling IGMP
     based on whether or not PIM (other mcast routing protos) are configured.
     On the backup, neither the igmp_inint()/mld_init() handler is run and nor
     are mgm_ifs created.
   - Do not time-out the mroutes on the backup.
   - The backup would explicitly delete the mroute when the kernel replication
     code informs the backup about the master having deleted the said mroute.
     (Change for triggering this is in rpd/os -> to be sent out as a separate
     review).
   - Resolve on backup must originate due to a trigger from the master
     installing the mroute (This mroute would be replicated by ksyncd to the
     backup and be seen on the master as a new route being added by the 
     kernel).    

3.0 NSR structures


3.0.1 PIM NSR context

The NSR contruct maintains book-keeping information about the state of PIM NSR
on the master and on the backup.

Additionally, the replication database for the replicated PIM is maintained as
a part of the PIM NSR context.

Also, the background task/timers/jobs to try to resolve the RDB entries also
reside in the PIM NSR context.

typedef struct pim_rdb_sync_ctx_ {
    patroot   psc_nbr_root;
    patroot   psc_rp_root;
    patroot   psc_rpf_root;
    patroot   psc_sg_root;
    patroot   psc_jp_root;
    patroot   psc_mdt_root;

    boolean   psc_ctx_inited;
    task      *sync_task;
} pim_rdb_sync_ctx_t;


After RE switch-over, the backup shall walk through all of the unresolved
entries in its RDB & delete the RDB entries that are not resolvable based on
its configuration and unicast routing state.

3.0.2 Replication database (RDB)

On a router with NSR configured, both the master & the backup RE shall 
maintain a replication database (RDB). The master shall retain the RDB even
after it has sent the entire state as part of the initial state replication
to the backup.

On a router not configured for NSR, neither RE shall maintain the RDB.

The master maintains entries in the RDB primarily to have efficient storage
of control state potentially-needing-mirroring to a backup RE. By having the
potentially-mirroable state stored for efficent walking, the master can 
reduce the intensity of processing in sending initial state to an initially
starting backup.

The backup maintains entries in its RDB based on the mirrored updates that
it receives from the master. This is a way for the backup to retain mirrored
state info (from the master) for which the backup has no immediate use due to
not having the relevant config or due to not having the relevant kernel-learnt
state like ifls/routes etc.

Resolved/Unresolved RDB entries: The whole idea of a resolved or unresolved 
RDB entry is relevant only on a backup. 

The conceptual PIM objects for which RDB entries are maintained are:
 - Nbr state.
 - RPSet state.
 - JoinPrune state (SM & SSM only).
 - SG (SM & SSM only).
 - RPF.
 - MDT (Data MDTs only).



3.1  Neighbor state maintenance
Maintaining the list of neighbors (on a per interface basis) accurately,
including all the known hello options associated with each neighbor is key
to maintaining the PIM routing state correctly.
This shall be achieved by mirrorig the neighbor state from the master to the
backup.


3.1.0 Data structures & message formats

The message format for the TLV list that shall be used to replicate the
neighbor state from the master to the backup.

     0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |                     Kernel Instance Index                     |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    | Address Family |             Reserved                         |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |             If Index on which Hello pkt recvd                 |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |                        Src address          ........          |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |                        .........                              |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |                        Hello pkt length                       |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+
    |                        Hello pkt contents .........           |
    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+

Based on such TLVized messages received from the master, the backup shall
maintain a ptree of pim_rdb_nbr_t structures. The key for the ptree nodes
shall be built using:
      - Kernel instance id.
      - Ifl index for intf on which this nbr exists.
      - Mcast addr family.
      - Primary address of the neighbor.

/* 
 * Key used to lookup in the ptree of nbr RDB entries.
 */
typedef struct pim_rdb_nbr_key_ {
    u_int        pr_nbr_k_inst_index; /* Kernel inst index */
    mc_af        pr_nbr_mc_af;        /* Mcast address family */
    ifl_idx_t    pr_nbr_ifl_idx;      /* ifl index on which nbr */
    union {                           /* primary address for nbr */
        in_addr  pr_nbr_addr_v4;
        in6_addr pr_nbr_addr_v6;
    } pr_nbr_addr;   
} pim_rdb_nbr_key;

/*
 * pim_rdb_nbr - PIM nbr replication database (RDB) entry.
 */
typedef struct pim_rdb_nbr_ {
    patnode          pr_nbr_node;         /* glue: per-af ptree of rdb_nbrs */
    pim_rdb_nbr_key  pr_nbr_key;

#define pr_nbr_k_inst_index pr_nbr_key.pr_nbr_k_inst_index;
#define pr_nbr_af           pr_nbr_key.pr_nbr_mc_af;
#define pr_nbr_ifl_idx      pr_nbr_key.pr_nbr_ifl_idx
#define pr_nbr_addr(af, prn_addr) ((af == MCAST_AF_IPV4) ? \
(prn_addr.pr_nbr_addr_v4): ((af == MCAST_AF_IPV4) ? \
(prn_addr.pr_nbr_addr_v6): NULL)) 

    sockaddr_un      *pr_nbr_src;         /* nbr's src address */

    void             *pr_nbr_hello;       /* Last hello pkt recvd from nbr */
    u_int32_t        pr_nbr_hello_len;    /* Length of hello pkt */

    /* Back-ptrs */
    pim_nbr          *pr_nbr_pim_nbr;

    /* Mirror sub-system vars */
    mirror_data_node pr_nbr_mirror_node;
    thread           pr_nbr_mirror_thr;

    /* If there exists a real struct corresp. to this replication entry */
    boolean          pr_nbr_resolved;
    u_int32_t        pr_lock_count;
} pim_rdb_nbr_t;


3.1.1 Pre-failover
Master:
For each known PIM neighbor, the master maintains a copy of the last PIM
hello received from that PIM neighbor.

Additionally, when the backup RE connects to the master RE on the mirror
connection, the master sends a TLVized list ALL PIM neighbors on a per ifl
basis. The master is able to quickly queue the appropriate state messages into
the mirroring system by walking the ptree of pim_rdb_nbr_t structs that the
master maintains in the NSR context. Pre-maintaining such a ptree in the NSR
context helps to make sending the nbr state to the backup RE more efficient
compared to generating such state just-in-time when the backup RE actually
implicitly requests state by connecting to the mirror connection.

After the intial state download is completed, the master continues to
replicate state for each new neighbor that gets added on the pif. Also, for
existing neighbors that undergo a change in parameters, an UPDATE mirror op
is sent for the neighbor in question.

ENH: The master can send a bitmask that uniquely indentifies each neighbor with
a bit. This can be sent periodically (say every Hello interval) to let the 
backup know that the neighbors corresponding to the listed bits in the bitmask
are alive and unchanged. This is a substitute to sending the, comparatively,
rather voluminous NBR mirroring msgs to the backup to refresh the nbr state.

This shall help to cover the case, wherein a neighbor from whom a recent
hello has not been heard for a while gets timed out on the backup almost
at the right time, if the master were to die just before finally sending a
delete when the neighbor would actually expire.

Backup:
The backup RE receives the TLVized neighbor state from the mirroring system.
Based on the received info, the backup builds a ptree of pim_rdb_nbr_t
structs (using the key as described earlier in this section) in the NSR
context.

The purpose of this is two-fold:
    - Config mismatch mitigation: To retain nbr state for eventual creation .
      of PIM nbrs, should the local config allow such an eventual creation.
    - Preparation to become master: should this RE becomes master in the
      future, it has this ready repository of state to send to a potential
      backup RE (This is preferable as against creating this state when the
      backup actually shows up on the other end of the mirror connection).

For each entry on the ptree of pim_rdb_nbr_t structs, the backup tries to
create a corresponding neighbor after the backup receives such state. However,
based on config on the backup, some of the entries on the ptree shall remain
unresolved.
The backup shall convert the unresolved entries into resolved entries (and
create corresponding PIM nbrs) or convert the resolved PIM neighbors into 
unresolved entries on the NBR RDB ptree, as dictated by:
       - Instance addition/deletion on the backup RE.
       - Ifl additiona/deletion on the backup RE.
       - PIM config changes.

At any given time, only the resolved NBR RDB entries have a corresponding
PIM nbr.

DR election: Prior to a failover, nothing special needs to be done for 
maintaining a correct view of the DR on an interface. The backup shall use 
the known resolved PIM neighbor state to independently compute a DR on 
an interface. In config-steady-state, the backup and the master shall
independently compute the same DR.


3.1.2 Post-failover:
After a failover, the backup becomes the master. The new master thus needs to
get rid of state that is invalid - specifically, all unresolved NBR RDB entries
need to be got rid of as invalid state.
In order to do this, the new master shall try to resolve each unresolved
entry as dictated by its local:
      - Corresponding instance state.
      - Ifl presence or otherwise.
      - PIM config.

DR election: Due to config mismatches, it is possible that the backup's view 
of the DR on a give interface might be different from that of the master. 
In order to get around this, the new master shall immediately send out Hellos 
on ALL of its PIM enabled interfaces. This shall ensure a minimal period of DR 
inconsistency on the given interface.



3.2  RP state maintenance - RPSet & pimd/pime dynamic interfaces
There are multiple ways that JunOS PIM learns of RP state information.

In maintaining the RP state across RE failovers, the conceptual objects 
whose consistency needs to be maintained across faiovers are:
  - pe/pd interfaces: Based on DR/RP state the old mater RE would have created
    some pe/pd ifls respectively. A failover can happen at any given instant.
    At a given instant, a pe/pd interface can be referenced in a route/nexthop.
    In order to maintain consistency of the installed routes immediately after
    a failover, the new master must reuse the dynamic ifls, that the old master
    created, that are relevant on the new master. 
  
  - RPSet (RP<->RP-group mappings) info:
    The RPSet info that the master learnt should be made available on the
    backup as soon as possible. This is to ensure that the backup has the
    relevant RPSet when it becomes the new master. The RPSet is mantained as
    a per-scheme-pair of ptrees for the pim_rps & the pim_rpgs.

The dynamic RP ifl handling is common across the various RP state
discovery/maintenance schemes. However, each RP discovery mechanism uses
a different scheme to learn, retain & time-out the retained RP state. Due to
this difference across RP discovery mechanisms, a customised (per-mechanism) 
way is required to replicate the learnt RPSet to the backup.

Common 

RPSet msgs (in this doc. generically) refers to msgs received from AutoRP or 
BSR or another RPSet discovery mechanism.

Storing the received RPSet msgs:
 - Need to avoid storing multiple copies of the same info received from the 
   same source.
 - Timing out the stored RPSet RDB entries:
	+ Unresolved: whenver the specific RPSet discovery mechanism would 
          have timed out that state.
	+ Resolved: Sooner of the following 2:
	     - When the RPSet mechanism would have timed out the state
               contained in the msg. -> True only on the master.
	     - When the actual RP/RPG struct of outermost scope that was built
	       from this msg is deleted.


What resolves a received RPSet msg:
 - The underlying pd/pe ifl becoming available.
 - The config knob enabling the relevant RPSet mechanism is enabled when the 
   the master had already sent us the relevant RDB entry prior to this config 
   change.

What converts a resolved RPSet msg into an
unresolved msg:
 - The underlying pd/pe ifl is deleted.
 - The config knob enabling the relevant RPSet mechanism is deleted without 
   the master sending a delete for the relevant RDB entry.

3.2.1 Data structures
/* 
 * Key used to lookup in the ptree of RPset rdb entries.
 */
typedef struct pim_rdb_rp_key_ {
    u_int        pr_rp_k_inst_index; /* kernel inst index */
    pr_ipx_addr  pr_rp_addr;          /* Sender of this RP-set msg? */
    mc_af        pr_rp_mc_af;         /* Mcast address family */
    ifl_idx_t    pr_rp_ifl_idx;       /* Interface this was received on? */
    u_int        pr_rp_mech_type;     /* What RP discovery mechanism */
} pim_rdb_rp_key;

typedef struct pim_rdb_rp_ {
    patnode          pr_rp_node;      /* Glue for RDB tree of RPs */
    pim_rdb_rp_key   pr_rp_key;
    
    void             *pr_rp_rpset_msg;    /* RPSet msg */
    u_int            pr_rp_rpset_msg_len; /* RPSet msg len */

    /* Backptr threads */
    thread           pr_rp_derived_rp_head; /* Thread of pim_rp's that
                                             * correspond to this RDB entry. */
    thread           pr_rp_derived_rpg_head; /* Thread of pim_rpg's that
                                              * correspond to this RDB entry */

    /* Mirror sub-system vars */
    mirror_data_node pr_rp_mirror_node;
    thread           pr_rp_similar; /* Thread of pim_rdb_rp_t's: RPSet msgs 
                                     * from same src, in same inst, for
                                     * same RPSet mechanism */
    
    /* Have we created real structs - pim_rp/pim_rpg/pim_rpif structs
       corresponding to this RDB entry? */
    boolean          pr_rp_resolved;
} pim_rdb_rp_t;


3.2.2 Dynamic RP-ifl state maintenance - pe/pime & pd/pimd interfaces

3.2.2.1 Pre-failover
Master:
 - Creates any pe/pd ifls as relevant & maps them to the appropriate
   rp_ifs.

Backup
 - Does not create any pe/pd dynamic ifls.
 - Learns of already existing pe/pd ifls and the subsequently created ones
   based on routing socket messages received on the interface socket in
   response to the pe/pd ifl creation by the master.
   The backup shall try to map the thus created pd/pe ifls to its rp_if
   state for the relevant RPs.
 - Handles the delete-processing for the pe/pd ifls deleted by the master.

3.2.2.2 Post-failover:
pe/pd interface state handling:
 - Dealing with redundant pe/pd interfaces. There could exist pe/pd ifls for
   which the new master has no RP state. The new master should delete these
   ifls at the earliest available chance, after removing all rp_if structs
   that depend on these ifls.
 - Dealing with partially created pe/pd ifls:
 - Creating pe/pd ifls for relevant RPs:
pe/pd (and pime/pimd as well. Henceforth, pe shall mean both pe & pime, and
likewise pd shall mean both pd & pimd).

Pe & pd are dynamic ifls that are created by rpd. A pe ifl is a PIM 
encryption interface that when present in the OIL of an (s,g) causes the 
packet sent on the pe interface to be unicast to the corresponding RP 
associated with the group G. Pe interfaces are used on a source's DR to
PIM-register encap & unicast them to the RP.
A pd ifl is a PIM decryption ifl that is created on a RP to decrypt the
register message sent by a source's DR.


3.2.3 Static RP

3.2.3.1 Pre-failover
Each RE independently creates the pair of ptrees for the pim_rps & the
pim_rpgs based on the statically configured configs.

Any RPs for which the the relevant pe/pd ifls are missing on the backup, are
treated as inactive RPs (including for the local RP case).

Any SG state that refers to such an unresolved RP shall also be kept as
unresolved. As a result such (s,g) entries would not be installed on the
(s,g) entry tree. Rather these would exist only in the RDB.

3.2.3.2 Post-failover
The configured RP mappings (not necessarily the same on both the REs) are
available on both the REs. Each RE maintains these mappings in the usual
data structures - the rp & the rp-group trees.   


3.2.4 AutoRP
Described under 3.2 Common.

3.2.5 Bootstrap Router (BSR)
Described under 3.2 Common.




3.3 Join/Prune state maintenance
Dense mode (DM) creates it join/prune state based on the arrival of data. 
There is no explicit signalling of join/prune state independent of what is 
first triggered the arrival of data.

For NSR, there isn't a need to explicitly replicate join/prune state for
DM.


3.4 SSM & SM maintain the join/prune 

Data-triggered state maintenance:
Case 1: Recvr's DR
     Race conditions (for the backup) between:
          - Resolve for data pkt on the RPT
          - Resolve for data pkt on the SPT
          - Receiving the (s,g) join from the master
Case 2: Intermediate router
Case 3: Src's DR
Case 4: RP


3.5 Route reconciliation at switchover


3.3 SM & SSM SG state maintenance

3.3.1 Basic SG state:
A pim_sgnode maintains the entire state relevant for PIM SG entry. A pim_sgnode
can be split coneptually into the following components:
    A. List of oifs with the per-oif Join/Prune (JP) state of a certain kind 
       (*G, SG, SGRPT).
    B. Glue fields to tie the list of oifs & the per-oif JP state to the 
       pim_sgnode.
    C. Ancillary state that a pim_sgnode needs: RPF info & the assert info.

B does not need to replicated to a backup as this is adminstrative glue that
a backup can build on its own to tie together A.
C shall be (partially) replicated & this is dealt with in the appropriate
section elsewhere in this document.

In recreating SG state on a backup, A is the critical SG state subset that
needs to be replicated.

A is maintained as a set of per-oif ptrees that are indexed into a bitmask 
from the pim_sgnode (sg_sitable or sg_ditable depending on sparse or dense mode
respectively). Each of these per-oif indexed ptrees correspond to the JP state
received from all the neighbors on that intf - a pim_rxjp. (Each node in the
ptree is the JP state received from a specific nbr on that intf).

In replicating A, it is best to leverage existing code wherever possible. To
make that happen, PIM JP msg types would be leveraged.

A PIM JP msg corresponds to JP state info sent by a downstream nbr for all 
known SG/*G state. Thus, the entire state contained in sg_sitable and/or
sg_ditable shall need to be partitioned on a per-nbr basis for eventual sending
out in a suitably formatted PIM JP msg stuck inside mirroring msgs.
This is available by walking the nbr_join_thread thread from the nbr.

In replicating SG information, no explicit indication about the group type is
sent by the master. The backup treats the group as a dense or as a sparse group
based on its local config.

3.3.2 Register/RegisterStop (SM only):
 - RP:
     + Decap route: indicates RP is still  getting the data for some group 
       from the relevant DR on the register tunnel. When the master is
       planning to send a SG join in response to a Register, it
       must get an ACK for the synced JP sent to the backup before it actually 
       sends a Join out towards the src-DR. Backup: for any SG Join state for 
       which this router is the RP, at failover the backup must send out SG 
       joins soon after becoming master. This is to contain the possibility 
       that in between syncing the SG Join  to the new-master & sending the 
       join to the src-DR, the old-master dies. Also, based on the master 
       adding the native route for the SG in question due to resolve (before 
       it replicates the SG Join to the new-master), the new-master should 
       also internally-trigger a resolve to create the relevant multicast
       route.	  
 - DR:
     + Registering:
	 - Mcast route Local sources with a pe (for same RP) in oif?
           Then continue to send registers. Only after the SG join has been
           recvd by the old master, it would remove the pe from the oif.
         - Master should remove the pe from the OIL only after the relevant
           SG updation due to SG Join propagated by RP has been processed by
           the backup.

3.3.3 Asserts (SM & SSM):

Upstream:
 - Upstream assert modifies the RPF nbr to which Joins are being sent.
 - For each SG, the RPF nbr/ifl will be replicated.
 - Also, RPF state shall be replicated. This shall help in the following ways:
	+ Adjusting the sending of the Joins to the correct upstream nbr at
          failover & correspondingly pruning towards the old RPF being used by
          the old master.
	+ Additionally, in order to take into consideration any upstream 
          assert state, the RPF node_type shall indicate if the RPF chosen by 
          the old master was due to an upstream Assert. If yes, and if the
	  new-master's unicast routing still provides this ifl as an iif for
          the best route (even if one of the equal-cost ones) use this RPF as 
          informed by the old master.

Downstream:
 - No explicit replication of state for downstream assert state is needed.
 - Assert state m/c has built in mechanism wherein a router with 
   (CouldAssert() == TRUE) that is a potential forwarder would re-trigger an 
   assert battle. So, for an oif, where the new master would have been a 
   winner....such an action (just forwarding without consideration for old
   master's assert state on this oif) just re-inforces the position of the 
   new-master as a forwarder. If the new-master would eventually
   be an assert-loser (based on its own unicast routing state) irrespective 
   of such state on the old-master, such an action (just forwarding 
   without consideration for old master's assert state on this oif) just 
   ensures that the new master learns it Assert state without the need
   for explicit synchronization.

3.3.4 Data event planning:
  - Resolve requests: mcast route addition by master, causes the backup to 
    trigger an internal resolve.
  - SPT-alert (Due to Iif mismatch): when this happens, the master syncs a 
    RDB_SG msg to the backup to indicate so. When the backup sees that the
    iif for the sg has changed along with a change in the sg_flags, it does 
    appropriate processing for the SPT-alert processing. The master ofcourse
    does the relevant SPT-alert processing only when the backup has acked
    that it has processed such an event.
  - Threshold alerts: this is indicated to the backup by updating the pim_jp 
    info for the SG in question.

3.3.5 Data structures
/* 
 * Key used to lookup in the ptree of SG rdb entries.
 */
typedef struct pim_rdb_sg_key_ {
    u_int        pr_sg_k_inst_index;  /* kernel inst index */
    mc_af        pr_rp_mc_af;         /* Mcast address family */
} pim_rdb_sg_key;


/*
 * pim_rdb_sg - PIM (S,G) entry replication database (rdb) entry.
 */
typedef struct pim_rdb_sg_ {
    patnode          pr_sg_node;      /* Glue for RDB tree of SGs */
    pim_rdb_sg_key   pr_sg_key;       /* Lookup key for this SG */

    pr_ipx_addr      pr_sg_rpf_addr;  /* RPF nbr being used for this SG */
    pr_rpf_ifl_idx   pr_sg_iif;       /* Ifl for RPF nbr */

    byte             pr_sg_flags;

    /* Back-ptrs */
    pim_nbr          *pr_sg_node;
    
    /* Mirror sub-system vars */
    mirror_data_node pr_sg_mirror_node;
    thread           pr_sg_mirror_thr;

    /* If there exists a real struct corresponding to this replication entry */
    boolean          pr_sg_resolved;
} pim_rdb_sg_t;



3.4  SM & SSM SG JoinPrune state maintenance
The need for replicating the JoinPrune state is described in 3.3.1 (Basic SG
state).

3.4.1 Data structures
/* 
 * Key used to lookup in the ptree of JP rdb entries.
 */
typedef struct pim_rdb_jp_key_ {
    u_int        pr_jp_k_inst_index;  /* kernel inst index */
    ifl_idx_t    pr_jp_ifl_idx;       /* Ifl corresponding to the Nbr whose JPs */
    mc_af        pr_jp_mc_af;         /* Mcast address family */
    pr_ipx_addr  pr_jp_nbr_addr;      /* Nbr addr - who sent this state?*/
} pim_rdb_jp_key;


/*
 * pim_rdb_jp - PIM (S,G) entry replication database (rdb) entry.
 */
typedef struct pim_rdb_jp_ {
    patnode          pr_jp_node;      /* Glue for RDB tree of JPs */
    pim_rdb_jp_key   pr_jp_key;       /* Lookup key for this JP */

    void             *pr_jp_jp;       /* JP msg recreated by master */
    u_int32_t        pr_jp_jp_len;    /* Length of this JP msg */

    /* Back-ptrs */
    pim_nbr          *pr_jp_nbr;     
    
    /* Mirror sub-system vars */
    mirror_data_node pr_jp_mirror_node;
    thread           pr_jp_mirror_thr;

    /* If there exists a real struct corresponding to this replication entry */
    boolean          pr_jp_resolved;
} pim_rdb_jp_t;



3.5  Dense mode SG state maintenance
SG creation, deletion and updation:
 - Based on mcast routes added by master.
 - Master should also replicate SG state to backup. This is to handle those 
   cases where the group in question is not a dense group on the backup. 
   This is inefficient in having to sync state.	When the master adds a new 
   mcast route, the backup checks if the group in question is dense. If it is 
   then it tries to build its pim_sgnode based on the route.

   There would exist a background job to reconcile the mcast routes for dense 
   groups.
     - When a new group-range becomes configured as dense, the job walks the
       mcast routes & for those SGs for which sg_nodes do not exist yet, shall
       trigger a resolve. This is to handle config mismatches.

Handling dense events:
  - Graft/GraftAcks: on receiving a Graft, the master should send a GraftAck 
    only after the mcast route has been known to be updated (with an additional
    oif) & after an upstream Graft has also been sent.
  - Upstream prunes: The new master on becoming master, for SGs with non-null 
    oifs, shall do a prune echo on each of the non-iif dense intfs followed by 
    appropriate processing based on whether or not it hears a Join on a
    non-iif ifl.
    This is to handle the case when the old-master might have wanted to send 
    out prune but died before it could. Also, this ensures that the new master
    has an accurate view of the interest in the SG on the non-iif ifl.

Asserts:
  - Similar logic as explained under the Sparse/SSM SG section.


3.6  Rosen MVPN state maintenance
Actions that b-RE does not take:
   - Sending create/delete/change msgs into the kernel to
     create/delete/change def-MDTs & create/delete d-MDTs.
   - Send MDT-join-TLVs.
   - Do stats-collection - so there is no data-triggerd d-MDT creation
     on a b-RE due to stats collected on the b-RE.
   - Create the mdt_tun_delay_timer on the src-PE.
   - Create the mdt_tun_timer on the src-PE or on the egress-PE.

Actions that the b-RE does take (more general description - not
necessarily inclusive of the specific actions during ISS, CSS & PFSR):
   - Maintains relevant state for def-MDTs (populates pim_vpn_info
     properly) by looking at the already created def-MDTs. The per
     def-MDT flags are not meant to track the various stages in the
     creation of te def-MDTs as this is irrelevant across REs.
   - Maintains properly created d-MDTs & maintain their association
     with c-SGs.
   - Installs the relevant mroutes (relevant to the c-SGs & the p-SGs -
     both def-MDT & d-MDT - into inet.1.

Issues:
   - MT interface state maintenance:
       + Not having to recreate the mt intfs on the backup after switchover,
         for the same set {lo0.Main address, and VPN group address}.
       + Handling config mismatches:
           - Dealing with the irelevant mt intfs:
               + Deleting: .
               + Updating: .
           - .
   - The subunit number space maintenance:
       + Post-switchover, not reusing the same subunit index that has already
         been used on the master to create an mt ifl.

3.6.0 Data structures
/*
 * pim_rdb_mdt - PIM Rosen data-MDT replication database (rdb) entry.
 */
typedef struct pim_rdb_mdt_key_ {
    u_int        pr_mdt_k_inst_index;   /* kernel inst index */
    ifl_idx_t    pr_mdt_ifl_idx;        /* Ifl corresponding to the Nbr */
} pim_rdb_mdt_key;

typedef struct pim_rdb_mdt_ {
    patnode                 pr_mdt_node;
    pim_rdb_mdt_key         pr_mdt_key;

    mc_af                   pr_mdt_mc_af;     /* Mcast address family */

    pr_ipx_addr             pr_mdt_vrf_sg_src; /* VRF SG src for this entry */
    pr_ipx_addr             pr_mdt_vrf_sg_src; /* VRF SG grp for this entry */

    pr_ipx_addr             pr_mdt_src_pe;    /* src of MDT join TLVs */

    /* Back-ptrs */
    pim_data_mdt_tun_info_t *pr_mdt_tun_info_ptr;

    /* Mirror sub-system vars */
    mirror_data_node        pr_mdt_mirror_node;
    thread                  pr_mdt_mirror_thr;

    /* If there exists a real struct corresponding to this replication entry */
    boolean                 pr_mdt_resolved;
} pim_rdb_mdt_t;


3.6.1 MT Dynamic ifl state maintenance

3.6.1.1 Pre-failover
Master
m-RE: synchronizes info about ALL of the data MDTs that already exist.

State that is synchronized, includes the info about the following:
   - Data MDTs: these need to be synced because we need to be able to 
     associate an ifl sub-index with the specific P-group<->(C-src, C-grp) 
     mapping in use for an MDT. 
     Also, on the egress-PE we need know the source of the MDT. We do not 
     sync the timer values as these do not serve the purpose reliably.

Backup
b-RE maps its config to create default mt ifl state (without actually
requesting the kernel) to create the default mt ifls, depending on the
presence of the mt ifls in the global ifl list.

3.6.1.2 Post-failover:
Resolve config-mistmatch state-diffs:
   - Per-VRF-instance validation:
      + Def-MDTs exist ? And should they be existing? Reconcile
        this to the config.
      + More than 2 def-MDTs exist? Or if the def-MTs that exist are
        not consistent with the config (srcIP &/or vpn-group for the
        existing def-MTs is not what it should be) then deal with the
        deletion of the inconsistent def-MTs & recreate new ones.
      + VPN group address same? If not, update the def-MDTs.
Timers:
   - mdt_tun_timer: 
     Src-PE: Send d-MDT join-TLV to the egress-PE on each of the
     data-MDTs. This timer would be created to fire in another 
     PIM_MT_MDT_UDP_PERIODIC seconds. In the worst case, doing this
     would end up sending d-MDT-join-TLVs too close together, which is
     acceptable.
              
     Egress-PE: set the timer to go off after PIM_MT_MDT_DATA_TIMEOUT
     seconds. In the worst case, this would d-MDT state to linger
     around for PIM_MT_MDT_DATA_TIMEOUT seconds longer than it should
     have (if the failover happened before the kernel route
     replication mechanim could delete the correspondng p-SG route
     installed by the om-RE. This should however,not affect the
     forwarding as the src-PE would already have switched back to the
     def-MDT. 
     If the data-MDT deletion was triggered due to the oif-list becoming NULL 
     for the corresponding C-SG, can be handled by having the b-RE do a 
     validation to ensure that the oif-list is non-NULL for each c-SG 
     corresponding to a extant d-MDT. This can be done using a job. In the 
     worst case that the om-RE died before syncing the prune state or the c-SG,
     the b-RE 
        - mdt_tun_delay_timer: Relevant only on a src-PE.
          Just the failover itself would not affect this timer in any
          regard. However, if the stats-collection were to independently
          trigger creation of a d-MDT then this timer would created just
          as in a 1-RE system.
            
Route handling:
   - Post-switchover, the kernel replication code would cause the
     deletion of the p-SG routes installed by the old-now-dead m-RE.
     At this time the b-RE's p-SG mroutes installed into the mc
     get installed in the kernel. However, the PFE should continue to
     forward without interruptions as the new routes would be identical
     to those installed by the om-RE.




3.7  Local receiver state (State derived from IGMP)
Any interfaces that are configured to run PIM, also get configured to run
IGMP. IGMP learns local receiver state & informs PIM of the same.
[IGMP-NSR] describes the changes to be made to IGMP so that IGMP would work
nicely with the scheme chosen for PIM NSR.

IGMP shall not explicitly replicate any group state. Instead PIM shall 
manage quick learning of updated local receiver state at failover. This shall
be achieved by the following:
 - Deletion of local receiver state for groups that lost receivers during the
   failover: There is a small timewindow (between the master failing & the 
   backup becoming an operational master) during which any received Leaves
   could be missed. To get around this, the backup (even prior to a failover)
   shall maintain an efficient data structure a walk of which shall provide
   pointers to all ifls with local listener state.
   At failover, the new-master would instantiate a timer to timeout the local
   receiver state for the groups.
   For groups for which the local listener state was lost during switchover,
   IGMP would not refresh the state. The timer would thus delete the local
   receiver state for such groups.
 - Quick learning of .local receiver state for groups with receivers that
   showed up which failover was in progress There is a small timewindow
   (between the master failing & the backup becoming an operational master)
   in which received unsolicited IGMP joins could be missed. To get around
   this, the new master starts off as a querier.



3.8  Handling Config Mismatches
Configs on the 2 REs at a given instant could be different for various reasons:
 - Inadvertent: There can be small time-windows in which
   time lags in committing the configs on the 2 REs and the said configs
   becoming active on the 2 REs cause transient config-mismatches.
 - Intentional: If the use of "commit" on an RE were allowed with NSR
   configured.

Possible config mismatches:
3.8.1   Global (non-PIM) config
        - Primary interface IP is different, for a given ifl.
        - List of secondary IPs, on a given ifl, is different.
        - Interface not belonging to the same instance on the 2 REs.
        - Loopback config:
            - Loopback interface of a given instance is different across the
              2 REs.
            - Primary IP address is different.


3.8.2   PIM config (on a per-instance basis):
        - Per-interface PIM config:
           + Interface not configured for PIM.
           + Mode is different.
           + Version is different.
           + Hello-interval is different.
           + DR priority is different.
           + Neighbor-policy is different.
        - Per-instance config
           + RIB-group is different.
           + Dense-groups list is different.
           + SPT-threshold is different.
           + Policies:
              - Sparse join import policy is different.
           + VPN-group-address (per-VPN) is different.
           + MDT config is different:
               - Group-ranges.
               - Thresholds.
               - Tunnel-limit.
           + Timer values are different:
               - Assert timeout.
           + RP config is different:
               - DR-register-policy is different.
               - RP-register-policy is different.
               - Configured RP discovery mechanism can be different.
                 eg. Auto-RP vs BSR.
               - Parameters for the same RP-discovery mechanim can be different
                   + Local RP setting is different:
                      - Address to use for the RP can be different.
                      - Local RP group-ranges are different.
                      - Hold-time & priority can be different.
                      - Anycast-RP config:
                          + Anycast-RP-set constituents are different.
                          + Local-address for the local-RP is different.
                   + Static RP:
                      - PIM version different.
                      - Group-ranges different.
                   + Auto-RP:
                      - Mapping/Discovery/Mapping-agent-election params can be
                        different.
                      - Dense groups to announce/reject are different.
                   + BSR:
                      - BSR priority is different.
                      - Import-policy is different.
                      - Export-policy is different.
                   + Embedded-RP:
                      - Group-ranges are different.
                      - Maximum # of embedded-RPs can be different.


3.8.3   Multicast routing-options:
        - Policies:
           + Flow-maps are different.
           + RPF-check policies are different.
           + Scope config is different.
           + Scope-policy is different.
           + SSM-group ranges are different.


3.8.4   IGMP config:
3.8.5   Config mismatch reconciliation technique:
Details to be added when we get to this stage It is not worthwhile to try to
list things here until these situations are actually handled in the code. This
is due to the totally adhoc nature of mechanisms that would be used to hande
each specific config mismtach.

Pre-failover
Master
Backup

Post-failover:
       Mismatched-config-reconciliation (MCR):
       A periodic job shall be spawned that shall try to reconcile all of the
       non-resolved mirroring entries into the relevant PIM structures. The
       unresolved mirroring entries of a certain type shall be on a thread.

       It is expected that the number of unresolved entries during the course
       of a normal operation would not be too many since, NSR now requries
       synchronized commits. However, in order to test mismatched configs,
       persistent config mismatches shall be generated.



3.9  RE switchover processing
This sub-section describes the processing needed when a backup RE becomes 
a master. All of the preceding sub-sections under section 3 describe
the RE switchover processing only from a perspective of the processing needed
specifically for the feature tackled in that sub-section. 

This sub-section collates all of the processing needed to be performed at
RE switchover.



3.10 Unicast routing mismatches
Each RE uses its own local unicast routing to determine RPF state. The backup
has a view of the RPF used by the master. At failover, the backup can
appropriately transition from one delivery tree to another by pruning up
the tree used by the old-master, if the immediate upstream branch used by the 
new master is different from that used by the old master.

3.11 Function vector
The master and the backup RE shall have some common code. Function vectors
shall be used to invoke the correct handler depending on whether the RE is a
master or if it is a backup. Listing of the vectored functions would be added
here as the code is more crystallized.

3.12 BFD support



3.13 ISSU preparation
To be added when we get to that stage in the coding.

4. APPENDIX & NOTES


4.1 Debugging:
    No new hidden CLI commands are planned. Rather new gdb macros shall be
    created to debug various interesting data structures.

4.2 Unresolved issues:
    - CAC.
    - Anycast RP support.
    - Unresolved entry: - need to define this accurately in the document.
	+ If unicast routing is not convering,
	  resolved or unresolved?
    


5. GLOSSARY

IIF:    Input Interface
IGMP:   Internet Group Management Protocol
NSR:    Non Stop Routing
OIF     Output Interface
OIL     Output Interface List
PIM:    Protocol Independent Multicast
RDB:    Replication Database
RE:     Routing Engine
RPD:    Routing Protocols Daemon




6. REFERENCES

[IGMP-NSR]      NSR: IGMP stateful replication - Functional Specification
                sw-projects/os/nsr/igmp-nsr-funcspec.txt
[NSR]           Non-Stop Routing (NSR) - Functional specification
                sw-projects/os/nsr/software_spec_3083.txt
[PIM-GR-RST]    Functional Specification - Graceful Restart for PIM
                sw-projects/routing/multicast/graceful-restart/func-spec
[PIM-NSR]       NSR: PIM stateful replication
                sw-projects/os/nsr/pim-nsr-funcspec.txt
[PIM-DM]        Protocol Independent Multicast - Dense Mode (PIM-DM):
                Protocol Specification (Revised). RFC3973
[PIM-SM]        Protocol Independent Multicast - Sparse 
                Mode (PIM-SM): Protocol Specification (Revised). RFC4601.




7  REVIEW COMMENTS